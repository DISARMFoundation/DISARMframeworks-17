# Technique T0166.003: AI-Nudified Imagery

**Summary**: This sub-technique can be used to assert that AI was employed to generate AI-nudified imagery. AI-nudified imagery refers to the use of artificial intelligence to modify an existing image or video by digitally altering the subject to appear unclothed, typically through the application of machine learning algorithms that remove or replace clothing with simulated content.<br><br>Platforms designed to nudify imagery require users to upload an image of a person fully clothed as the source content. The AI then applies algorithms to remove the clothing, effectively generating a synthetic image or video of the subject in a nude state. <br><br>To make this assertion, analysts can identify the original image, and confirm that the published image has been edited using AI to appear undressed. They may use evidence such as images being submitted to AI platforms, our AI platforms identifying images they produce as nudifications.<br><br>As AI continues to develop capability to produce more realistic media, it is difficult to provide a list of methods to spot materials that have been created using AI which will not rapidly become outdated; platforms providing AI media generation are attempting to make the materials they produce indistinguishable from media made by humans. <br><br>At the same time, organisations create tools designed to help identify AI-generated content, and publish resources listing the latest tells and identifiers for AI-generated content. Analysts may look for such tools or resources to help identify the use of AI.

**Tactic**: TA06 Develop Content <br><br>**Parent Technique:** T0166 AI-Generated Content


| Associated Technique | Description |
| --------- | ------------------------- |
| [T0161.004 Imagery Depicting Individual Edited to Introduce Sexual Material](../../generated_pages/techniques/T0161.004.md) | Where T0166.003: AI-Nudified Imagery documents the use of AI to convert an image of a clothed individual into the same image of them but unclothed, T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material documents manual edits (i.e. not made using AI) which splice images or videos depicting an individual into existing sexual media (including another person’s unclothed body). |
| [T0162.006 AI-Generated Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.006.md) | T0166.003: AI-Nudified Imagery which has been published alongside material which implies or directly claims that the T0166.003: AI-Nudified Imagery is actually a depiction of the real world (i.e. captured by a camera, a recording device, or otherwise manually produced by a human without the use of AI) should also be documented using T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality.  |
| [T0166.002 Sexual Deepfake Impersonation](../../generated_pages/techniques/T0166.002.md) | Where AI-nudification digitally alters an existing photo or video to produce a fabricated nude; T0166.002: Sexual Deepfake Impersonation creates falsified sexual imagery or video of a targeted individual imposing them into sexual source material, or creating entirely synthesized sexual content. |



| Incident | Descriptions given for this incident |
| -------- | -------------------- |
| [I00251 The Deepfake Porn Crisis Is Here](../../generated_pages/incidents/I00251.md) | <i>In June 2024, Guistolise was having dinner with friends when she received a call that changed everything. It was Samantha*, the ex-wife of Guistolise’s friend Ben, whom she had known for many years. Samantha said she needed to meet. Immediately.<br><br>When they spoke, Samantha shared with Guistolise that Ben had secretly been making deepfake pornographic images and videos of women, including Guistolise, using a “nudify” app, which takes real images and turns them into naked photos or videos using AI (T0166.003: AI-Nudified Imagery). There weren’t just a few images—there were hundreds, depicting 80 different women. And that’s just what Samantha was able to catalog and capture of what Ben had made over the course of a single week.<br><br>"It was me, but it wasn’t me,“ Guistolise tells Glamour about how she felt when she first saw the images. “It was so bizarre and dystopian and confusing.”<br><br>After learning about the deepfakes, Guistolise and her friends got to work identifying as many of the women as they could. Some of those women, in turn, helped identify others.<br><br>Each deepfake was created using screengrabs from the woman’s social media channels. Nudify apps (T0154.002: AI Media Platform) like the one Ben used typically only need one image of a person’s face to create a deepfake image. For Guistolise, it was from a photo taken at her goddaughter’s graduation and another from a family vacation, which she had posted to Facebook. This is a problem mostly faced by women, as the apps used to make nudify images aren’t trained to create images of the male form.<br><br>A year later, the women still haven’t been able to identify every single person in the images. They also haven’t seen any justice—the police officers they contacted said there is little they can do, even though Ben admitted to creating the images. And that’s because the laws around deepfake porn remains murky at best.<br><br>“This was not really something that had been on our radar,” says Omny Miranda Martone, the founder and CEO of the Sexual Violence Prevention Association (SVPA). “We’d been working on child abuse, sexual violence, and rape. We had not been working on any digital sexual violence until about three years ago.”<br><br>That changed when a victim came to them, sharing a story of how a rejected date at the gym turned into a man creating a deepfake of her and sharing it with her entire fitness community. When Martone asked the organization’s lawyers if there was more the survivor could do to protect herself, their answer was a resounding, “Not really.”<br><br>According to the lawyers, the act of creating that image and showing it to others didn’t constitute defamation because the man hadn’t been trying to pass it off as real. The victim wasn’t underage. And there was no law yet requiring social media platforms to take it down if he decided to distribute it. Still, SVPA knew that this was sexual violence. So Martone, their organization, and a few lawmakers got to work.<br><br>In May, President Donald Trump signed the Take It Down Act, a bipartisan bill introduced by senators Ted Cruz (R-Tex.) and Amy Klobuchar (D-Minn.) and endorsed by organizations like Martone’s and the Rape, Abuse & Incest National Network (RAINN). The bill “prohibits the nonconsensual online publication of intimate visual depictions of individuals, both authentic and computer-generated,” and, critically, will require platforms like Instagram and Meta to take down the images if they receive notice of their existence within 48 hours.<br><br>“That one was a really big win,” Martone says of the bill, which goes into effect in 2026. Stefan Turkheimer, the vice president for public policy at RAINN, noted that this law protects everybody, whether or not the images are authentic or fake. It also doesn’t matter whether they were taken consensually or nonconsensually, as long as they were shared nonconsensually.<br><br>But this is where things get complicated: This law protects people against the dissemination of deepfakes or intimate images shared without their consent. There is little to nothing someone can do about a person—be it someone they know or a total stranger—making and keeping the deepfakes for themselves.<br><br>“It’s not enough,” says Guistolise of the Take It Down Act. While it’s certainly a step forward, Guistolise says it still requires people to find the content, which she likens to trying to “find a needle in a needle stack.” If and when victims do find it and report it, they must then wait 48 hours for it to be removed, revictimizing themselves in the process.<br><br>“Searching for your own fake porn is not something, mental-health-wise, I’m willing to do,” Guistolise says. And again, this is only if the perpetrator distributes images in the first place. What Guistolise and many advocates want, instead, is to make the technology to create these deepfake images illegal in the first place.<br><br>“There are all kinds of things that have been said about it, like, ‘We don’t wanna block innovation,’” Guistolise says. “But women’s and children’s bodies are being sacrificed on the altar of capitalism. This is not innovation. This is exploitation. There is no reason for it to exist other than making money off of apps.”<br><br>It’s an issue that is absolutely exploding. In 2023, Time reported on an analysis by Graphika that found 24 million people had visited “undressing websites” in the month of September alone. These nudify apps have also been advertised everywhere from Google to Reddit and across social platforms. In July, Wired reported that the apps are worth about $36 million thanks to paid users and free users, who pony up their personal data that is later sold to the highest bidder.<br><br>There is, however, a little hope on the horizon. Representative Alexandria Ocasio-Cortez (D-N.Y.) along with Representative Laurel Lee (R-Fla.), Senator Richard J. Durbin (D-Ill.), and Senator Lindsey Graham (R-S.C.) has reintroduced another bill known as the Disrupt Explicit Forged Images and Nonconsensual Edits Act (DEFIANCE Act). If passed, it will give victims the right to bring a civil action against “individuals who knowingly produce, distribute, solicit, and receive or possess with the intent to distribute nonconsensual sexually explicit digital forgeries.” This means victims would have the right to sue for damages, and it’s supported by both RAINN and the SVPA.<br><br>“We are reintroducing the DEFIANCE Act to grant survivors and victims of nonconsensual deepfake pornography the legal right to pursue justice,” Ocasio-Cortez said in a statement. “I am proud to lead this legislation with Representative Lee, and senators Durbin and Graham, to provide victims with the federal protections they deserve.”<br><br>Elsewhere in the world, nations like Denmark are hoping to help protect people by passing legislation that gives everyone the copyright to their own likeness, including their voice. Our counterparts at Glamour UK lobbied tirelessly to change the law in the United Kingdom; it's now a criminal offense to create sexually explicit digital forgeries.<br><br>In Minnesota, Guistolise and the women in her group are working alongside state senator Erin Maye Quade and the state’s Senate Judiciary and Public Safety Committee on a potential bill that would outlaw nudification, requiring AI companies to disable the function that allows it to create the images, or fine them up to $500,000 for each nonconsensual deepfake.<br><br>“For these AI-generated photos and videos, the harm begins at creation,” Quade told MPR News. “Dissemination currently in Minnesota of nonconsensual, sexual deepfakes is illegal. But they can download these apps on their phones, and they’re doing that. They’re nudifying their teachers, their classmates, their siblings, friends.”<br><br>These laws, however common sense they may feel, will still face an upward battle. In May the Elon Musk–owned platform X sued the state of Minnesota over its law banning the creation of deepfakes to influence an election, which it said violated free speech. In August, Musk won a similar lawsuit against the state of California for its deepfake ban.<br><br>For Guistolise, this event has caused immeasurable pain. She’s lost trust in others. She’s afraid of how this may affect her future and her career. However, there is a “next” for her. She gets to go on being the sister and friend she’s always been. She’s excited to go to work tomorrow. She’s training her pitbull puppy, whom she appropriately named Olivia Benson, how to give a high five. And despite it all, “I love humans,” she says, before pausing. “I guess I still do.”</i> |



| Counters | Response types |
| -------- | -------------- |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW