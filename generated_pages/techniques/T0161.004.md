# Technique T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material

**Summary**: Threat Actors have been observed harassing individuals by producing edits of them which introduce their visage to sexual content. This can have the impact of harming their reputation, or inducing feelings of shame or fear, and in some cases can risk physical violence against the target. While this technique can be used to target anyone, it is often levelled at women. <br><br>This Sub-Technique covers cases where images or videos depicting individuals are spliced into sexual media, or images or videos depicting them are edited to introduce sexual elements.<br><br>To make this assertion, analysts may identify the source imagery of the targeted individual (i.e. which media their visage was taken from) and confirm it didn’t originally contain sexually explicit content. They may also look for editing artefacts (e.g. mismatched lighting, poor cropping or blurring, or unnatural angles where the visage has been introduced to the sexual scene).

**Tactic**: TA06 Develop Content <br><br>**Parent Technique:** T0161 Falsified Content


| Associated Technique | Description |
| --------- | ------------------------- |
| [T0162.008 Context Reframed by Edits to Media](../../generated_pages/techniques/T0162.008.md) | Undisclosed edits to sexual media framing it as legitimately depicting an individual should be documented alongside T0162.008: Context Reframed by Edits to Media. |
| [T0166.002 Sexual Deepfake Impersonation](../../generated_pages/techniques/T0166.002.md) | Where T0166.002: Sexual Deepfake Impersonation documents the use of AI to create a new image or video depicting an individual in an entirely fabricated sexual situation, T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material documents manual edits (i.e. not made using AI) which splice images or videos depicting an individual into existing sexual media. |
| [T0166.003 AI-Nudified Imagery](../../generated_pages/techniques/T0166.003.md) | Where T0166.003: AI-Nudified Imagery documents the use of AI to convert an image of a clothed individual into the same image of them but unclothed, T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material documents manual edits (i.e. not made using AI) which splice images or videos depicting an individual into existing sexual media (including another person’s unclothed body). |



| Incident | Descriptions given for this incident |
| -------- | -------------------- |
| [I00249 ‘It’s Disgusting’: Rosalía Fires Back at Artist Who Shared Photoshopped Nude Photos of Her](../../generated_pages/incidents/I00249.md) | <i>[Spanish musician] Rosalía vented her frustration with Spanish artist JC Reyes after he posted photoshopped images of her naked on social media, calling out the musician for not asking for consent and “creating a false narrative when I don’t even know you.” (T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material)<br><br>[...]<br><br>According to screengrabs circulating on social media, Reyes, or someone with access to his Instagram account, shared the photographs on his Stories. They appeared to be altered versions of photos Rosalía had originally taken and shared of herself (T0165: Edited Content, T0084.002: Plagiarise Content).<br><br>[...]<br><br>While the photos have since been removed from Reyes’ Instagram Stories, he seemed to boast about them — and suggest Rosalía had sent them to him — in a subsequent live video.</i> |



| Counters | Response types |
| -------- | -------------- |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW