# Technique T0162.001: Incorrect Subtitled Speech Reframes Context

**Summary**: Subtitles added to content to support viewers in understanding what was said by an individual can misrepresent what was said, which is difficult for viewers to detect when watching a speaker of another language, or videos without an audio track.<br><br>To make this assertion, analysts need to confirm that what was said was meaningfully different to the subtitles provided for the speech (i.e. not nit-picking minor translation issues, but changes which make changes to the context of what was said).<br><br>Analysts may require bilingual speakers to identify mistranslations. They can also search for the original media to search for original subtitles.

**Tactic**: TA14 Develop Narratives <br><br>**Parent Technique:** T0162 Reframe Context


| Associated Technique | Description |
| --------- | ------------------------- |



| Incident | Descriptions given for this incident |
| -------- | -------------------- |
| [I00064 Tinder nightmares: the promise and peril of political bots](../../generated_pages/incidents/I00064.md) | <i>In the days leading up to the UK‚Äôs general election (T0068: Respond to Breaking News Event or Active Crisis), youths looking for love online encountered a whole new kind of Tinder nightmare. A group of young activists built a Tinder chatbot to co-opt profiles and persuade swing voters to support Labour. The bot accounts sent 30,000-40,000 messages to targeted 18-25 year olds in battleground constituencies like Dudley North, which Labour ended up winning by only 22 votes.<br><br>The tactic was frankly ingenious. Tinder is a dating app where users swipe right to indicate attraction and interest in a potential partner. If both people swipe right on each other‚Äôs profile, a dialogue box becomes available for them to privately chat. After meeting their crowdfunding goal of only ¬£500, the team built a tool which took over and operated the accounts of recruited Tinder-users. By upgrading the profiles to Tinder Premium, the team was able to place bots in any contested constituency across the UK (T0097.101: Local Persona). Once planted, the bots swiped right on all users in the attempt to get the largest number of matches and inquire into their voting intentions.<br><br>Yara Rodrigues Fowler and Charlotte Goodman, the two campaigners leading the informal GE Tinder Bot team, explained in a recent opinion piece that if ‚Äúthe user was voting for a right-wing party or was unsure, the bot sent a list of Labour policies, or a criticism of Tory policies,‚Äù (T0136.006: Cultivate Support for Ally) with the aim ‚Äúof getting voters to help oust the Conservative government.‚Äù<br><br>Pieces in major media outlets like the New York Times and BBC have applauded these digital canvassers for their ingenuity and civic service. But upon closer inspection, the project reveals itself to be ethically dubious and problematic on a number of levels. How would these same outlets respond if such tactics were used to support the Tories? And what does this mean for the use of bots and other political algorithms in the future?<br><br>The activists maintain that the project was meant to foster democratic engagement. But screenshots of the bots‚Äô activity expose a harsher reality. Images of conversations between real users and these bots, posted on i-D, Mashable, as well as on Fowler and Goodman‚Äôs public Twitter accounts, show that the bots did not identify themselves as automated accounts, instead posing as the user whose profile they had taken over (T0146.007: Automated Account Asset, T0150.005: Compromised Asset, T0145.005: Compromised Persona, T0097.109: Romantic Suitor Persona). While conducting research for this story, it turned out that a number of our friends living in Oxford had interacted with the bot in the lead up to the election and had no idea that it was not a real person.</i> |
| [I00066 The online war between Qatar and Saudi Arabia](../../generated_pages/incidents/I00066.md) | <i>In the early hours of 24 May 2017, a news story appeared on the website of Qatar's official news agency, QNA, reporting that the country's emir, Sheikh Tamim bin Hamad al-Thani, had made an astonishing speech (T0152.004: Website Asset, T0150.005: Compromised Asset, T0097.202: News Outlet Persona, T0145.005: Compromised Persona).<br><br>The quotes then appeared on the QNA's social media accounts (T0146: Account Asset, T0150.005: Compromised Asset, T0097.202: News Outlet Persona, T0145.005: Compromised Persona) and on the news ticker running along the bottom of the screen on videos uploaded to the agency's YouTube channel (T0162.002: Edits Made to News Report which Reframe Context).<br><br>The emir was quoted praising Islamist groups Hamas, Hezbollah and the Muslim Brotherhood. And perhaps most controversially of all, Iran, Saudi Arabia's arch-rival (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>But the story soon disappeared from the QNA website, and Qatar's foreign ministry issued a statement denying the speech had ever taken place. No video footage has ever emerged of the emir actually saying the words supposedly attributed to him.<br><br>Qatar claimed that the QNA had been hacked. And they said the hack was designed to deliberately spread fake news about the country's leader and its foreign policies. The Qataris specifically blamed UAE, an allegation later repeated by a Washington Post report which cited US intelligence sources. The UAE categorically denied those reports.<br><br>But the story of the emir's speech unleashed a media free-for-all. Within minutes, Saudi and UAE-owned TV networks - Al Arabiya and Sky News Arabia - picked up on the comments attributed to al-Thani. Both networks accused Qatar of funding extremist groups and of destabilising the region.</i> |
| [I00071 Russia-aligned hacktivists stir up anti-Ukrainian sentiments in Poland](../../generated_pages/incidents/I00071.md) | <i>On August 16, 2022, pro-Kremlin Telegram channel Joker DPR (–î–∂–æ–∫–µ—Ä –î–ù–†) published a forged letter allegedly written by Ukrainian Foreign Minister Dmytro Kuleba (T0085.004: Develop Document, T0161.001: Impersonated Content, T0097.111: Government Official Persona). In the letter, Kuleba supposedly asked relevant Polish authorities to rename Belwederska Street in Warsaw ‚Äî the location of the Russian embassy building ‚Äî as Stepan Bandera Street, in honor of the far-right nationalist who led the Ukrainian Insurgent Army during WWII.<br><br>More than sixty years after his death, Bandera remains a polarizing figure. While still highly regarded in far-right Ukrainian nationalist circles, he is largely remembered for his role in the massacres of Polish civilians and collaborating with Nazi officials. In Russia, the word Banderite has become synonymous with ‚Äúfascist‚Äù or even ‚ÄúNazi.‚Äù During the early months of the war in Ukraine, Kremlin propaganda spoke of ‚Äúde-nazifying‚Äù Ukraine and removing its democratically elected government, which it referred to contemptuously as ‚ÄúBanderite.‚Äù<br><br>The forged letter claimed that renaming the street after Bandera would be seen as a gesture of support to Ukraine, and highlighted that Russia changed the names of the streets in Moscow where the US and UK embassies are located. The letter is not dated, and Dmytro Kuleba‚Äôs signature seems to be copied from a publicly available letter signed by him in 2021.<br><br>The day after releasing the letter, Joker DPR published another document on Telegram, allegedly signed by Polish Deputy Foreign Minister Marcin Przydacz (T0085.004: Develop Document, T0161.001: Impersonated Content, T0097.111: Government Official Persona). The document contained several orders supposedly issued by Przydacz, including an order for the President of the Polish Institute of National Remembrance to provide a written expert opinion by August 31 about the possibility of changing the street in Warsaw to Stepan Bandera Street. It also proposed to conduct a publicity campaign to improve Bandera‚Äôs popularity among Polish citizens, despite his role in WWII-era Polish massacres.<br><br>Deputy Foreign Minister Przydac wrote on Twitter that the document published under his name was forged and that no one at the Polish Foreign Ministry had written such a letter. ‚ÄúIt‚Äôs a fake,‚Äù he tweeted. ‚ÄúNever would such a magazine be created by the Polish Ministry of Foreign Affairs. The linguistic errors clearly point to the potential authors of this provocation.‚Äù<br><br>The August 17 Telegram post also contained screenshots of Facebook posts that appeared on two Facebook accounts belonging to Polish nationals Piotr G√≥rka, an expert in the history of the Polish Air Force, and Dariusz Walusiak, a Polish historian and documentary maker. The G√≥rka post suggested that he fully supported the Polish government‚Äôs decision to change Belwederska Street to Stepan Bandera Street.<br><br>In a statement to the DFRLab, G√≥rka said his account was accessed without his consent (T0150.005: Compromised Asset, T0145.005: Compromised Persona, T0097.101: Local Persona, T0097.108: Expert Persona). ‚ÄúThis is not my post loaded to my Facebook page, (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)‚Äù he explained. ‚ÄúMy site was hacked, some days ago.‚Äù At the time of publishing, Piotr G√≥rka‚Äôs post and his Facebook account were no longer accessible.<br><br>[...]<br><br>The fact that Joker DPR‚Äôs Telegram post included screenshots of their Facebook posts raises the strong possibility that both Facebook accounts were compromised, and that hackers planted false statements on their pages that would seem out of character for them in order to gain further attention to the forged documents (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).</i> |
| [I00129 Teen who hacked Joe Biden and Bill Gates' Twitter accounts sentenced to three years in prison](../../generated_pages/incidents/I00129.md) | <i>An 18-year-old hacker who pulled off a huge breach in 2020, infiltrating several high profile Twitter accounts to solicit bitcoin transactions, has agreed to serve three years in prison for his actions.<br><br>Graham Ivan Clark, of Florida, was 17 years old at the time of the hack in July, during which he took over a number of major accounts including those of Joe Biden, Bill Gates and Kim Kardashian West.<br><br>Once he accessed them, Clark tweeted a link to a bitcoin address and wrote ‚Äúall bitcoin sent to our address below will be sent back to you doubled!‚Äù (T0146.003: Verified Account Asset, T0150.005: Compromised Asset, T0145.005: Compromised Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution) According to court documents, Clark made more than $100,000 from the scheme, which his lawyers say he has since returned.<br><br>Clark was able to access the accounts after convincing an employee at Twitter he worked in the company‚Äôs information technology department, according to the Tampa Bay Times.<br><br>The incident represented a major security breach for the social network, which was forced to suspend all verified accounts from sending tweets for several hours while it worked to secure the platform. Clark, who was at the time a senior in high school, was arrested several days later.</i> |
| [I00130 Images of a ‚ÄòPalestinian girl‚Äô being rescued were taken in Syria in 2016](../../generated_pages/incidents/I00130.md) | <i>Posts on Facebook and X (formerly Twitter) sharing images of a girl being rescued claim: ‚ÄúThis Palestinian girl is saved by 3 different people from 3 different locations on 3 different days and all locations are 50 KM apart from each other. Wondering why she keeps travelling so far especially in the conflict zone?‚Äù<br><br>It‚Äôs not entirely clear what these posts are suggesting. We have not seen any reports that could misleadingly suggest the same Palestinian girl has been rescued on three separate occasions. And if we take the posts literally, they are not true, because we can say for certain that these images come from Aleppo, Syria (T0162.004: Content Incorrectly Presented as Depicting Another Location), in the aftermath of a bombing that took place on 27 August 2016 (T0162.003: Historic Content Incorrectly Presented as Current).<br><br>They do not come from the current conflict in Israel and Gaza, and do not show rescues taking place in Gaza.<br><br>Reverse image searches of each photo show that they were all taken on the same day, at the same location, and show the same girl being passed between different rescuers (T0162: Reframe Context).<br><br>The first photo featured in coverage of the Aleppo bombing by the Daily Mail and The Sun. The second featured in reports on the same bombing by NBC News, while the third was published alongside a report on the bombing by Arab Times, and also appears in an ABC News report on the impact of the war in Syria on children, with a caption stating that it was taken at the 27 August 2016 bombing.<br><br>Social media posts about these images were previously fact checked by Snopes in 2016 (T0160.006: Content Previously Fact Checked), and Africa Check in 2019 (T0160.006: Content Previously Fact Checked) after it was claimed that CNN had used them to illustrate three different refugee crises.</i> |
| [I00131 Video shows scenes in Algiers, not bombing in Gaza](../../generated_pages/incidents/I00131.md) | <i>A viral video is being shared on social media with the claim it shows the Gaza Strip following the recent conflict between Israel and Hamas. But it actually comes from Algiers (T0162.004: Content Incorrectly Presented as Depicting Another Location) and predates events in October 2023 (T0162.003: Historic Content Incorrectly Presented as Current). The clip is likely to actually show celebrations from football fans (T0162: Reframe Context).<br><br>The video shows a skyline illuminated in red with thousands of small fires and the sound of explosions. It has been shared across social media platforms, including Facebook, X (formerly Twitter) and TikTok with the claim it shows the Gaza Strip.<br><br>Multiple posts have the caption: ‚ÄúIf Russia did this in Kiev it would be all over the news and everyone would be screaming "genocide", but it's happening in Gaza and no one cares about the civilian casualties.‚Äù Another post claims it is an ‚Äúold video from Gaza‚Äù. <br><br>However, the video does not show either current or past events in the Gaza Strip (T0160.002: Information is False). The lights and explosions appear to be fireworks and flares.<br><br>Journalists at Bellingcat located the clip to Algiers using a roundabout and billboard that can be seen in the background of the footage, which has also been verified by fact checkers at Reuters. The video was also reportedly first posted to TikTok on 28 September‚Äîbut has since been deleted‚Äîand was reshared on YouTube that same day, predating the recent conflict that began on 7 October. <br><br>While the specific context of the clip is not absolutely clear, footage showing similar scenes was posted online on 8 August 2023 in celebration of the 102nd anniversary of Algiers-based football club Mouloudia Alger. There are several other videos showing Algerian football fans celebrating with bright red fireworks on many different occasions dating as far back as 2014.</i> |
| [I00132 Video of ‚Äòcorpse running away‚Äô is not related to Israel conflict](../../generated_pages/incidents/I00132.md) | <i>A video of an apparent funeral procession where what appears to be a corpse suddenly runs away when an alarm rings out is being shared with a misleading claim that it shows people ‚Äúscared of Israel‚Äôs drones‚Äù.<br><br>During the video, a group appear to be carrying a shrouded body, which they put down and run away from when a siren sounds. The ‚Äòdead body‚Äô then also gets up and runs off, to laughter from the person filming.<br><br>The footage has been shared with the caption: ‚ÄúEven dead are scared of Israel‚Äôs Drones [sic].‚Äù That suggests the video shows people being targeted by Israeli drones, possibly in Gaza or Lebanon given current events in the Middle East. But this is not the case (T0160.002: Information is False).<br><br>The video was actually filmed in Jordan (T0162.004: Content Incorrectly Presented as Depicting Another Location) in 2020 (T0162.003: Historic Content Incorrectly Presented as Current), during the Covid-19 pandemic. It was reported by an Arabic-language news channel on 25 March 2020, and the same video was also published on Facebook on 23 March 2020 by a Jordanian news outlet.<br><br>[...]<br><br>Although we‚Äôve not been able to verify for certain what the video shows, it was filmed in the early days of the pandemic when strict Covid-19 rules were in place. Lockdown restrictions were implemented nationwide in Jordan in March 2020, which included an army-enforced curfew and closures of borders with neighbouring countries.<br><br>The fact that the video includes a supposed corpse moving also appears to echo a narrative, often referred to as ‚ÄòPallywood‚Äô (a portmanteau of Palestine and Bollywood), which is used by some online to describe the alleged false staging of harm to Palestinian civilians (T0162: Reframe Context). <br><br>Full Fact has debunked a number of videos and photos that were posted with misleading claims they showed Palestinians pretending to be dead or injured during the ongoing Israel-Gaza conflict.<br><br>This video has been previously debunked by other fact checking organisations (T0160.006: Content Previously Fact Checked), when it was claimed to show various different instances of Palestinians faking injuries, as far back as 2021.</i> |
| [I00133 ‚ÄòDead Gazan body miraculously texting‚Äô is actually Thai child in Halloween costume](../../generated_pages/incidents/I00133.md) | <i>Multiple images shared on social media appear to show a child wearing a body bag using a phone, with captions falsely (T0160.002: Information is False) suggesting the person is in Gaza (T0162.004: Content Incorrectly Presented as Depicting Another Location) and has been pretending to be dead for the cameras (T0162: Reframe Context). <br><br>Posts sharing the photo make claims such as: ‚ÄúOne of the Gazans killed brutally by #Israel is texting from another world. #Gaza in need of actors‚Äù. Another claims the picture shows ‚Äúa ‚Äòdead‚Äô Gazan body miraculously texting‚Äù.<br><br>But the image was not taken recently (T0162.003: Historic Content Incorrectly Presented as Current), or in Gaza (T0162.004: Content Incorrectly Presented as Depicting Another Location). The first example we could find of the image was posted on Facebook in October 2022 by a mother who dressed her children up for a Halloween costume contest in Thailand. Other images of the child show them in their full costume, including a cardboard coffin. The post has over 21,000 shares.<br><br>The pictures were published by multiple Thai-language outlets, which describe (translated by Google) that the child in the white shroud did not place in the competition, but that their sibling came third.<br><br>Some of the posts claiming the picture shows a Gazan ‚Äúactor‚Äù mention the word ‚ÄúPallywood‚Äù or ‚ÄúPalliwood‚Äù. This is a portmanteau of Palestine and Bollywood, first coined over a decade ago by historian Richard Landes to describe what he claims as the ‚Äústaging of scenes by Palestinian journalists in order to present the Palestinians as hapless victims of Israeli aggression‚Äù.<br><br>The term has recently seen a resurgence online to caption videos often incorrectly claiming to show a concerted effort by those in Gaza to fake images of harm to civilians as part of the ongoing conflict in Israel and Gaza. We‚Äôve seen such captions on AI-generated images of people in Gaza, miscaptioned photos from other places, or videos falsely claiming to show actors pretending to be dead or injured. <br><br>It‚Äôs not always clear how these images came to be misrepresented, who was responsible and whether this was done on purpose rather than inadvertently.</i> |
| [I00134 Video does not show Palestinians pretending to be dead](../../generated_pages/incidents/I00134.md) | <i>A video circulating on Facebook and X (formerly Twitter) showing a row of people lying under what appear to be shrouds, while occasionally moving, has been shared alongside captions suggesting these are Palestinians pretending to have died in the recent Israel-Gaza conflict.<br><br>However this footage is actually from a 2013 (T0162.003: Historic Content Incorrectly Presented as Current) student protest (T0162: Reframe Context) in Cairo, Egypt (T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>In the video, the camera pans over multiple people lying in a row under what appear to be white shrouds, which have messages written on them in Arabic. Over the course of the video, multiple people can be seen to move under the shrouds. Someone off-camera removes the cloth from the face of a man who appears to laugh. <br><br>The text of the video shared on Facebook says ‚ÄúWake up world/They think you are fools! The masters of fake news/ Welcome the palestinians [sic] & their Hamas‚Äù (T0162: Reframe Context), while the caption states ‚ÄúDude, stop moving‚Äù and is accompanied by the hashtags ‚Äú#standwithisrael‚Äù, ‚ÄúHamasisISIS‚Äù and ‚Äúpallywood‚Äù. (T0015.001: Use Existing Hashtag)<br><br>This latter phrase is a portmanteau of Palestine and Bollywood used by some to describe the alleged false staging of images or videos to portray Palestinian victimhood. Full Fact has written about this term before after it was included in several misleading posts online. <br><br>This video was not taken recently (T0162.003: Historic Content Incorrectly Presented as Current), or in Gaza or the West Bank (T0162.004: Content Incorrectly Presented as Depicting Another Location), and does not show Palestinians. <br><br>The watermark in the top right corner of the video says ‚ÄòAl-Badil‚Äô in Arabic, which was an Egyptian news service. The same video and watermark is evident in this original version of the video, which was uploaded to the organisation‚Äôs YouTube channel on 28 October 2013. <br><br>The caption (translated with Google) says the video shows Muslim Brotherhood students at Al-Azhar University in Cairo. In October 2013, students at the university undertook multiple protests, demanding the reinstatement of Mohammed Morsi as President of Egypt after he was deposed by the military in July 2013. <br><br>This video has been used multiple times to falsely claim (T0160.002: Information is False) that deaths are being staged in various conflicts (T0162: Reframe Context): in 2016, the title of this video suggests the people are ‚Äúvictims‚Äù of ‚ÄúRussian bombings‚Äù faking their deaths; in 2017 and 2018 social media posts suggested this video showed apparent ‚ÄúVictims of [Syrian President] Assad‚Äôs chemical attack‚Äù or ‚Äúliving corpses in Syria after the poison gas attack‚Äù; and in 2021 it was claimed that the video showed Palestinians ‚Äúposing‚Äù as ‚Äúcorpses‚Äù (T0160.006: Content Previously Fact Checked).</i> |
| [I00135 Hamas Attacks, Israel Bombs Gaza and Misinformation Surges Online](../../generated_pages/incidents/I00135.md) | <i>Misinformation has been spreading rapidly online ever since Hamas launched a surprise attack on Israel on October 7. At least 1,000 people were killed and the Islamist militant group took between 100 and 150 hostages. Israel retaliated, threatening a complete siege of Gaza and hitting the area with multiple airstrikes, killing more than 900 people. <br><br>[...]<br><br>During breaking news events, there is often uncertainty about the claims we see on social media. As we discuss in our 2021 guide, determining when a video was filmed is an essential part of debunking dis- and misinformation. <br><br>We have identified multiple examples of footage being reused from past conflicts and military events as evidence of current events in Israel-Palestine (T0162.003: Historic Content Incorrectly Presented as Current, T0160.002: Information is False).  <br><br>For example, the video below, filmed at night, depicts multiple rockets being fired in quick succession, which is being circulated with the claim that they were fired at Israel by Hamas. While the video shows military activity, it does not show its impact or target.<br><br>[Here, Bellingcat provides a collection of three screenshots, two showing posts made to TikTok (T0151.008: Microblogging Platform), the first captioned ‚ÄúBREAKING NEWS Hamas just Fired hundreds of rockets towards israel iron dome stopped working #IsraelPalestineWar‚Äù (T0068: Respond to Breaking News Event or Active Crisis), the second captioned ‚ÄúIsrael under attack‚Ä¶ #israel #hamas #usaüá∫üá∏ #nowar #warzone‚Äù. The third screenshot is a post made to X by a verified account controlled by the leader of Britain First, a political party and hate group (T0146.003: Verified Account Asset, T0097.110: Party Official Persona, T0143.001: Authentic Persona). The post contains the video with the text ‚ÄúRockets being fired at the Israel capital Tel Aviv‚Äù]<br><br>However, the video is at least three years old, as it was uploaded to YouTube in February 2020 by a Russian-speaking account whose name translates as ‚ÄòWorld on the Brink.‚Äô </i>, <i>Misinformation has been spreading rapidly online ever since Hamas launched a surprise attack on Israel on October 7. At least 1,000 people were killed and the Islamist militant group took between 100 and 150 hostages. Israel retaliated, threatening a complete siege of Gaza and hitting the area with multiple airstrikes, killing more than 900 people. <br><br>[...]<br><br>Misinformation can also be spread using imagery that doesn‚Äôt show conflict or violence. There are numerous examples of videos being decontextualised from their original source to appear relevant to the current Israel-Hamas attacks (T0162: Reframe Context). <br><br>For example, there is a video that depicts multiple paragliders with a sunset in the background. There are some palm trees and a building shown in the video. Accounts are claiming these are Hamas soldiers paragliding into Israel.<br><br>However, this video does not take place in Israel. Reuters was able to geolocate this video to Egypt. The building shown in the video is a military academy and can be found at (30.108686, 31.358481), in Cairo‚Äôs El Nozha district (T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>We were able to locate an older version of this video shared on TikTok on 24 September 2023 (T0162.003: Historic Content Incorrectly Presented as Current). The coat of arms of this institution can be seen in the foreground of this original footage ‚Äî though in the misleading TikTok video claiming it was filmed in Israel, this detail are obscured by the caption in red, while the entire building has been cropped out from the version shared on X (T0162.008: Context Reframed by Edits to Media, T0165.002: Cropped Content).</i>, <i>Misinformation has been spreading rapidly online ever since Hamas launched a surprise attack on Israel on October 7. At least 1,000 people were killed and the Islamist militant group took between 100 and 150 hostages. Israel retaliated, threatening a complete siege of Gaza and hitting the area with multiple airstrikes, killing more than 900 people. <br><br>[...]<br><br>During breaking news events, there is often uncertainty about the claims we see on social media. As we discuss in our 2021 guide, determining when a video was filmed is an essential part of debunking dis- and misinformation. <br><br>We have identified multiple examples of footage being reused from past conflicts and military events as evidence of current events in Israel-Palestine.  <br><br>[...]<br><br>On the morning of October 9, the X account of pro-government Syrian newspaper Al-Watan (T0097.202: News Outlet Persona, T0143.001: Authentic Persona) published footage of explosions in a cityscape with the words ‚Äòscenes of the Israeli bombing of Gaza city‚Äô in Arabic (T0068: Respond to Breaking News Event or Active Crisis). Avichay Adraee, IDF Spokesperson for the Arab Media (T0097.110: Party Official Persona, T0143.001: Authentic Persona), posted a composite of three pieces of footage, the first of which also depicted this scene (T0068: Respond to Breaking News Event or Active Crisis).<br><br>Al-Watan has since deleted their tweet. At the time of writing, Adraee‚Äôs tweet is still available.<br><br>This footage was neither filmed in 2023 (T0162.003: Historic Content Incorrectly Presented as Current) nor was it filmed in Palestine (T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>When zooming in on the video, we noticed the distinctive shape of this schoolyard with a basketball court, highlighted in both images below in blue. It could easily be matched to satellite imagery of Ariha, a town in the Idlib Governorate of northern Syria (35.810781, 36.614178).<br><br>[...]<br><br>AirWars, an independent project to catalogue civilian harm caused by bombing campaigns in the Middle East, also investigated this incident. The website‚Äôs database entry incident R2349 details a June 2021 attack on the city of Ariha which includes additional information about this incident. AirWars shared a collage of imagery of the attack; the bottom left photo matches the footage shared earlier. </i> |
| [I00136 FALSE: Kim Atienza in critical condition after house robbery](../../generated_pages/incidents/I00136.md) | <i>Claim: Television host Kim Atienza is in critical condition after he was shot by robbers who intruded into his home. The incident was supposedly caught on security camera or CCTV.<br><br>The website gmatodayupdatesnews.blogspot.com posted an undated story with the headline, ‚ÄúKuya Kim Atienza Kritikal Ngayon matapos Barilin ng mga Magnanakaw sa Kanyang Bahay. Nakunan ng CCTV.‚Äù According to Facebook‚Äôs Claim Check dashboard, it was first shared around July 11, 2020. Another website, newsupdatestr3ndingtoday.blogspot.com, also posted a similar story and was first shared on Facebook on July 16.<br><br>The Claim Check dashboard also shows that the first website‚Äôs article has been reported by Facebook users 363 times as of posting.<br><br>The supposed CCTV video of the incident is embedded in these articles. However, only a few seconds of the video would play, after which users would be asked to share the article first before they could watch the rest of the video (T0120: Incentivise Sharing). <br><br>A Facebook user also shared the thumbnail or featured image of these articles on a public Facebook group on July 12. His post was shared only a few times, and received around 100 reactions and comments as of posting.<br><br>Rating: FALSE<br><br>The facts: The video used in the claim is unrelated to Atienza, who is alive as of posting. Meanwhile, the article‚Äôs thumbnail image misuses a photo of a medical operation done on him in 2010 (T0162.003: Historic Content Incorrectly Presented as Current).<br><br>The embedded video in these articles is about an entirely different robbery incident. The security footage, uploaded on March 28, 2014, by the YouTube channel Sunday Times Driving, is entitled, ‚ÄúCaught on CCTV: thieves steal Audi RS4.‚Äù It shows how thieves broke into a Sunday Times reader‚Äôs vehicle in around 90 seconds. Sunday Times Driving is the motoring section of The Sunday Times newspaper in the United Kingdom (T0162.003: Historic Content Incorrectly Presented as Current, T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>Meanwhile, the thumbnail image that shows Atienza undergoing an operation was actually taken in 2010. It was part of the photos Atienza posted on his Facebook page in November 2018 to mark the 8th anniversary of a heart surgery done on him ‚Äúto seal a hole‚Ä¶that caused a stroke.‚Äù<br><br>Atienza continues to provide weather updates every night at the TV Patrol newscast on cable and online platforms as of posting (T0160.002: Information is False).<br><br>After clicking on the link to open these articles about Atienza, users will actually be redirected to a different website (T0149.004: Redirecting Domain Asset), randomnames.club, which has hosted death hoaxes and false claims about famous personalities, newsmakers, and current events. These false claims had been rated as false in previous Rappler fact checks (T0160.007: Claim Previously Fact Checked).</i> |
| [I00137 Old cremation video in Vietnam unrelated to Taiwanese celebrity's death](../../generated_pages/incidents/I00137.md) | <i>After Taiwanese star Barbie Hsu died from pneumonia during a vacation in Japan over the Lunar New Year holiday, social media users falsely claimed a video shared thousands of times was filmed at her cremation ceremony in Japan. But the clip was taken in a Vietnamese crematorium in December 2024 and has no connection to Hsu (T0162.003: Historic Content Incorrectly Presented as Current, T0162.004: Content Incorrectly Presented as Depicting Another Location, T0068: Respond to Breaking News Event or Active Crisis). <br><br>"Barbie Hsu cremated in Japan today. It makes me cry for her children who were orphaned," reads the caption of a Facebook video shared on February 4 in a mix of English and Tagalog. <br><br>The 59-second video shows mourners -- including kids -- weeping as they watch a brown coffin being sent into a cremator. <br><br>Overlaid text on the video says: "It's heartbreaking, Barbie's children are still so young."<br><br>It was shared over 4,600 times before it was deleted and has spread on Instagram, Threads, YouTube and TikTok alongside similar claims. <br><br>Taiwanese star Barbie Hsu, who was hugely popular across Asia for her leading role in the 2001 television drama "Meteor Garden", died from a respiratory illness on February 2. She was 48 (T0068: Respond to Breaking News Event or Active Crisis). <br><br>Hsu passed away after developing pneumonia during a family trip to Japan over the Lunar New Year holiday, the younger of her two sisters was quoted as saying. <br><br>The urn containing Hsu's ashes was flown back to Taiwan from Tokyo's Haneda Airport on February 5, according to Singaporean news outlet The Straits Times.  <br><br>But the circulating video has nothing to do with her death (T0160.002: Information is False). <br><br>Taken in Vietnam<br><br>Keyword searches found the video surfaced on TikTok on December 29, 2024 -- months before Hsu's death (T0162.003: Historic Content Incorrectly Presented as Current). <br><br>At the four-second mark of the TikTok video, Vietnamese text can be seen on the television screen above the cremator's metal door. AFP's bureau in Vietnam confirmed the name of the deceased on the screen did not belong to Hsu. <br><br>Subsequent Google reverse image searches led to similar footage posted on TikTok on December 30, 2024 by a user who identifies himself as Pham Son Hai and a worker at the Hoa Lac Vien crematorium in Vietnam (T0162.004: Content Incorrectly Presented as Depicting Another Location). <br><br>It contains several elements seen in the false video, including the television screen, the cremation chamber number, the casket design and the yellow statuette on the table. <br><br>When shown the circulating Facebook video, Pham told AFP on February 11: "The user posted the wrong thing. This is the Hoa Lac Vien crematorium of Long Thanh, Dong Nai, Vietnam". </i> |
| [I00138 Images of Syrian Civil War Take on a Second Life in Gaza Conflict](../../generated_pages/incidents/I00138.md) | <i>[Bellingcat has] identified a trend of footage and images from Syria‚Äôs long-running civil war being recycled and misattributed to the [2023] Israel-Hamas conflict (T0162.003: Historic Content Incorrectly Presented as Current, T0162.004: Content Incorrectly Presented as Depicting Another Location). This phenomenon has also been reported by other outlets including The New York Times.  In particular, a number of prominent figures who have a history of denying the Syrian government‚Äôs chemical weapons attacks have re-shared Syrian civil war images, claiming they show Israel and Gaza. <br><br>[...]<br><br>The far-right US influencer Jackson Hinkle has quickly emerged as one of the most popular users on X during the ongoing Israel-Hamas war. [...] Some of the imagery Hinkle has claimed to show Palestine was actually taken in Syria. For example, on November 12, he shared an image of a woman holding a toy car descending stairs with exposed walls and damaged infrastructure. He captioned this image ‚ÄúYou CANNOT BREAK the Palestinian spirit.‚Äù (T0068: Respond to Breaking News Event or Active Crisis).<br><br>However, this photo was featured in the Siena International photography competition in 2020. It was taken by the Iranian photographer Hassan Ghaedi, who won an award for the image in the competition‚Äôs documentary and photojournalism category. According to AFP‚Äôs Fact Check, which cited Ghaedi‚Äôs social media accounts, the image was taken in 2016 (T0162.003: Historic Content Incorrectly Presented as Current) and shows a woman returning to a destroyed building in Homs, Syria (T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>On November 1, Olga Robinson of BBC Verify reported that Hinkle had shared a video he claimed to be footage of Gaza‚Äôs Al-Sadaqa Hospital being bombed by Israel (T0068: Respond to Breaking News Event or Active Crisis). This video depicted civilians in a hospital and a subsequent bombing and was also shared widely on X.<br><br>[...]<br><br>The clip shared by Hinkle, in fact, is an excerpt from a video that shows the bombing of the Omar Bin Abdul Aziz Hospital in Aleppo, Syria from the perspective of those inside (T0162.004: Content Incorrectly Presented as Depicting Another Location). As the Guardian reported at the time, this attack came during a 2016 (T0162.003: Historic Content Incorrectly Presented as Current) surge of bombing raids by Syrian regime forces and their Russian allies. The attack on the hospital was condemned by the World Health Organization (WHO).<br><br>The video is credited to the Aleppo Media Center (AMC), a Middle Eastern Independent documentary film production company. It is one of hundreds on AMC‚Äôs YouTube channel to document attacks in Syria by Syrian government forces. The relevant scene in the hospital can be seen from timestamp 00:40 (T0165.001: Clipped Content).<br><br>In the recent viral version of the video shared by Hinkle, the AMC logo had been cropped out (T0165.002: Cropped Content, T0165.004: Source Edited Out of Content).</i>, <i>[Bellingcat has] identified a trend of footage and images from Syria‚Äôs long-running civil war being recycled and misattributed to the [2023] Israel-Hamas conflict. This phenomenon has also been reported by other outlets including The New York Times.  In particular, a number of prominent figures who have a history of denying the Syrian government‚Äôs chemical weapons attacks have re-shared Syrian civil war images, claiming they show Israel and Gaza. <br><br>[...]<br><br>Even beyond Pro-Assad accounts, the misuse of Syria war footage has continued across X since the latest conflict erupted in early October. In particular, there have been several videos and images shared on X misappropriating the suffering of Syrian children to spread disinformation. <br><br>For example, on October 11 a video of a young boy covered in soot crying about the death of his family was posted to X. It racked up tens of thousands of retweets and likes and millions of views. Accounts claimed that he was a Palestinian child whose sisters were killed by Israel (T0068: Respond to Breaking News Event or Active Crisis).<br><br>However, this video was actually of a boy in Aleppo, Syria (T0162.004: Content Incorrectly Presented as Depicting Another Location. Like the hospital video Hinkle misleadingly shared, this video originated from the Aleppo Media Center‚Äôs YouTube page, where is was shared on April 11, 2014 (T0162.003: Historic Content Incorrectly Presented as Current). The boy can be seen from timestamp 01:40.<br><br>In the viral version of the video on X, the AMC logo in the top left-hand corner was cropped out (T0165.002: Cropped Content, T0165.004: Source Edited Out of Content). The video had been clipped down in length (T0165.001: Clipped Content), and its colour tones adjusted (T0165: Edited Content), which aided in obscuring the original source. This was also debunked in a post on X on October 12 by Sardarizadeh, the BBC Verify journalist (T0160.002: Information is False).<br><br>[...]<br><br>One egregious example of misuse of images of Syrian children came from Sulaiman Ahmed, a journalist (T0097.102: Journalist Persona, T0143.001: Authentic Persona) and influencer with over 270,000 followers on X.<br><br>On October 14, Ahmed posted ‚ÄúCHILD GENOCIDE IN PALESTINE 614 Palestinian children murdered by the Israeli IOF Forces‚Äù (T0068: Respond to Breaking News Event or Active Crisis) along with a picture of several dead children shrouded in white cloth, on the ground, with their faces visible.<br><br>However, this gruesome image is not from Gaza but actually from Damascus (T0162.004: Content Incorrectly Presented as Depicting Another Location). The photo was taken in August 2013 (T0162.003: Historic Content Incorrectly Presented as Current), showing the evidence of chemical weapons attacks by pro-Assad forces as confirmed through multiple sources. In multiple articles, this photo is attributed to the aftermath of a chemical attack in Syria.<br><br>[...]<br><br>Sulaiman Ahmed also shared a video on October 13 depicting children searching for food among rubble and burnt buildings. In the post on X, he claimed that these were children in Gaza after an Israeli airstrike (T0068: Respond to Breaking News Event or Active Crisis).<br><br>However, the original source of the video indicates otherwise (T0160.002: Information is False). The original creator‚Äôs TikTok handle can be seen in the video uploaded by Ahmed. Thanks to this we can see that this video was posted there on June 21, well before the start of the recent war (T0162.003: Historic Content Incorrectly Presented as Current).</i>, <i>[Bellingcat has] identified a trend of footage and images from Syria‚Äôs long-running civil war being recycled and misattributed to the [2023] Israel-Hamas conflict. This phenomenon has also been reported by other outlets including The New York Times. In particular, a number of prominent figures who have a history of denying the Syrian government‚Äôs chemical weapons attacks have re-shared Syrian civil war images, claiming they show Israel and Gaza. <br><br>[...]<br><br>On November 19, there were news reports of the death of two Palestinian journalists in Gaza, Hassouna Sleem and Sari Mansour.<br><br>Shortly thereafter, disinformation about murdered journalists appeared online. A video spread across X showing two men wearing bulletproof vests labelled ‚Äúpress‚Äù. The video begins with footage of rockets being fired behind the men before cutting to the same men unveiling rockets in the back of a truck. Users claimed that these were the two journalists killed in Gaza and that they were terrorists posing as journalists (T0068: Respond to Breaking News Event or Active Crisis). <br><br>Among the users who shared this claim are Bree A. Dail, a correspondent for the US conservative news site Daily Wire (T0097.102: Journalist Persona, T0143.001: Authentic Persona), a user claiming to be an independent citizen journalist (T0097.102: Journalist Persona), and an account pretending to be an official account for Hamas (T0097.110: Party Official Persona, T0143.002: Fabricated Persona).<br><br>However, this video was actually filmed in Syria (T0162.004: Content Incorrectly Presented as Depicting Another Location) and was originally posted on October 7 (T0162.003: Historic Content Incorrectly Presented as Current). The man on the right in the video appears to be Jamil Al-Hassan, a journalist who has reported from Syria for months.<br><br>Al-Hassan posted videos and images of the same content on his X and Instagram accounts. Al-Hassan, who describes himself as a ‚ÄúMedia activist in the Syrian revolution‚Äù on his YouTube channel, has recently reported on retaliatory shelling by the Syrian government in early October, which Bellingcat covered here.<br><br>Al-Hassan is still posting content and there have been no reports of his death (T0160.002: Information is False).</i> |
| [I00139 2021 video of tractor rally to support farmers' protests falsely linked to wrestler's protest](../../generated_pages/incidents/I00139.md) | <i>Indian wrestlers have been protesting since April 2023 [against] lack of action against Wrestling Federation of India President Brij Bhushan Sharan Singh over accusations of sexual harassment by female wrestlers. Demanding that Singh be arrested, the wrestlers have asked for support from farmers, and other groups. Amid these events, some old misleading videos and images have been associated with the ongoing protests. One such video on Twitter showing tractors with Indian flags was shared as part of the wrestlers' ongoing protest. The post's caption read: "Walk to Delhi with a tractor to get justice for your daughters," and tagged women wrestlers Geeta Phogat and Babita Phogat. This post received over 4,000 views. Another Twitter user shared the video, claiming it showed the farmers joining the wrestlers' protest. The video was also shared by the Delhi Pradesh Youth Congress' official Facebook account.<br><br>Although farmers have joined the wrestler's protest in Delhi's Jantar Mantar, the viral video showing tractors with Indian flags is not recent (T0162.003: Historic Content Incorrectly Presented as Current) and is unrelated to the current demonstrations against sexual harassment (T0162: Reframe Context, T0068: Respond to Breaking News Event or Active Crisis).<br><br>We noted that the viral video, which apparently showed a tractor march in support of the wrestlers' protest, had a Manorama News Channel logo. However, the channel did not broadcast such footage of the tractor march during the wrestlers' protests.<br><br>After further examination, we found that a YouTube video by Manorama News contained the same visuals as the viral video between timestamps 0:46 and 1:45 (T0165.001: Clipped Content). However, the viral video had been edited to increase the playback speed (T0165.003: Playback Speed Altered) and had a different audio track (T0162.002: Edits Made to News Report which Reframe Context). The English translation of the Malayalam description on the YouTube video stated, "Solidarity with Farmers' Struggle; Kisan March led by Shafi Parambil," dated 2021. The rally was clearly held in support of the farmers‚Äô protest in Delhi against the three farm laws.</i> |
| [I00140 Old video from Iraq shared as 'Israeli soldiers firing at mosque in Gaza'](../../generated_pages/incidents/I00140.md) | <i>A video showing gunshots and smoke emanating from a minaret is being shared with the claim that it depicts Israeli soldiers firing at a mosque in Gaza during prayer. Social media users shared the video with captions such as, "Israeli soldiers fired on a mosque minaret in Gaza while the Adhan (call to prayer) was being recited (translated from Arabic)."<br><br>However, we found that the video actually shows U.S. Marines firing at a mosque in Iraq (T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>A reverse image search revealed the video was posted on American filmmaker Michael Moore's YouTube channel on March 16, 2008. (T0162.003: Historic Content Incorrectly Presented as Current The video was captioned, "WINTER SOLDIER: U.S. Marines Fire on Mosques Unprovoked." <br><br>The description read, "Cpl. Jon M. Turner's (3/8 Kilo Company, 1st Platoon, U.S. Marine Corps) testimony on March 15, 2008, included these two videos of Turner's squad firing on Mosques unprovoked, a violation of international law."<br><br>A September 2011 YouTube video features former U.S. Marine Jon Turner discussing his time in Iraq. Beginning around the 14:20 mark, Turner describes an incident involving a minaret, stating, "One of the guys in the weapons company had gotten shot, and this is the way we would take out our aggression." The viral video appears on a projector screen at 14:28.<br><br>Turner adds, "For those of you who don't know, it is illegal to shoot into a mosque unless you were taking fire from it. There was no fire that was taken from that mosque. It was shot into because we were angry."</i> |
| [I00141 Old videos passed off as recent anti-government protests in Egypt](../../generated_pages/incidents/I00141.md) | <i>Various visuals of large crowds gathered in multiple locations are being widely circulated on social media [in December 2024], claiming that they show recent protests in Egypt against President Abdel Fattah El-Sisi (T0160.002: Information is False).<br><br>[...]<br><br>Through a reverse image search, Logically Facts found that all the viral videos are old and date back to 2019 (T0162.003: Historic Content Incorrectly Presented as Current). Our search showed no evidence of similar large-scale protests against the president in Egypt currently.<br><br>The first video of a large crowd waving their phone torches can be traced to an X post by the Arab media outlet Noon Post, dated September 21, 2019.<br><br>[...]<br><br>[A second video] shows shaky footage of people running on the street, is also old. Al Araby TV shared a longer video of protests happening in Egypt on its official Facebook page on September 21, 2019 (T0162.003: Historic Content Incorrectly Presented as Current). At the 2:10 timestamp (T0165.001: Clipped Content), the same visuals now being widely shared can be seen, showing people walking along a commercial street and chanting slogans.<br><br>[A third video showing] people protesting and holding a banner with Sisi‚Äôs image, is also from the 2019 protests (T0162.003: Historic Content Incorrectly Presented as Current). We found the same video in an article by Al Jazeera, published on September 21, 2019. The article, titled ‚ÄúWatch‚Ä¶ Egyptians tear up pictures of Sisi,‚Äù stated that the video was recorded during protests in the cities of Damietta and Mansoura. According to the report, hundreds of demonstrators chanted anti-Sisi slogans and demanded his resignation before tearing up pictures of the president.</i> |
| [I00142 Picture of Bono and Bob Geldof holding Israeli flags is AI-generated](../../generated_pages/incidents/I00142.md) | <i>Posts shared on social media, including by Greek politician and economist Yanis Varoufakis (T0097.110: Party Official Persona, T0143.001: Authentic Persona), include a screenshot of a post on X (formerly Twitter) which is captioned: ‚ÄúBREAKING: Bono and Bob Geldof stage two-man vigil outside the Israeli Embassy in Dublin.‚Äù (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality, T0162.011: Content Originally Produced as Satire Presented as Not Satire)<br><br>The bio for the X account which appears to have originally posted the image on 16 December describes itself as offering ‚Äúfake news and topical satire‚Äù (T0097.202: News Outlet Persona, T0143.004: Parody Persona). A post on 18 December from the same account confirmed the image was ‚Äúfake‚Äù.<br><br>The person behind the account also told fact checkers at Ireland‚Äôs The Journal that ‚Äúthe image was created by Grok AI and the tweet is satire‚Äù. (T0166: AI-Generated Content, T0160.005: Content Produced as Satire)<br><br>[...]<br><br>There are several other clues suggesting the picture has been generated by AI. The Israeli flag Mr Geldof is holding is wrong. It should have a white background with two horizontal blue stripes and a central Star of David‚Äîbut in the picture, it has two additional vertical blue stripes. <br><br>The fingers on Mr Geldof‚Äôs right hand are also slightly distorted. As we have written before, AI image generators often have particular difficulty replicating fingers.<br><br>In addition to this, there is no evidence that the singers staged a vigil outside the Israeli embassy in Dublin. <br><br>This week Israel announced it will close its embassy in Dublin over "the extreme anti-Israel policies of the Irish government". But there are no credible media reports that Bono or Mr Geldof have staged any such vigil at the embassy. </i> |
| [I00143 Article claiming Texas declared Pride flags in classrooms illegal is satirical](../../generated_pages/incidents/I00143.md) | <i>A claim being shared online that the US state of Texas has declared bringing an LGBTQ pride flag into a school classroom a ‚Äúcrime‚Äù originates from a satirical news article.<br><br>Posts sharing the claim on X (formerly Twitter) and Facebook‚Äîsometimes include a link to a page with a full story and other versions have a screenshot including the headline: ‚ÄúIt‚Äôs Official: Texas Declares Bringing a Pride Flag to the Classroom a Crime.‚Äù<br><br>Many of the accounts posting or commenting on it on social media appear to believe it is a genuine policy (T0162.011: Content Originally Produced as Satire Presented as Not Satire).<br><br>The website it originates from tags the article under ‚Äòsatire‚Äô (T0160.005: Content Produced as Satire). It reports that the bill is titled ‚ÄúThe Classroom Sanctity Act‚Äù. We can find no evidence that a bill of this name or nature has been tabled or approved by lawmakers in Texas.<br><br>The article also quotes Governor of Texas, Greg Abbott as saying at a signing ceremony of the bill: ‚ÄúNo child in Texas should ever be exposed to a rainbow unless it‚Äôs in a coloring book, a weather forecast, or the Lucky Charms aisle at the grocery store.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>We can also find no evidence that he ever said this, or that such a signing took place.</i> |
| [I00144 Viral Kamala Harris-attributed ‚Äòvengeance‚Äô quote is from 2019 satirical news article](../../generated_pages/incidents/I00144.md) | <i>A quote about Trump supporters feeling ‚Äúthe vengeance of a nation‚Äù if the Democrats are re-elected is circulating on social media and being falsely attributed to Kamala Harris.<br><br>Posts sharing the quote have appeared on X (formerly Twitter), and also Facebook, with claims it was said by the now-Democratic presidential candidate in June 2020 (T0162.011: Content Originally Produced as Satire Presented as Not Satire).<br><br>It reads: ‚ÄúOnce Trump‚Äôs gone and we have regained our rightful place in the White House, look out if you supported him and endorsed his actions because we‚Äôll be coming for you next.<br><br>‚ÄúYou will feel the vengeance of a nation. No stone will be left unturned as we seek you out in this great nation. For it is you that betrayed us.‚Äù<br><br>But this is not a genuine quote from the Vice President (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>The text originates from a satire website called ‚Äòbustatroll‚Äô, which published it in an article in 2019. Its about page says: ‚ÄúEverything on this website is fiction. It is not a lie and it is not fake news because it is not real.‚Äù<br><br>While the website is no longer active, an archived version of the story can be found online, which is apparently authored by ‚ÄòColon Crusher‚Äô. <br><br>It is marked as ‚ÄúSatire and/or Conservative Fan Fiction‚Äù (T0160.005: Content Produced as Satire) underneath the headline: ‚ÄúKamala Harris: ‚ÄòAfter We Impeach, We Round Up The Trump Supporters‚Äô‚Äù.<br><br>In the same satirical article, Ms Harris is claimed to have also said ‚Äúthe path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men‚Äù‚Äîa quote from the film Pulp Fiction and is accompanied by a photo of Samuel L Jackson, whose character in the movie said that line.<br><br>The same quote being shared online was also previously debunked by US fact checkers in 2020 (T0160.006: Content Previously Fact Checked).</i> |
| [I00145 Elon Musk has not agreed to buy CNN for $3 billion](../../generated_pages/incidents/I00145.md) | <i>Social media posts claiming Elon Musk has agreed to buy the US news channel CNN for $3 billion are not true and originate from a satirical article (T0160.005: Content Produced as Satire).<br><br>A number of posts on Facebook and Threads have been shared with the caption: "Elon Musk Agrees to $3 Billion to Buy CNN. Elon Musk Reportedly Eyeing CNN Acquisition: ‚ÄòI‚Äôll Fix the Media, One Network at a Time‚Äô‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0162.011: Content Originally Produced as Satire Presented as Not Satire). The claim has also spread to X, which Mr Musk owns. <br><br>But the claim actually stems from a satirical article (T0160.005: Content Produced as Satire) published by a parody news website called SpaceXMania, dated 18 October 2024 (T0097.202: News Outlet Persona, T0143.004: Parody Persona). The headline ‚ÄúElon Musk Reportedly Eyeing CNN Acquisition: ‚ÄòI‚Äôll Fix the Media, One Network at a Time‚Äô‚Äù is similar to many of the social media posts. However the article is clearly labelled as satire, and the website‚Äôs ‚Äòabout page‚Äô also says it provides the ‚Äúfreshest fake news, some sassy analysis, and a good dose of satire‚Äù focused on Mr Musk. <br><br>CNN also told Full Fact: ‚ÄúThere is no truth in rumours of a sale or change in ownership of CNN whatsoever. Any statements to the contrary are completely false.‚Äù <br><br>Likewise Mr Musk has not issued any statement about purchasing CNN and there is no evidence from credible news outlets that any such arrangement is on the cards. Full Fact has also contacted Mr Musk about the claim, and will update this article if we receive a response. <br><br>Although some claims may seem obviously false, we may still fact check them because it may not be clear to everyone, particularly more casual internet users, that they are untrue. </i> |
| [I00146 Fact check: Clip of plane crashing is from a video game](../../generated_pages/incidents/I00146.md) | <i>Soon after Russia invaded Ukraine in February, the world's largest plane, the Antonov An-225 "Mriya," was destroyed when Russian forces attacked the airport where the aircraft was reportedly undergoing repairs.<br><br>Nevertheless, a video that purports to show the plane crashing into a runway has been circulating widely on social media.<br><br>"World's Biggest Airplane Emergency Landing After Four Engines Exploding," reads the title of the four-minute video, which was posted to Facebook on June 23.<br><br>Several commenters on the post appear to believe the video is real. The video accumulated more than 7,000 reactions within four days.<br><br>"It's frightening to see it how the people on board felt is unimaginable," one commenter wrote.<br><br>‚ÄúTerrifying for all aboard. I know pilot and crew very meticulously trained but I can‚Äôt imagine any training prepping them for the real-life drama. Well done majorly to pilot who kept that safe,‚Äù another commenter added.<br><br>But the video does not show an authentic plane crash (T0160.002: Information is False).<br><br>The footage is from a popular video game, Grand Theft Auto V (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality). The Facebook user who shared the video was playing the game.<br><br>USA TODAY reached out to the Facebook user who shared the claim for comment.<br><br>The footage is a recording from the action-adventure video game Grand Theft Auto V. USA TODAY, like other independent fact-checking organizations, found similar clips of the plane crashing on YouTube by other game players.<br><br>Funan Gaming, the Facebook page that posted the video, identifies itself as a "gaming video creator" and often uploads clips of aircraft crashes from the video game.<br><br>In the video game, the world‚Äôs largest cargo plane, an Antonov-225 named "Mriya," crash lands in the fictional city "Los Santos" with all four engines on fire.<br><br>Only one Atonov-225 has been completely built, according to Air Charter Service. However, the aircraft was destroyed in February during the early stages of Russia's invasion of Ukraine.<br><br>Based on our research, we rate FALSE the claim that a video shows the world's largest plane crashing. The footage is from a video game, and the aircraft was destroyed earlier this year after Russia invaded Ukraine.</i> |
| [I00147 Fact Check: Video game clip mislabeled as Russian jet flying past US aircraft](../../generated_pages/incidents/I00147.md) | <i>A clip of two fighter jets created using a combat simulation videogame has been shared online as depicting a September 2024 incident in which a Russian jet flew close to a U.S. plane during an intercept by the U.S. military (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality).<br><br>Reuters reported that on Sept. 23, a Russian Su-35 aircraft conducted an unsafe maneuver during a routine aerial intercept by a North American Aerospace Defense Command (NORAD) F-16 aircraft in the Alaska Air Defense Identification Zone (T0068: Respond to Breaking News Event or Active Crisis).<br><br>The online video shows a fighter jet approaching another from behind and crossing over to the other side. ‚ÄúA U.S. Air Force F-16 is provoked by a Russian Sukhoi Su-35 in the skies of Alaska,‚Äù said Facebook posts with the footage (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality).<br><br>The online clip, however, is visibly different from the original footage by the Department of Defense, published on Sept. 30.<br><br>The video in the posts can be traced to an Oct. 1 post on YouTube (timestamp 0:13) (T0165.001: Clipped Content) titled ‚ÄúNew Angles: SU-35's Dangerous Flyby Over Alaska ‚Äì DCS World,‚Äù by the channel ‚ÄúMasterArm‚Äù which regularly uploads scenes created using‚ÄØdigital battlefield game Digital Combat Simulator World (DCS).<br><br>The account‚Äôs owner, who took credit for making the clip, said in an email to Reuters that all the videos on the channel are created using the DCS World simulator game, adding: ‚ÄúIt is not the first time when my videos are taken and published with another title and some think they are real.‚Äù</i> |
| [I00148 Fact Check: Video of Russian and US aircraft is from simulator video game](../../generated_pages/incidents/I00148.md) | <i>Video game content has been miscaptioned online as showing authentic footage of Russian fighter jets flying alongside an American reconnaissance aircraft (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality).<br><br>The animation of three Sukhoi Su-27 fighter jets and a Lockheed SR-71 was shared on Facebook with the caption: ‚ÄúTwo Russian Su-27 jets intercepted and drove away a USAF SR-71 Blackbird that was conducting surveillance over Russian airspace earlier.‚Äù (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality)<br><br>But the footage originates from Digital Combat Simulator World (DCS), a popular digital combat flight simulation game, the video‚Äôs creator told Reuters.<br><br>The original uncropped footage (T0165.002: Cropped Content) was posted to YouTube by ‚Äúiceman_fox1‚Äù, who regularly shares DCS-generated videos, with the text ‚ÄúCreated with DCS‚Äù in the description.<br><br>The account‚Äôs owner, who took credit for creating the video, said in an email to Reuters, ‚ÄúAll of my content is digital and I make it clear in each video description that the videos are created using Digital Combat Simulator.‚Äù<br><br>The account owner added that it is common for their simulation videos to be shared on social media as real events and without attribution.</i> |
| [I00149 Fact Check: Clip of UFOs in Nevada created with simulation game](../../generated_pages/incidents/I00149.md) | <i>Footage from a video game has been miscaptioned as saying it shows UFOs flying over Nevada (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality). Other online posts use the clip to fuel a conspiracy called Project Blue Beam, which says the U.S. government falsifies UFO footage.<br><br>Both claims are false, as the footage stems from Digital Combat Simulator (DCS), a free-to-play combat flight simulation game.<br><br>[...]<br><br>Some shared the clip with the caption: ‚Äúwas project blue beam on your 2023 bingo card? Certainly no coincidence the government tells us all about ‚Äòaliens‚Äô and now we get crystal clear footage‚Äù.<br><br>The posts reference the July 26 U.S. House hearing on UFOs (T0068: Respond to Breaking News Event or Active Crisis).<br><br>References to Project Blue Beam in the posts and comments refer to a conspiracy theory that the government will coordinate events that will lead people to adopt a new religion to establish a so-called New World Order.<br><br>Believers of the conspiracy do not presume the existence of aliens and express a distrust in government, Joseph A. Vitriol, an expert in conspiracy theories and professor at Lehigh University, said in a phone interview.<br><br>‚ÄúWhat it focuses on is the expectation that the government will create a false narrative or a false flag of an alien-type invasion or some big event,‚Äù he said. ‚ÄúProject Blue Beam is saying this clip is fake, and the government is using this fake event as part of a broader attempt to implement a New World Order.‚Äù<br><br>The social media clip was first shared by TikTok user @iceman_fox1 with the caption: ‚ÄúBREAKING: Three UFO's are intercepted by USAF F-22 Raptors over the central Nevada desert‚Äù.<br><br>An identical clip can be seen on the @iceman_fox1‚Äôs Instagram page. However, both posts by the user note that it was ‚ÄúFilmed with Digital Combat Simulator.‚Äù<br><br>User @iceman_fox1 said in an email: ‚ÄúI create multiple videos a week of random military/sci-fi content,‚Äù confirming the clip was generated with the Digital Combat Simulator game.</i> |
| [I00150 Fact Check: Animation miscaptioned as if to show video of Ukrainian fighter jet shooting down Russian plane](../../generated_pages/incidents/I00150.md) | <i>A clip from the videogame Digital Combat Simulator has been miscaptioned online, with social media users claiming it shows a Ukrainian fighter jet shooting down a Russian plane (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality).<br><br>Social media users are sharing the story of a Ukrainian jet fighter that purportedly singlehandedly downed six Russian aircrafts, dubbed online as ‚Äúthe Ghost of Kyiv.‚Äù Reuters has not independently verified this account.<br><br>The Ukrainian military said on Feb. 24 that five Russian planes and a Russian helicopter were shot down in the Luhansk region. It has not been confirmed that a single Ukrainian pilot shot down the aircrafts, however.<br><br>Iterations of the miscaptioned simulator clip have garnered thousands of views. The vertical video appears to show a jet shooting another aircraft in the sky.<br><br>The same clip was posted on YouTube on Feb. 24, 2022. The description of the video clarifies the footage ‚Äúis from DCS‚Äù and adds it was ‚Äúmade out of respect for ‚ÄòThe Ghost of Kiev.¬¥‚Äù<br><br>DCS is short for Digital Combat Simulator World, ‚Äúfree-to-play‚Äù digital battlefield game developed by Eagle Dynamics.<br><br>Matthias Techmanski, a spokesperson for Eagle Dynamics, confirmed to Reuters via email that the footage circulating on social media is from DCS. ‚ÄúWe are not responsible for its distribution, nor do we endorse such content,‚Äù he added.</i> |
| [I00151 Video game footage misrepresented as fighting between India and Pakistan](../../generated_pages/incidents/I00151.md) | <i>Before New Delhi and Islamabad agreed to a ceasefire following days of deadly jet fighter, missile, drone and artillery attacks, two videos were shared in posts falsely claiming they showed Indian and Pakistani aircraft shot out of the sky (T0160.002: Information is False). While Pakistan claims to have downed five Indian warplanes and both sides said they had destroyed scores of drones during the worst violence between the nuclear-armed neighbours in decades, the clips circulating online were taken from the video game Arma 3 (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality, T0068: Respond to Breaking News Event or Active Crisis).<br><br>Videos purportedly showing fighter jets being shot down were shared on Facebook and Instagram as fighting erupted along India and Pakistan's contested frontier on May 7, 2025.<br><br>They exchanged heavy artillery fire after New Delhi launched deadly missile strikes on its arch-rival, in the worst violence between the nuclear-armed neighbours in decades.<br><br>The fighting came two weeks after New Delhi blamed Islamabad for backing an attack on the Indian-run side of disputed Kashmir, which Pakistan denied.<br><br>The Burmese-language caption of the Facebook video reads: "Indian fighter jets have entered and attacked Pakistan-administered Kashmir. It's been said that five Indian fighter jets were shot down by Pakistan missiles." (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality, T0068: Respond to Breaking News Event or Active Crisis)<br><br>Superimposed text on the Instagram video, meanwhile, says it shows an "Automatic Attack by Raddar (sic)" that was filmed on May 6. (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality, T0068: Respond to Breaking News Event or Active Crisis)<br><br>The days of deadly attacks were halted by a ceasefire agreed on May 10 -- news surprisingly announced by US President Donald Trump, who congratulated New Delhi and Islamabad on using "common sense".<br><br>[...]<br><br>A combination of keyword searches and reverse image searches using keyframes from the first falsely shared video led to a higher-definition clip shared on Facebook on April 19.<br><br>"This is my own video created from a game called Arma 3," the user told AFP on Facebook Messenger on May 10, adding it was "unrelated to the current war".<br><br>Arma 3, a tactical shooter simulation created by Czech video game developer Bohemia Interactive, allows players to create their own combat simulations using realistic military gear. </i> |
| [I00152 Video alleging that Russia destroyed NATO weapons is a video game clip](../../generated_pages/incidents/I00152.md) | <i>A video circulating on Facebook with over 800 reactions and 100 comments falsely alleges that "Russia destroyed NATO weapons and ammunition depots in Ukraine." The footage, however, is from a popular military game called Arma 3 (T0162.005: Video Game Content Incorrectly Presented as Depicting Reality). <br><br>Using Google Reverse Image Search, we found similar images from Arma 3. Next, we examined the footage closely to extract meaningful information, noting the landscape, structures, and objects. We cross-referenced these with the game developer Bohemia Interactive's Community Wiki page, which catalogs objects used in the game, confirming that the footage was from Arma 3.<br><br>The Wiki showed numerous other identical objects visible in the video, including a diesel storage tank, a rusty cistern, and blue cargo containers. <br><br>This is not the first time footage from Arma 3 has been used for war-related propaganda. Fact-checking organizations and other news outlets, including USA Today and France 24, have previously debunked similar claims.<br><br>Bohemia Interactive has addressed the recent spread of their games falsely presented as footage from actual conflicts, like Russia's invasion of Ukraine. Their PR manager, Pavel K≈ôi≈æka, emphasized the importance of collaborating with reputable media outlets and fact-checkers to combat the spread of fake news effectively.<br><br>The developers also shared valuable tips to distinguish video game footage from real-life events. These include paying close attention to the video resolution and sound effects.<br><br>A video alleging that Russia destroyed NATO weapons is a clip from a popular video game, Arma 3. Therefore, we have marked this claim as false (T0160.002: Information is False). </i> |
| [I00153 Fact Check: Clip of schoolchildren being instructed to chant ‚ÄòAllahu Akbar‚Äô likely AI, experts say](../../generated_pages/incidents/I00153.md) | <i>A video shared online purporting to show a teacher in a headscarf instructing white children to bow and chant ‚ÄúAllahu Akbar‚Äù has probably been created using AI (T0166: AI-Generated Content), according to two AI forensics analysts who reviewed the footage for Reuters.<br><br>The 15-second clip, shared widely on social media on November 7 as if authentic, mimics CCTV footage and has a timestamp of 10:24 on November 6, 2025. It shows around a dozen uniformed pupils kneeling on prayer mats in a classroom, led by a woman, apparently a teacher, who has a British accent and is wearing a headscarf.<br><br>The children raise their hands and repeat ‚ÄúAllahu Akbar‚Äù (‚ÄúGod is Great‚Äù in Arabic) after the teacher, who then stands up and lowers herself as if sitting down, tells the children to repeat: ‚ÄúSubhan Allah al-A'la‚Äù (‚ÄúGlory be to God the Most High‚Äù in Arabic).<br><br>One X post with 1.8 million views captioned the clip: ‚ÄúYoung, white children are being indoctrinated into Islam. They raise their hands in the air and chant Allah Akbar. This has to stop‚Äù (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality), while another X post viewed 1.1 million times said: ‚ÄúThis is sick. This is the Muslim indoctrination‚Äù (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality).<br><br>However, the two AI analysts told Reuters that visual inconsistencies that would not occur in a genuine video implied it had been created using AI (T0166: AI-Generated Content).<br><br>Siwei Lyu, a computer science professor at the University at Buffalo, United States, said via email the clip ‚Äúexhibits multiple signs of AI generation‚Äù.<br><br>He said visual anomalies included the teacher sitting on an invisible chair and her face appearing distorted, the heads of students in the front row stretching unnaturally, wall decorations and texts changing over the course of the video, and a girl‚Äôs twin braids appearing and disappearing.<br><br>Rob Cover, a professor of digital communication at RMIT University in Australia, said in an email to Reuters that the clip ‚Äúis very likely to have been AI-generated‚Äù.<br><br>In terms of the audio, he pointed to ‚Äúmetallic sounding‚Äù voices that, he said, are common for lower-quality AI-generated content and, visually, to the bodies, heads, and hair of the children being less in focus compared to their nearby surroundings.<br><br>Cover noted unusual movement in wall posters, inconsistencies in the hair tie of the girl in the front row on the far left and that the ‚Äúmost obvious indicator‚Äù was a missing chair when the teacher appears seated.</i> |
| [I00154 UK opposition leader targeted by AI-generated fake audio smear](../../generated_pages/incidents/I00154.md) | <i>An audio clip posted to social media on Sunday, purporting to show Britain‚Äôs opposition leader Keir Starmer verbally abusing his staff, has been debunked as being AI-generated by private-sector and British government analysis (T0166.001: Deepfake Impersonation, T0097.110: Party Official Persona).<br><br>The audio of Keir Starmer was posted on X (formerly Twitter) by a pseudonymous account on Sunday morning, the opening day of the Labour Party conference in Liverpool. The account asserted that the clip, which has now been viewed more than 1.4 million times, was genuine, and that its authenticity had been corroborated by a sound engineer (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality).<br><br>Ben Colman, the co-founder and CEO of Reality Defender ‚Äî a deepfake detection business ‚Äî disputed this assessment when contacted by Recorded Future News: ‚ÄúWe found the audio to be 75% likely manipulated based on a copy of a copy that's been going around (a transcoding) (T0160.002: Information is False).<br><br>[...]<br><br>‚ÄúIt is also our opinion that the creator of this file added background noise to attempt evasion of detection, but our system accounts for this as well,‚Äù he said.</i> |
| [I00155 AI-generated clip shared as man escaping Syrian prison with spider](../../generated_pages/incidents/I00155.md) | <i>A clip of a man scrambling through a tunnel while holding a large spider has gone viral on social media, claiming to show a rescued prisoner escaping Sednaya (also known as Saydnaya) prison in Syria. <br><br>The clip surfaced in multiple languages after Syrian rebels took control of Damascus, the capital of Syria, on December 8, 2024 (T0068: Respond to Breaking News Event or Active Crisis).<br><br>[...]<br><br>A Facebook user shared the screenshot of the video and wrote, in parts, "Syria: The terrified look of a prisoner the moment he was rescued from the lower floors of the Red Wing in the notorious Sednaya prison in Damascus.‚Äù Similarly, an X user also shared the screenshot with the caption, ‚ÄúA prisoner was just found deep underground in Syria. His expression says it all. He never expected to be found‚Ä¶" (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality, T0160.002: Information is False)<br><br>[...]<br><br>Upon conducting a reverse image search of keyframes from the viral video, we found that it was first shared on TikTok on December 3 with the caption, "No sir, I don't like it #fyp #creepytok #crawlspace #ai.‚Äù The user did not associate the video with Syria in the caption or the comments. The user's profile description also reads, ‚ÄúFollow for more weirdness üòµ |
| [I00156 Clipped video shared to claim Haryana CM didn't drink Yamuna river water](../../generated_pages/incidents/I00156.md) | <i>Ahead of the 2025 Delhi Legislative Assembly elections, the official X account of the governing Aam Aadmi Party (AAP) (T0097.110: Party Official Persona, T0143.001: Authentic Persona) and multiple AAP-affiliated users have circulated a video of Haryana Chief Minister Nayab Singh Saini, claiming that he "pretended to drink the Yamuna water and then spat the water back into the river."<br><br>Former Delhi Chief Minister and Aam Aadmi Party (AAP) chief Arvind Kejriwal also shared this video with the caption, ‚ÄúHaryana Chief Minister Naib Singh Saini pretended to drink Yamuna water... and then spat the same water back into the Yamuna. When I said that Yamuna water could be dangerous for the lives of Delhiites due to ammonia contamination, they threatened to file an FIR against me. They want to make the people of Delhi drink the same poisonous water which they themselves cannot drink. I will never let this happen (sic) (translated from Hindi)." At the time of writing this story, the post had amassed over 1 million views and 9,700 likes. <br><br>[...]<br><br>However, we found a longer version of the viral video in which Saini can be seen drinking the Yamuna water (T0162.008: Context Reframed by Edits to Media, T0165.001: Clipped Content).<br><br>We conducted a Google search using relevant keywords and found an extended video shared by multiple news outlets and Bharatiya Janata Party (BJP)-affiliated accounts. The video shows Saini first spitting out some water and then taking a sip of the river water.<br><br>The full video was uploaded by news agency ANI on January 29, 2025, with the caption, ‚Äú#WATCH | Haryana CM Nayab Singh Saini takes a sip of water from the Yamuna River in Delhi's Palla Village.‚Äù This video features the viral section from the 0:03 to 0:07 seconds timestamp.</i> |
| [I00157 Edited video of Bihar politician Tejashwi Yadav used to claim he was ‚Äòdrunk‚Äô during media briefing](../../generated_pages/incidents/I00157.md) | <i>Social media users have shared a video claiming that Tejashwi Yadav, Bihar politician and chairman of Rashtriya Janata Dal (RJD), was inebriated during a media address. The 43-second video, circulated on X (formerly Twitter) and YouTube, shows Yadav speaking slowly and appearing to slur his words while addressing reporters.<br><br>[...]<br><br>Using the logo in the viral clip and Hindi keywords from Yadav‚Äôs statement, we located the original video on the Indian news outlet Republic Bharat‚Äôs YouTube channel. Uploaded on June 11, 2024, under the title "What did Tejashwi Yadav say when taking a dig at the division of ministries (translated from Hindi)?‚Äù, the video is only 30 seconds long and does not show Yadav slurring or speaking slowly.<br><br>When we slowed down the playback speed of this video to 0.75x, we observed that it matched the slurred and slowed effect seen in the now-viral videos. Additionally, the edited video length matches the 43-second duration of the circulated clips. A comparison reveals that social media users utilized an edited, slowed-down version of the video to propagate a false claim (T0162.008: Context Reframed by Edits to Media, T0165.003: Playback Speed Altered).<br><br>[...]<br><br>A user on X shared the clip with the caption, ‚ÄúTejasvi Yadav, Ex-Deputy CM- Bihar. Drunk. & (sic) Bihar is Dry State (No Alcohol). Jhum Barabar Jhum (keep on dancing)‚Äù.</i> |
| [I00158 Was Capitol Rioter in Horns and Furs an Antifa Instigator Who Took Part in BLM Protests?](../../generated_pages/incidents/I00158.md) | <i>In the aftermath of unprecedented rioting in the U.S. Capitol building on Jan. 6, 2021, attention turned to identifying those responsible, many of whom were photographed in the course of their criminal actions, and some of whom even recorded their own involvement in selfies and social media live streams.<br><br>One of the individuals who featured prominently in press photographs and video footage from the rioting was a shirtless man who wore horns and furs on his head and had a painted face and tattoos on his body.<br><br>Beginning on the afternoon of the riots, social media users and conspiracy theorists shared photographs of the man, Arizona resident Jake Angeli, juxtaposed to suggest that he had previously taken part in demonstrations organized by the Black Lives Matter (BLM) movement and therefore was not, in fact, a supporter of outgoing U.S. President Donald Trump, but rather an agent provocateur or adherent to the antifa (anti-fascist) movement, seeking to discredit Trump supporters and efforts to overturn the victory of President-elect Joe Biden in the Nov. 3, 2020, presidential election.<br><br>[...]<br><br>On Facebook, users widely shared a screenshot that included two photographs of Angeli, along with the caption "AZ [Arizona] BLM rally in June, DC Capital [sic] in January":<br><br>Some social media users explicitly presented the photographs as proof that Angeli was "antifa" and "not MAGA." In reality, Angeli is an enthusiastic supporter of Trump with a long public record of promoting the bizarre and baseless "QAnon" cluster of conspiracy theories.<br><br>The photograph on the right of the tweet ‚Äî which was originally published by Trump supporter Cari Kelemen ‚Äî showed Angeli taking part in the Capitol riots on Jan. 6, 2021. It was cropped from a larger photograph taken by Saul Loeb and published by Getty (T0162.008: Context Reframed by Edits to Media, T0165.002: Cropped Content).<br><br>The photograph shown on the left of the widely shared screenshot did show Angeli at a BLM demonstration in the summer of 2020, but it was cropped to occlude the fact that he was carrying a pro-QAnon sign, and failed to mention that Angeli had reportedly gone there not to take part in the main event, but to recruit others to his causes (T0162.008: Context Reframed by Edits to Media, T0165.002: Cropped Content).<br><br>It appears to have been taken by Brett Lewis, who emphasized on Twitter on Jan. 6 that Kelemen and others had misleadingly cropped it so that it no longer showed the QAnon-themed placard that Angeli was holding at the time. Lewis further explained that Angeli attended the BLM march in order to instigate conflict, not to join in the messages of other participants</i> |
| [I00159 Obama's speech on 'disinformation as a threat to democracy' taken out of context](../../generated_pages/incidents/I00159.md) | <i>Several posts on X (formerly Twitter) and Facebook have shared an approximately 40-second clip of a speech by former U.S. President Barack Obama. In the clip, he states, "You just have to flood a country‚Äôs public square with enough raw sewage. You just have to raise enough questions, spread enough dirt, and plant enough conspiracy theorizing that citizens no longer know what to believe. Once they lose trust in their leaders, in mainstream media, in political institutions, in each other, in the possibility of truth, the game‚Äôs won."<br><br>The captions accompanying these posts quote parts of his speech, with some labeling Obama as a villain and insinuating that he advocates for using conspiracy theories to manipulate people through disinformation.<br><br>[...]<br><br>The clips of Obama‚Äôs speech, however, have been taken out of context, creating the false impression that he is endorsing disinformation. In reality, Obama was addressing how authoritarian leaders worldwide utilize disinformation as a weapon against democratic nations (T0162.009: Statement Reframed by Removal from Context, T0162.008: Context Reframed by Edits to Media).<br><br>A keyword search revealed that the clip was extracted from Obama's keynote address at Stanford University on April 21, 2022. The address focused on the dangers of disinformation to democracy.<br><br>Following this lead, we found the full speech on Youtube, uploaded by multiple channels. The relevant clip appears between 31:38 and 32:16 in the video (T0165.001: Clipped Content).</i> |
| [I00160 Did Ruth Bader Ginsburg Say that Pedophilia Was Good for Children?](../../generated_pages/incidents/I00160.md) | <i>The language in a 1974 report that was co-authored by Supreme Court justice Ruth Bader Ginsburg, has been analyzed and criticized for more than four decades. The piece tackled sex bias in the United States penal code. As these critics have devolved from scholars, to senators, to pundits, to conspiracy-minded web sites, to the lowly meme maker, the accusations against Ginsburg have grown more crude and distorted.<br><br>In February 2018, for instance, we came across a meme featuring an image of the Supreme Court Justice and a quote ostensibly uttered by her about pedophilia being good for children:<br><br>This is not a genuine quote from Ruth Bader Ginsburg (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>This claim is based upon a gross misinterpretation of another misinterpretation, which was itself based upon a simple misreading of a 1974 report entitled "The Legal Status of Women Under Federal Law" that was co-authored by Ginsburg, who at the time was a professor of law at the Columbia Law School. The other co-author was Brenda Feigen-Fasteau, a former director of the American Civil Liberties Union's women's rights project.<br><br>In 1974 Ginsburg and Feigen-Fasteau published a report examining how federal law frequently employed gendered language. This report was used as the basis for the "Sex Bias in the U.S. Code," a report published in 1977, which included a passage explaining the purpose of the study:<br><br>The Constitution, which provides the framework for the American legal system, was drafted using the generic term "man." While the United States Supreme Court, the ultimate interpreter of the Constitution, might have determined that "man" also means "woman" in terms of rights, duties, privileges, and obligations under the Constitution, the Court instead has chosen on numerous occasions to deny to women certain rights and privileges not denied to men.<br><br>While explaining the "equality principle" and arguing that pronouns should be altered in the existing penal code so that both men and women were equally accountable for crimes against both boys and girls, Ginsburg quoted a proposed 1973 Senate bill as an example of legislation which used gender neutral language:<br><br>A person is guilty of an offense if he engages in a sexual act with another person, not his spouse, and (1) compels the other person to participate: (A) by force or (B) by threatening or placing the other person in fear that any person will imminently be subjected to death, serious bodily injury, or kidnapping; (2) has substantially impaired the other person's power to appraise or control the conduct by administering or employing a drug or intoxicant without the knowledge or against the will of such other person, or by other means; or (3) the other person is, in fact, less than 12 years old.<br><br>It is the highlighted line that has been repeatedly misinterpreted and distorted over the ensuing decades.<br><br>It appears that Ginsburg was first accused of wanting to lower the age of consent to 12 shortly before she was confirmed to the Supreme Court in 1993. This accusation reemerged in 2005 after John Roberts was nominated. Both Senator Lindsey Graham and Fox News host Sean Hannity, for instance, used this line to argue that Ginsburg was "very left-wing" and immoral:<br><br>HANNITY: I guess where I am on this, if you look at Ruth Bader Ginsburg, I mean, she -- the Ginsburg rule, she doesn't have to answer specific questions, clearly pro-choice going in, thinks there may even be a constitutional right to polygamy, has a controversial view we should lower the age of consent to 12, supports legalized prostitution, very left-wing.<br><br>GRAHAM: Well, there are all kind of hearts. There are bleeding hearts and there are hard hearts. And if I wanted to judge Justice Ginsburg on her heart, I might take a hard-hearted view of her and say she's a bleeding heart. She represents the ACLU. She wants the age of consent to be 12. She believes there's a constitutional right to prostitution. What kind of heart is that?<br><br>However, Ginsburg never actually said that the age of consent should be lowered to 12 (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>Ginsburg's report was about changing gendered language, not the age of consent, in our existing laws. In the quoted passage, she was not arguing for or against lowering the age of consent; rather, she was quoting a proposed Senate bill as an example of how appropriate gender-neutral pronouns should be used. Ginsburg wrote that she used this bill because it "conform(ed) to the equality principle," not because she agreed with the presented age of consent.<br><br>Furthermore, Ginsburg mentioned another section of the penal code a few paragraphs earlier which referenced a different age of consent: 16. In both cases, Ginsburg's focus was on the gender of the victim, rather than the age, as her report was specifically concerned with gendered-language in U.S. law:<br><br>18 U.S.C. ¬ß ¬ß1154 and 2032 make it a crime for a person to have carnal knowledge of a female, not his wife, who has not attained the age of sixteen years.<br><br>[...]<br><br>The "statutory rape" offense defined in these sections follows the traditional pattern: the victim must be a female and the offender, a male. Protection of the girl's virtue as an asset to be traded by her family at marriage time can no longer survive as a justification for such provisions. The immaturity and vulnerability of young people of both sexes can be protected through appropriately drawn, sex-neutral proscriptions.<br><br>The claim that Ginsburg said that "pedophilia was good for children" appears to be the result of a decades-long game of telephone that started with a misreading of a 1974 report.<br><br>It started in 1993, after Ginsburg was nominated to the Supreme Court, when this report was quoted out of context as evidence that Ginsburg wanted to lower the age of consent to 12 (T0162.009: Statement Reframed by Removal from Context). As this errant argument was reiterated by pundits such as Sean Hannity it morphed from a single out of context quote to an alleged personal belief at the core of Ginsburg's political views. When the "Pizzagate" controversy exploded during the 2016 presidential election, this rumor underwent another devolution as conspiracy theorists claimed that Ginsburg once wrote that she wanted to legalize child rape.</i> |
| [I00161 Pro-Kremlin Telegram channels promote narrative that Poland will annex western Ukraine](../../generated_pages/incidents/I00161.md) | <i>Pro-Kremlin Telegram channels continue speculating on alleged Polish aspirations to annex parts of western Ukraine. Previously, channels and other sources published a falsified letter to ‚Äúconfirm‚Äù this theory. A new round of disinformation continues this campaign in an apparent attempt to discredit the Polish Army and drive a wedge between Poland and Ukraine.<br><br>The Russian Telegram channel Signal published forged photos of multiple billboards (T0161.003: Falsified Graffiti or Signage) depicting Jaros≈Çaw Mika, General Commander of Branches of the Polish Armed Forces, alongside the quote, ‚ÄúIt‚Äôs time to remember history,‚Äù a reference to the historical fact that parts of western Ukraine were once Polish territory . The channel also mentioned the removal of Ukrainian flags from Polish public transportation, as well as a previous statement by Sergei Naryshkin, the chief of Russia‚Äôs Foreign Intelligence Service, claiming the US and Poland were plotting to partition Ukraine. While the flag removal and Naryshkin‚Äôs statement both occurred, the channel claimed without evidence that Poland was about to invade Ukraine, using the forged billboards as additional evidence. Polish outlet Konkret24 was the first to confirm the billboard images had been forged.<br><br>The fake billboard story was later amplified by the Kremlin-tied Telegram channel Gossip Girl, which had previously published the forged letter alleging Poland‚Äôs intent to annex Ukrainian territory ( T0151.007: Chat Broadcast Group). ‚ÄúSo what does Poland really want?‚Äù Gossip Girl asked. ‚ÄúHelp secure western Ukraine by sending in troops or regain historical lands?‚Äù<br><br>Another Kremlin-tied channel, Legitimniy (‚ÄúLegitimate‚Äù), forwarded the forged billboard images and claimed, ‚ÄúPoland is preparing for expansion, many factors indicate this, but no one will tell Ukrainians about this.‚Äù The channel also discussed that Poland might attempt to censor Ukrainian history, ironically reflecting what Kremlin is actually doing.<br><br>On May 3, Telegram channel Rokot|Ryk, which uses the Russian pro-invasion Z symbol in its logo, published a short clip of a speech by Polish President Andrzej Duda in which he said that there wouldn‚Äôt be any borders between Poland and Ukraine. In its original context, President Duda‚Äôs quote was in reference to a new era of Ukraine-Poland cooperation and opposition to Russian imperialism and its occupation of Crimea and Eastern Ukraine. Presented out of context, though, the Telegram post misleadingly implied that President Duda was discussing the ‚Äúinclusion‚Äù of Ukraine within Polish territory (T0162.009: Statement Reframed by Removal from Context, T0162.008: Context Reframed by Edits to Media, T0165.001: Clipped Content).<br><br>Pro-Kremlin channels embraced the false interpretation that President Duda intended to annex Ukraine, even suggesting the new country would be renamed ‚ÄúUkropol.‚Äù On May 5, a video with Russian subtitles appeared on another channel with the ‚ÄòZ‚Äô symbol and a user handle similar to the one used by Rokot|Ryk; the video was also posted to the pro-Lukashenka Telegram channel Zheltye slivi (‚Äú–ñ–µ–ª—Ç—ã–µ —Å–ª–∏–≤—ã,‚Äù or ‚ÄúYellow Leaks‚Äù). The Signal Telegram channel picked up the video and claimed that in the future, President Duda ‚Äúwould be able to rely on the potential of its neighbors‚Äù in the Baltic states to build a community of nations. The channel concluded by describing this as an ‚Äúimperialist statement,‚Äù and reiterated that Poland intended to expand its territory. Ukrainian Kremlin-tied channel ZeRada also published the video with the comment, ‚ÄúOn what grounds [does Poland] propose to live on Ukrainian land?‚Äù and asked whether Ukrainian President Volodymyr Zelenskyy should reply to these claims. The post also alluded to the forged images of Polish billboards.<br><br>Meanwhile, on May 4, a video displaying the BBC News logo appeared online, repeating the same allegation that Poland was preparing to send troops to Western Ukraine ‚Äúunder the pretext of protection from Russia.‚Äù (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona) Captions in the video suggested that the Polish Commander-in-Chief of the Armed Forces had already ordered the army to ‚Äúprepare for an invasion of Ukraine,‚Äù which was ‚Äúconfirmed‚Äù by a ‚Äúpublished order‚Äù signed by General Mika. The video also asserted that Washington had endorsed Poland‚Äôs invasion to Ukraine, while NATO would ‚Äúofficially stand aside.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>As evidence, the video included a forged document previously analyzed by the DFRLab (T0085.004: Develop Document, T0161.001: Impersonated Content, T0097.206: Government Institution Persona), which allegedly ordered Polish armed forces to prepare airborne units to be deployed in Ukraine. It also showed the fake billboards of General Mika, as well as a helicopter and Polish soldiers allegedly filmed in northern Poland preparing to deploy to Ukraine. The video was disseminated on Twitter, Telegram, and Facebook on multiple languages including Russian, French, Italian, Turkish and Czech (T0101: Create Localised Content).<br><br>A comparison of the fake video with authentic BBC News videos shows that placement of captions, font and graphics mimics authentic BBC videos, including a promotion for viewers to download BBC mobile app. However, as the BBC told Reuters and Konkret 24, the video was not published by the outlet; BBC News journalists Alistair Coleman and Shayan Sardarizadeh also noted on Twitter that the video was fake. Thus it appears to be yet another attempt by malicious actors to impersonate a legitimate news source, similar to another fake BBC News video from April 2022 that accused Ukraine of bombing the Kramatorsk railway station.</i> |
| [I00162 Fact check: Alien moon footage is from a fictional short film, not a Chinese lunar rover](../../generated_pages/incidents/I00162.md) | <i>Some social media posters are sharing video footage they claim shows an alien walking on the moon. <br><br>The video, which is embedded in a 2021 blog post, shows a tall, thin creature walking and squatting on a barren moon-like landscape.<br><br>"Strange Alien Walking On The Moon Was Filmed By The Chinese Lunar Rover," reads the caption of a June 20 Facebook post that linked to the blog. <br><br>The blog garnered more than 13,000 interactions on Facebook, according to social media analytics tool CrowdTangle.<br><br>But the footage is not authentic. It is taken from a fictional short film series (T0162.010: Entertainment Media Content Incorrectly Presented as Depicting Reality).<br><br>USA TODAY reached out to the Facebook users who shared the post for comment. The blog author could not be reached. <br><br>The footage the blog presents as coming from a lunar rover is a cropped (T0165.002: Cropped Content) version of a video created for a short film series by a production company called MeniThings.<br><br>The video, which was uploaded to YouTube in 2015, is titled "Alien on Moon | Project Blue Book." The video's caption describes it as being "part of the Project Blue Book short film series."<br><br>Other films associated with Project Blue Book are also available on the MeniThings YouTube home page. <br><br>The production company's website describes MeniThings as "a full featured production company that supports ... original live action and animated work." <br><br>Check Your Fact previously debunked a claim involving similar footage of a purported moon alien (T0160.006: Content Previously Fact Checked). In that case social media users said the footage was captured by NASA. <br><br>MeniThings did not respond to a request for comment. <br><br>Based on our research, we rate FALSE the claim that a video of a walking alien is authentic footage from a Chinese lunar rover. The footage is from a fictional short film series.</i> |
| [I00163 Visuals from a 2022 movie shoot shared with false ‚ÄòPallywood' narrative](../../generated_pages/incidents/I00163.md) | <i>A video being shared on social media shows a boy lying on a table under some wooden blocks with two people approaching him and laughing as they pick him up. Sharing the video, users have alleged that people in Gaza, who are facing regular bombardment from Israel, are faking injuries amid the ongoing conflict. Archived versions of posts sharing the video with such claims can be viewed here, here, and here. <br><br>Several accounts have shared the viral visuals with the hashtag ‚ÄòPallywood‚Äô, a narrative that became popular among a section of social media users amid the Israel-Hamas conflict, which alleges that Palestinians are faking injuries, deaths, and funerals. The dehumanizing narrative is not new; an analysis by Logically Facts found a surge in such posts aimed at denying and mocking civilian harm on the Palestinian side since the conflict that began on October 7, 2023.<br><br>However, the now-viral clip depicts behind-the-scenes (BTS) footage of a movie that was shot in 2022. It does not depict ‚Äòcrisis actors‚Äô allegedly staging deaths or injuries in Gaza (T0162.010: Entertainment Media Content Incorrectly Presented as Depicting Reality).<br><br>A reverse image search of a screenshot from the viral clip led us to a video uploaded on TikTok  by an account named ‚ÄòMahmoud Maher Zaqout‚Äô on February 15, 2022. According to his bio, Zaqout is a Palestinian director and photographer. We can see the now-viral visuals in this video, with overlaid Arabic text that translated to ‚ÄúA new movie‚Äù (translated to English). A comment posted by the creator below the video added that it shows the filming of a movie.<br><br>[...]<br><br>The movie ‚ÄòAl-Ahed‚Äô was uploaded on Zaqout‚Äôs official Instagram account on March 1, 2022. There are various similarities between the film‚Äôs visuals and the BTS footage posted by Zaqout on TikTok.</i> |
| [I00164 Video of ‚Äòactor in Gaza limping on the wrong foot‚Äô is from comedy skit filmed in Iraq](../../generated_pages/incidents/I00164.md) | <i>A video is circulating on social media alongside claims it shows an actor in Gaza who is accidentally limping on the wrong foot. <br><br>The posts say the actor ‚Äúpretends to be a ‚Äòvictim‚Äô‚Äù and that ‚Äúthe media is lying to us about what's happening in Gaza. Why would they need to act if there was an actual genocide?‚Äù<br><br>But the video actually shows a comedy skit (T0162.010: Entertainment Media Content Incorrectly Presented as Depicting Reality, T0162.011: Content Originally Produced as Satire Presented as Not Satire) filmed in Baghdad, Iraq (T0162.004: Content Incorrectly Presented as Depicting Another Location), which was uploaded to Instagram on 8 December. It is unrelated to the current conflict in the Middle East.<br><br>That account appears to belong to a shop selling cleaning products, cosmetics and electrical items and its Instagram bio lists its location as an address in Baghdad, Iraq. Searching images of the Bab al-Derwaza Market mentioned in the bio, Full Fact was able to find a picture which matched the buildings and street signs in the back of the video, confirming it was filmed at this location in Iraq, not Gaza. The original video‚Äôs caption makes no mention of Gaza or Palestinians.<br><br>In a later post commenting on the video‚Äôs spread, and misattribution to Gaza, the account holder confirms the video was filmed in Baghdad and questions why the video ended up being wrongly connected to Gaza. <br><br>The fact that the video is being shared with captions suggesting the man in it is an actor in Gaza pretending to be a victim appears to echo a narrative, often referred to as ‚ÄòPallywood‚Äô (a portmanteau of Palestine and Bollywood), which has been used online to describe the alleged false staging of harm to Palestinian civilians.</i> |
| [I00165 Video of ‚ÄòRafah actors‚Äô actually from Palestinian TV drama series](../../generated_pages/incidents/I00165.md) | <i>A video is being shared online with the implication that it shows ‚ÄúRafah actors‚Äù preparing to pretend to be dead or injured in Gaza. This comes amidst reports Israel appears to be set to launch a large-scale assault on the city in the south of Gaza.<br><br>In the clip, which is circulating on Facebook, Instagram and YouTube, and shared over 4,000 times on X (formerly Twitter), a man lies on a stretcher while a woman applies makeup to his chest and neck, and another man sits in what appears to be a body bag‚Äîwhile smoking a cigarette. <br><br>Text overlaid on the video says ‚ÄúMake-up Gaza Style‚Äù and has a watermark of an account which shares what it claims are ‚ÄúPallywood‚Äù videos‚Äîa portmanteau of Palestine and Bollywood. <br><br>Full Fact has written several times about this term, which in some cases has been previously used to caption videos or images incorrectly claiming to show those in Gaza faking images of harm to civilians during the war. <br><br>A caption with one post sharing the video on Facebook says: ‚ÄúRafah actors are preparing, and we may soon witness disturbing footage from Rafah.‚Äù<br><br>But this video actually shows behind-the-scenes footage of a Palestinian drama series, called Bleeding Dirt (T0162.010: Entertainment Media Content Incorrectly Presented as Depicting Reality), and is unrelated to Israel‚Äôs planned military offensive in Rafah (T0160.002: Information is False).<br><br>The actors in the clip being shared can be seen in the same makeup and costume during an episode of the show, available online. The description with the video says it premiered on 12 March 2024. <br><br>According to Palestinian fact checkers Kashif, the scene was filmed in a town near Nablus, in the West Bank, not Gaza.</i> |
| [I00166 Old video from Guatemala of woman set ablaze by mob shared as India](../../generated_pages/incidents/I00166.md) | <i>A horrific video of a young woman set ablaze by a mob has been posted on social media [in October 2019] with the implication that the incident took place in India, and is the handiwork of the Rashtriya Swayamsevak Sangh (RSS).<br><br>[...]<br><br>The above image is a screenshot of a tweet posted on October 6 [2019] by an account claiming to be Pakistani politician Aitzaz Ahsan (T0097.110: Party Official Persona). His tweet has been retweeted over 2400 times so far. The message along with it, posted in Urdu, rather loosely translates to, ‚ÄúThe RSS terrorist organization occupied the whole of India, controlling them from the film industry to the Indian Army. And yet so much grace on them, even our media groups speak their language.‚Äù It is clear that the blame for the ghastly incident is placed on the RSS, suggesting that the incident took place in India.<br><br>The video in question is old (T0162.003: Historic Content Incorrectly Presented as Current), and not from India (T0162.004: Content Incorrectly Presented as Depicting Another Location). The incident had occurred in Guatemala in Central America. The woman seen in the video was accused of having murdered a cab driver along with two male accomplices. She was apprehended by a mob and lynched. Her accomplices meanwhile had managed to escape. The incident had been covered extensively by the media when it had occurred in early 2015.<br><br>In 2017, the same video was shared with the false, communal narrative that it represented a Marwari woman who had married a Muslim man and was lynched because she refused to wear a burqa (T0160.006: Content Previously Fact Checked).<br><br>Again, in 2018, the video was portrayed by certain sections on social media as a Hindu girl burnt alive in Madhya Pradesh because she had attended a prayer meeting at a church (T0160.006: Content Previously Fact Checked).</i> |
| [I00167 Video from Poland dance festival falsely shared as Ukraine being 'diversified' during the war](../../generated_pages/incidents/I00167.md) | <i>A viral video of a dance festival is being shared on TikTok claiming it shows Ukraine's "diversification" during the ongoing war with Russia. The clip shows people from different races dancing together, and the clip seems to have been sped up (T0165.003: Playback Speed Altered).<br><br>The video shared on TikTok shows a person clapping while the video plays in the background behind him, before saying, "Truly one of the most beautiful things I have ever seen. Ukraine is already being diversified. While a million Ukrainian men have lost their lives defending the homeland, the homeland is being diversified already. Truly beautiful diversity I tell you, everybody needs more diversity." [...].<br><br>However, the video was filmed in Poland (T0162.004: Content Incorrectly Presented as Depicting Another Location) in 2022 (T0162.003: Historic Content Incorrectly Presented as Current) and is not related to Ukraine or the ongoing conflict.<br><br>Logically Facts conducted a reverse image search using different keyframes from the viral video and discovered a YouTube video uploaded by Kizomba Corner on July 18, 2022. The video carried similar visuals with the same background and individuals but at a normal playback speed. The same people from the now-viral video are visible in this 2022 video.<br><br>The YouTube video title states that it was recorded during a dance festival called Kizz Me More Kizomba Festival in 2022. A Google search led us to a website named Dance Place, which stated that the festival was held at Hotel Novotel Warszawa Airport in Warsaw, Poland. Kizz Me More is a dance festival that celebrates music and dance, particularly kizomba, urban kiz, taraxxo, and afrohouse. </i> |
| [I00168 Video does not show a Palestinian woman saying ‚Äòwe‚Äôre prisoners of Hamas‚Äô](../../generated_pages/incidents/I00168.md) | <i>Fake subtitles have been added to footage circulating on social media to suggest a Palestinian woman said in Arabic ‚Äúwe are prisoners of Hamas‚Äù. However, this is not a correct translation. (T0162.001: Incorrect Subtitled Speech Reframes Context)<br><br>The video shows a woman speaking in Arabic to someone behind the camera. Overlaid text says in English: ‚ÄúWatch this courageous Palestinian woman saying the truth‚Äù, while text in Arabic says: ‚ÄúOne woman worth a thousand men‚Äù (translated by Google). <br><br>According to the subtitles, the woman is explaining how Hamas forced her to ‚Äústay‚Äù and threatened to ‚Äúslaughter‚Äù her if she tried to escape. She supposedly says: ‚ÄúWe‚Äôre prisoners of Hamas. I prefer the Jews.‚Äù <br><br>One post sharing the video on X (formerly Twitter) says: ‚ÄúImagine @IDF actually frees them from HAMAS and the internet flooded with these videos of people who were opressed by hamas but were afraid to complain because of the deathrape. Imagine then that the Gaza will actually start to thrive after Isreal intervention‚Ä¶Thankyou @IDF for saving these people. [sic]‚Äù<br><br>However, freelance fact checker, Arwa Kooli, confirmed to Full Fact that the subtitles are not a correct translation (T0160.002: Information is False). Ms Kooli said the woman is actually speaking about searching for her son and recognising his body because of the belt. </i> |
| [I00169 Fake subtitles show Putin and Erdogan ‚Äòwarning America‚Äô over Israel support](../../generated_pages/incidents/I00169.md) | <i>Videos being shared online falsely claim to show Russia‚Äôs President Putin and Turkey‚Äôs President Erdogan ‚Äúwarn America‚Äù about its support of Israel. <br><br>The two videos show footage of the leaders speaking in Russian and Turkish, with English subtitles. However, the footage predates recent conflict in October 2023 (T0162.003: Historic Content Incorrectly Presented as Current) and the subtitles do not translate what the politicians actually said (T0162.001: Incorrect Subtitled Speech Reframes Context, T0160.002: Information is False).<br><br>[...]<br><br>One video shows President Putin supposedly saying: ‚ÄúAmerica wants to destroy Israel. As we destroy Ukraine in past. I am warning America. Russia will help Palestine and America can do nothing [sic].‚Äù<br><br>[...]<br><br>A USA Today logo is visible in the top corner of some versions of the videos but the subtitles are not in the media company‚Äôs traditional colours or style (T0162.002: Edits Made to News Report which Reframe Context). Some versions of the video feature a flashing red dot to give the impression it was recorded live.<br><br>Thousands of people have also viewed a video of President Erdogan appearing to say: ‚ÄúI am warning America don‚Äôt in Israel Palestine war. We are with our innocent Palestinian brother‚Äôs. We are ready to defend Palestine at any price [sic].‚Äù<br><br>[...]<br><br>The videos both use old footage of the politicians which predate current events in the Middle East, and show mistranslated subtitles. <br><br>The clip of President Putin actually shows him during a meeting with the country‚Äôs ‚ÄòCouncil for Civil Society and Human Rights‚Äô in December 2022, and there‚Äôs no mention of Israel or Palestine in the subtitles from USA Today or in the transcript for the whole meeting released by the Kremlin. Media reports from the time don‚Äôt mention him saying this either.<br><br>The video of President Erdogan was published by news website Middle East Eye in July 2023 with the publication‚Äôs logo appearing in the clip on social media (T0162.002: Edits Made to News Report which Reframe Context, T0162.003: Historic Content Incorrectly Presented as Current, T0162.001: Incorrect Subtitled Speech Reframes Context, T0160.002: Information is False). While he is speaking in Turkish about relations with Israel and ‚Äúthe Palestinian state‚Äù, the subtitles show there is no reference to America. </i> |
| [I00170 Kim Jong Un video doesn‚Äôt show him talking about Israel-Gaza conflict](../../generated_pages/incidents/I00170.md) | <i>A video of North Korean leader Kim Jong Un making a speech has been claimed to show him commenting on the Israel-Gaza conflict [...]<br><br>A caption on the video states: ‚ÄúKim Jong-un speech about Israel Palestine war [sic].‚Äù (T0068: Respond to Breaking News Event or Active Crisis)The fake subtitles say: ‚ÄúUnder the Biden administration conflicts erupt yearly. This year a war begins between Israel and Palestine. Last year a war begins [sic] between Russia and Ukraine and two years ago billions worth of military equipment was left to the Taliban. <br><br>‚ÄúI‚Äôm afraid that if the Biden admin. does not cease to exist in the next election World War 3 may begin. Who knows what next year‚Äôs war will be. I support Donald Trump for president in 2024. Good luck to Mr Trump‚Äù.<br><br>But the video predates the current conflict (T0162.003: Historic Content Incorrectly Presented as Current) and the subtitles are fake (T0162.001: Incorrect Subtitled Speech Reframes Context). <br><br>Kim Jong Un was actually speaking at a military parade to celebrate the 75th anniversary of the ruling party and thanked citizens for overcoming unexpected obstacles. According to Politico, he ‚Äúextended an olive branch‚Äù to South Korea and ‚Äúavoided direct criticism of Washington‚Äù.<br><br>The video was taken in October 2020, before the current Israel-Gaza conflict or the 2022 Russian invasion of Ukraine. It also predates the US election in November 2020, when President Biden was elected.<br><br>A section of the footage being shared (T0165.001: Clipped Content) matches video from BBC reporting of his speech (T0162.002: Edits Made to News Report which Reframe Context). The BBC doesn‚Äôt feature a direct translation of all that he said, but text on screen says: ‚ÄúHe said no one could appreciate [North Korean troops‚Äô] ‚Äòheroic devotion‚Äô without ‚Äòshedding tears of gratitude‚Äô.‚Äù</i> |
| [I00171 FALSE: Angel Locsin ‚Äòtests positive‚Äô for coronavirus](../../generated_pages/incidents/I00171.md) | <i>Claim: Actress Angel Locsin ‚Äútested positive‚Äù for the coronavirus after conducting several relief operations.<br><br>Facebook Claim Check, the site‚Äôs monitoring tool that identifies suspicious posts, flagged at least 6 unique website links containing this information. All links were from website ramdomnames.club.<br><br>The links redirect to a webpage with the headline ‚ÄúPagkatapos ng Kaniyang Initiative Para Sa C0V|D19, Angel Locsin Nagpositibo sa Naturang Sakit!‚Äù (After her COVID-19 initiatives, Angel Locsin tested positive for the disease!)<br><br>The webpage also contained an embedded video. When clicked, the video played only for a few seconds then stopped to ask the reader to share the link to ‚Äúuncover‚Äù the rest of the clip. The first 3 seconds showed newscaster Atom Araullo saying, ‚ÄúThe DOH confirmed 2 new cases of COVID-19.‚Äù<br><br>The website links were shared on Facebook as early as April 26. Based on Claim Check data, the links have been reported by Facebook users at least 33 times for containing false information. A reader also sent the claim to Rappler‚Äôs email for verification.<br><br>Rating: FALSE (T0160.002: Information is False)<br><br>The facts: On April 26, the day the links were first shared on Facebook, Locsin announced that she had tested negative for the coronavirus. GMA News Online reported it the same day.<br><br>Locsin posted a photo of two test kits on Instagram Stories on April 26. One was hers and the other was Neil Arce‚Äôs, her celebrity boyfriend. Both test kits showed negative results. Locsin said ‚ÄúCOVID free‚Äù in her post.<br><br>Rappler also found the video embedded in the webpages. It was an edited version of a clip (T0165.001: Clipped Content) from GMA Network‚Äôs Stand for Truth episode aired on March 6, 2020, and uploaded by YouTube channel News Reports. A photo of Locsin was placed over the original video (T0162.002: Edits Made to News Report which Reframe Context, T0162.008: Context Reframed by Edits to Media). The doctored clip had 4,659 views as of writing.<br><br>In an email to Rappler, Stand for Truth said they never aired a report on Angel Locsin testing positive. ‚ÄúStand for Truth condemns this blatant resort to spreading fake news for the sake of getting more views online. We urge the public to practice responsible use of social media and refrain from sharing such items that only encourage the spread of misinformation,‚Äù it said.<br><br>In the original video uploaded by the official YouTube channel of GMA Public Affairs, Araullo talked about two new cases of COVID-19 in the country, but neither of them was Locsin. The new cases were the first two Filipinos who tested positive in the country on March 6.</i> |
| [I00172 FACT CHECK: Ex-president Duterte is alive](../../generated_pages/incidents/I00172.md) | <i>A TikTok video using the logo of GMA Newscast 24 Oras, published on June 19, reports the alleged death of the former president as supposedly announced by the Office of the Press Secretary. <br><br>The video already has over 157,000 reactions, 11,000 comments, 23,100 shares, and 6.7 million views as of writing. Similar misleading videos have been spreading across TikTok, Facebook, and YouTube, generating significant engagement.<br><br>The facts: Duterte is alive (T0160.002: Information is False). The Office of the Press Secretary has not released any statement or announcement relating to Duterte‚Äôs supposed demise. The former president appeared in a Facebook live video posted by Senator Bong Go on Thursday, June 20, showing he is fine. Go further emphasized that Duterte is ‚Äúalive and kicking‚Äù and is currently relaxing at home. <br><br>Contrary to the claim, Duterte is 79 years old, not 94.<br><br>Fidel Ramos death report: The footage used in the misleading video is a 24 Oras news report by GMA reporter Bernadette Reyes about the death of former President Fidel V. Ramos, who passed away in July 2022 at the age of 94.<br><br>The video‚Äôs creator altered the audio, replacing Ramos‚Äô name with Duterte‚Äôs (T0162.002: Edits Made to News Report which Reframe Context, T0162.008: Context Reframed by Edits to Media).</i> |
| [I00173 Edited ABC News clip fuels false claims of 1 million Ukrainian soldiers' deaths](../../generated_pages/incidents/I00173.md) | <i>[In December 2024 a] 15-second video showing Ukrainian soldiers walking with prosthetic limbs has gone viral on social media, claiming to be an ABC News report estimating the deaths of nearly one million Ukrainian soldiers in the ongoing war with Russia. The voiceover in the viral video states, "That only a small part, in just 2.5 years of war, about a million Ukrainian soldiers have died, and according to the most optimistic estimates, hundreds of thousands have lost limbs, these statistics are incredible, just incredible."<br><br>[...]<br><br>An X user shared this video and wrote, ‚ÄúABC NEWS: 'In just about 2.5 years of war about 1 million Ukrainian soldiers have died' and 'hundreds of thousands have lost limbs.' All of this was preventable. They lost an entire generation.‚Äù At the time of writing this check, the post had amassed over 250,000 views and more than 5,200 likes. <br><br>[...]<br><br>However, we found that a 2023 ABC News report about Ukrainian soldiers with prosthetic limbs was digitally altered to create the viral clip (T0162.002: Edits Made to News Report which Reframe Context). <br><br>A reverse image search of keyframes from the viral video led us to similar footage published by ABC News on March 28, 2023, headlined: ‚ÄúGood Samaritans outfit Ukrainian soldiers with prosthetics." The video mentioned two Ukrainian soldiers who lost their legs to a landmine in Ukraine and were treated with prosthetic legs in the United States. A similar video was uploaded to the ABC News YouTube channel on March 29, 2023.<br><br>The visuals in the viral video are identical to those in multiple instances of the original 2023 ABC News YouTube video. The initial two seconds of the viral clip can be seen from the timestamp of 2:33 to 2:36 seconds of the ABC News video (T0165.001: Clipped Content). In this segment, one of the soldiers can be seen walking, and the reporter in the background can be heard saying, ‚Äúinjured during a combat mission near Davydiv Brid, despite all he's lost,‚Äù unlike the viral video where the voiceover starts with, "That only a small part, in just 2.5 years of war."<br><br>[...]<br><br>The narration of the viral video is different from the original video, and the reporter does not mention any estimate of the killings of the Ukrainian soldiers in the war with Russia that began in February 2022. The only estimated figure comes from an expert at the timestamp of 3:59 minute mark, stating, "much more has to be done we don't know, exact number we believe in thousands of amputees."<br><br>This indicates that the viral video's visuals were taken from the ABC News video published in 2023. An unrelated audio, likely AI-generated (T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality), was added to it, confirming that the viral video is fabricated.<br><br>Deepfake-o-meter indicated a high likelihood of the audio clip being AI-generated, with its RawNet3 (2023) rating the clip at 91.3 percent AI-generated. Additionally, another tool, Resemble Detect, found the audio clip to be fake.</i> |
| [I00174 Academic journals, journalists perpetuate misinformation in their handling of research retractions, a new study finds](../../generated_pages/incidents/I00174.md) | <i>Journals can take months to years to retract unreliable research while journalists often fail to inform the public when the scientific discoveries they have reported on are later determined invalid or fraudulent.<br><br>The analysis reveals researchers play a role, too. Some researchers rely on and use discredited research in their work (T0163.003: Narrative Cites Retracted Academic Research) ‚Äî even years after a journal has withdrawn the research from publication because problems such as scientific misconduct, significant errors, plagiarism or ethics violations have rendered those results untrustworthy.<br><br>Lead author Stylianos ‚ÄúStelios‚Äù Serghiou explained the new study highlights key problems in the pipeline for communicating science to the public. In an email interview, he stressed the need to correct the scientific record as well as the public record following a retraction, ‚Äúespecially in the case of consequential or highly popularized articles.‚Äù<br><br>‚ÄúWhen highly popularized research is retracted, we need to consider the obligation to inform the public that this research is no longer considered credible by the scientific community,‚Äù Serghiou, who has a medical degree and completed a doctoral degree in epidemiology and clinical research at Stanford University last year, wrote to The Journalist‚Äôs Resource.<br><br>The best way to do that, though, is unclear, Serghiou and his colleagues explain in their new paper. When they looked at how people discussed a small sample of retractions on social media, they discovered that, in some cases, ‚Äúretraction was directly or indirectly associated with the promotion of the initial misinformation.‚Äù<br><br>Even though thousands of papers have been retracted over time, they remain rare. About 4 in 10,000 papers are retracted, Science magazine reported in 2018. Retraction Watch, a project of the nonprofit Center for Scientific Integrity, tracks retractions and has built a database of more than 26,000 retractions going back to 1756.<br><br>[...]<br><br>- Most papers and their retractions tend to draw similar amounts of attention from news outlets, social media and other sources. But the most popular papers received 2.5 times as much attention before they were retracted than afterward.<br><br>- A median 457 days passed between a paper‚Äôs publication and its retraction.<br><br>- Of the retracted papers examined, 31 were cited more than 100 times each in other research.<br><br>- The most common reason for retraction during the study period was duplication, when a researcher or group of researchers publish the same information in two different papers. The second most common was ‚Äúfake peer review,‚Äù meaning the authors cheated ‚Äî intentionally or unintentionally ‚Äî the peer-review process through which scholars vet and evaluate one another‚Äôs work. Plagiarism was the third most common reason.<br><br>- Tumor Biology had the largest retraction during the study period. In 2017, the journal published a retraction note withdrawing 103 articles due to fake peer review.<br><br>[...]<br><br>Serghiou explained that journalists should report on retractions to correct the public record, but it‚Äôs unclear the best way to do that. He told JR there could be unintended consequences when journalists correct the record ‚Äî potentially spurring social media users to promote the retracted findings as valid.<br><br>One retracted paper they studied, published in JAMA Pediatrics in 2012, claimed that putting an Elmo sticker on an apple encouraged children to choose apples over cookies. Before its retraction, people tweeted about the paper four times. The retraction, Serghiou noted, led to a flurry of tweets promoting the invalid finding. Someone tweeted a sentence suggesting stickers make children choose fruit over cookies and it was retweeted 742 times (T0163.003: Narrative Cites Retracted Academic Research).<br><br>‚ÄúIn our limited study of tweets about recent popular retracted articles, we were very surprised to witness that retraction without appropriate communication may lead to inadvertent promotion of the results being retracted,‚Äù Serghiou wrote to JR.</i> |
| [I00175 Many scientists citing two scandalous COVID-19 papers ignore their retractions](../../generated_pages/incidents/I00175.md) | <i>In June 2020, in the biggest research scandal of the pandemic so far, two of the most important medical journals each retracted a high-profile study of COVID-19 patients. Thousands of news articles, tweets, and scholarly commentaries highlighted the scandal, yet many researchers apparently failed to notice. In an examination of the most recent 200 academic articles published in 2020 that cite those papers, Science found that more than half‚Äîincluding many in leading journals‚Äîused the disgraced papers to support scientific findings and failed to note the retractions.<br><br>COVID-19 "is such a hot topic that publishers are willing to publish without proper vetting," even in the face of retractions that made global headlines, says Elizabeth Suelzer, a reference librarian at the Medical College of Wisconsin who has written about problematic citations to a retracted 1998 study in The Lancet falsely linking vaccination to autism.<br><br>Both of the retracted COVID-19 papers, one in The New England Journal of Medicine (NEJM) and the other in The Lancet, were based on what appeared to be a huge database of patient records compiled from hospitals worldwide by Surgisphere, a small company operated by vascular surgeon Sapan Desai, who was a co-author on each article. The 22 May 2020 Lancet paper ostensibly showed that hydroxychloroquine, an antimalarial drug promoted by President Donald Trump and others, could harm rather than help COVID-19 patients. Its publication led to a temporary halt in a major clinical trial and inflamed an already-divisive debate over the drug, which has proved to be no help against COVID-19. The 1 May NEJM article corroborated other evidence that people already taking certain blood pressure medicines did not face a greater risk of death if they developed COVID-19.<br><br>Questions soon arose about the validity, and even existence, of the Surgisphere database, however, and the retractions followed on 4 June. But of the 200 papers examined by Science‚Äîall published after the retractions‚Äî105 inappropriately cited one of the disgraced studies (T0163.003: Narrative Cites Retracted Academic Research). In several cases it was a primary source for a meta-analysis combining multiple studies to draw overarching conclusions. In most, the studies were cited as scientific support or context. Science also found a handful of articles that uncritically cited an influential April preprint (T0163.004: Narrative Cites Academic Research not Peer Reviewed) based on the same Surgisphere data set, which described the antiparasitic drug ivermectin as beneficial in critical COVID-19 cases. (There is no standard way to retract preprints, however.)<br><br>Ivan Oransky, co-founder of the website Retraction Watch, says such blunders occur because "people are either willfully or negligently not checking references." Many authors copy and paste lists of apparently relevant citations from similar papers without actually reading them, he says. "It's frightening. It's terrible, but common."</i> |
| [I00176 Japanese study misrepresented in posts claiming ‚Äúheart failure surges among Covid-vaccinated‚Äù](../../generated_pages/incidents/I00176.md) | <i>In February 2025, viral social media posts appeared, claiming that ‚ÄúJapan sounds alarm as heart failure surges among Covid-vaccinated‚Äù. These posts shared what appeared to be a screenshot of an article‚Äôs headline. One example was posted by the Facebook page KAG Save America 2024, which received more than 17,000 user engagements and more than 10,000 shares. Another example was this TikTok video, viewed more than 209,000 times.<br><br>The link shown in the image takes one to this study, published in the Journal of Infection and Chemotherapy, titled ‚ÄúSARS-CoV-2 mRNA vaccine-related myocarditis and pericarditis: An analysis of the Japanese Adverse Drug Event Report database‚Äù.<br><br>We were unable to find an article carrying the same headline shown in these posts, but we did find this article by Slay News, published on 18 January 2025, with a similar headline: ‚ÄúJapan Sounds Alarm as Heart Failure Surges 4900% Among Covid-Vaxxed‚Äù. This article also referenced the same study.<br><br>The Slay News article was shared on X by accounts with large followings, such as Patrick Soon-Shiong, the owner of the Los Angeles Times who‚Äôs endorsed U.S. Department of Health and Human Services (HHS) secretary nominee RFK Jr.; Jim Ferguson, a former candidate for the Brexit Party in the 2019 UK General Elections who‚Äôs spread vaccine misinformation; and Mary Talley Bowden, an otolaryngologist who spread COVID-19 and vaccine misinformation.<br><br>But the study doesn‚Äôt support the claim (T0163.002: Narrative Misrepresents Findings of Cited Academic Research), and the scientific evidence shows that on balance, COVID-19 vaccination reduces the risk of complications from the disease, including heart complications. We explain below.<br><br>What did the study do and what did it find?<br><br>The study examined the Japanese Adverse Drug Event Report database, which is the Japanese equivalent of the U.S. Vaccine Adverse Event Reporting System (VAERS). Like VAERS, this database collects information about adverse events occurring after vaccination. And like VAERS, a report alone isn‚Äôt sufficient evidence that a vaccine caused an adverse event.<br><br>Examining data from April 2004 to December 2023, the researchers found 880,999 adverse event reports following COVID-19 mRNA vaccination, of which 1,846 were myocarditis and 761 were pericarditis. The data showed that most affected individuals were male and aged 30 years or below. The period between vaccination to onset of symptoms was eight days and below. These findings are consistent with that of previous studies that found an association between COVID-19 mRNA vaccines and myocarditis.<br><br>The researchers concluded that ‚ÄúIn the Japanese population, SARS-CoV-2 mRNA vaccination was significantly associated with the onset of myocarditis/pericarditis [‚Ä¶] although most adverse events occurred early after vaccination, overall outcomes were good‚Äù. The study didn‚Äôt report any alarming trends, contrary to the implications in the posts.<br><br>Study didn‚Äôt look at heart failure; benefits of vaccines outweigh their risks<br><br>Contrary to the claim in the posts, the study was unrelated to heart failure, a condition in which the heart no longer pumps blood as well as it should. Myocarditis, on the other hand, is inflammation of the heart muscle, and it isn‚Äôt the same as heart failure. Heart failure can be a complication of myocarditis, but it‚Äôs important to note that myocarditis associated with COVID-19 vaccination is typically mild. Most patients go on to make a full recovery after rest and medication.<br><br>Furthermore, the study didn‚Äôt account for whether affected individuals had COVID-19 or other viral infections before developing myocarditis. This is important because one of the most common causes of myocarditis is infection by a virus, like those that cause the flu and the common cold. This is one reason the study cannot reliably assess whether myocarditis cases were actually caused by the COVID-19 mRNA vaccines.<br><br>Studies have shown that compared to COVID-19 vaccination, COVID-19 is associated with a greater risk of myocarditis, along with other complications like blood clots. The American Heart Association considers the benefits of the COVID-19 vaccines to outweigh their risks. An expert consensus by the American College of Cardiology found that ‚Äúa very favorable benefit-to-risk ratio exists with the COVID-19 vaccine for all age and sex groups evaluated thus far‚Äù.<br><br>Conclusion<br><br>Social media posts misrepresent a Japanese study about adverse events following COVID-19 vaccination (T0163.002: Narrative Misrepresents Findings of Cited Academic Research), falsely claiming it showed heart failure is surging in vaccinated people (T0160.002: Information is False). The study examined myocarditis cases following COVID-19 vaccination, but its findings are consistent with those reported in previous studies, and it didn‚Äôt mention heart failure.<br><br>While COVID-19 mRNA vaccines are associated with rare cases of myocarditis, particularly in young males, COVID-19 itself is associated with a greater risk of myocarditis, along with other complications. On balance, the benefits of the vaccines outweigh their risks.</i> |
| [I00177 No, study didn't blame COVID-19 vaccines for excess pandemic deaths | Fact check](../../generated_pages/incidents/I00177.md) | <i>The claim: A study identified COVID-19 vaccines as a cause of excess deaths<br><br>A June 5 Facebook post claims a new study has revealed alarming news about the COVID-19 vaccines.<br><br>‚ÄúNew study: 'Excess deaths' linked to COVID-19 vaccines in the West,‚Äù reads the post, which includes a link to an article from the website Right Side Broadcasting Network making the claim.<br><br>The post was shared more than 90 times in six days. Similar claims circulated on Facebook and X, formerly Twitter.<br><br>Our rating: False<br><br>The study cited in the post does not conclude COVID-19 vaccines were linked to the excess deaths (T0163.002: Narrative Misrepresents Findings of Cited Academic Research). It encouraged further studies to identify the causes.<br><br>Excess deaths remain high with no cause identified<br><br>The study referenced in the post and article was published June 3 in BMJ Public Health, a publication of the British Medical Journal. In it, researchers from the Netherlands stated that excess mortality in Western nations remained high from 2020 to 2022, even after the adoption of preventative measures and vaccines to slow or stop the spread of COVID-19.<br><br>While noting the correlation, the study was explicit in its conclusion ‚Äì and the British Medical Journal reiterated in a follow-up statement ‚Äì that the research did not identify a cause of the excess deaths.<br><br>‚ÄúExcess mortality has remained high in the Western World for three consecutive years, despite the implementation of containment measures and COVID-19 vaccines,‚Äù the study‚Äôs conclusion reads. ‚ÄúThis raises serious concerns. Government leaders and policymakers need to thoroughly investigate underlying causes of persistent excess mortality.‚Äù<br><br>As the misrepresentation of the report spread, the British Medical Journal released a statement further stressing that researchers didn't identify a cause for the excess deaths.<br><br>‚ÄúThe researchers looked only at trends in excess mortality over time, not its causes,‚Äù the statement said in part. ‚ÄúWhile the researchers recognize that side effects are reported after vaccination, the research does not support the claim that vaccines are a major contributory factor to excess deaths since the start of the pandemic. Vaccines have, in fact, been instrumental in reducing the severe illness and death associated with COVID-19 infection.‚Äù<br><br>The statement also notes that different methods of data reporting and varying data quality from nation to nation further complicate identifying causal relationships.</i> |
| [I00178 Research summary misleadingly cited as evidence Covid-19 shots are dangerous](../../generated_pages/incidents/I00178.md) | <i>A preliminary summary of research in an American Heart Association (AHA) journal claiming that common Covid-19 vaccines more than double the risk of heart problems is being cited as evidence the shots are dangerous. But the AHA says there are potential errors in the research, which has not been peer reviewed (T0163.004: Narrative Cites Academic Research not Peer Reviewed), and the US Centers for Disease Control and Prevention and an expert raised questions about its conclusion and methodology.<br><br>"Mrna COVID Vaccines Dramatically Increase Endothelial Inflammatory Markers and ACS Risk as Measured by the PULS Cardiac Test: a Warning," says the title of the abstract, which was published on November 8, 2021 on the website of the AHA journal Circulation.<br><br>The summary -- known as an abstract -- of the research by heart surgeon Steven Gundry says there is an increased five-year risk of Acute Coronary Syndrome (ACS) in patients, from 11 percent pre-vaccination to 25 percent after vaccination.<br><br>The abstract has been cited in multiple online articles raising concerns about the shots, and has also circulated on social media.<br><br>The research focuses on mRNA shots, which use technology that differs from traditional vaccines, and have been the subject of a number of inaccurate claims debunked by AFP Fact Check.<br><br>Shortly after the publication of the abstract, the AHA issued an expression of concern about the research by Gundry, who declined through a spokeswoman to comment on the issue.<br><br>"Soon after publication of the above abstract in Circulation, it was brought to the American Heart Association Committee on Scientific Sessions Program's attention that there are potential errors in the abstract," the AHA said.<br><br>"Specifically, there are several typographical errors, there is no data in the abstract regarding myocardial T-cell infiltration, there are no statistical analyses for significance provided, and the author is not clear that only anecdotal data was used," it added.<br><br>Michelle Kirkwood, the AHA's media relations director, told AFP that abstracts are not peer reviewed, meaning they have not been assessed by other scientists prior to publication.</i> |
| [I00179 Digging into the math of a study attacking the safety of the abortion pill](../../generated_pages/incidents/I00179.md) | <i>The Ethics and Public Policy Center, a think tank that says it opposes ‚Äúthe extreme progressive agenda while building consensus of conservatives,‚Äù recently issued a report on a key abortion medication, mifepristone, that it says raises questions about its safety. After analyzing insurance claims for more than 865,000 prescribed mifepristone abortions, the group said it had determined that almost 11 percent of women experienced a ‚Äúserious adverse event,‚Äù much higher than an overall 0.5 percent rate found in clinical studies.<br><br>That headline number led Republicans like Sen. Josh Hawley (Missouri) to declare that the abortion pill is dangerous. ‚ÄúThose are astounding, jaw-dropping numbers, and they have been largely hidden from the public,‚Äù he said in an opinion article in the Federalist. An adverse health event, he noted, is ‚Äúa major, potentially life-threatening medical disaster. We‚Äôre talking about things like sepsis, infection, and hemorrhaging, the kind of things that land you in an ER.‚Äù<br><br>Citing the report, Hawley on Thursday secured a commitment from Jim O‚ÄôNeill, the Trump administration nominee for deputy secretary of Health and Human Services, to review the safety of mifepristone.<br><br>A medication abortion using a combination of mifepristone and misoprostol in 2023 accounted for 63 percent of abortions, double the percentage in 2014, according to the Guttmacher Institute, which supports abortion rights. Since 1988, about 100 countries have approved the use of mifepristone, according to Gynuity Health Projects, which also favors abortion rights.<br><br>We dug into the EPPC report, which calls the use of mifepristone ‚Äúchemical abortions,‚Äù and posed 20 questions to EPPC, which were answered in detail via email by spokesman Hunter Estes after consultation with the researchers. The report lists 94,605 adverse events, for a rate of 10.93 percent, between 2017 and 2023. (Some women had more than one event, so the report says it avoided double-counting when calculating overall rate.) But the headline number seems less solid when the individual data presented in the report is examined.<br><br>We should note that, unlike most credible medical studies, the report did not undergo a formal external peer review before publication (T0163.004: Narrative Cites Academic Research not Peer Reviewed). Estes said ‚Äúthe study was internally reviewed and adjudicated by a panel of physicians, who carefully evaluated the clinical classifications, coding, and outcome assessments to ensure medical accuracy and consistency.‚Äù</i> |
| [I00180 RFK Jr.‚Äôs health report shows how AI slips fake studies into research](../../generated_pages/incidents/I00180.md) | <i>The authors of the ‚ÄúMake America Healthy Again‚Äù report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing ‚Äúcommon scientific basis‚Äù to shape health policy.<br><br>But that ‚Äúscientific basis‚Äù appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report‚Äôs citations were problematic, as NOTUS first reported. Four contained titles of papers that don‚Äôt exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles‚Äô findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they‚Äôre saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) ‚Äúis a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,‚Äù said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included ‚Äúoaicite‚Äù in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report‚Äôs legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) ‚Äî another error common to generative AI tools, which ‚Äúcan confidently produce incorrect summaries of research,‚Äù Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, ‚ÄúChanges in mental health and substance use among US adolescents during the COVID-19 pandemic.‚Äù The line that cited it read, ‚ÄúApproximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.‚Äù<br><br>A closer examination of the citation shows why it‚Äôs not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying ‚ÄúDOI not found,‚Äù and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed ‚Äúvague symptom lists that overlap with typical adolescent behaviors‚Äù and was linked to ‚Äúinappropriate parental requests for antidepressants.‚Äù<br><br>The new version of the report now reads, ‚ÄúDTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,‚Äù now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said ‚Äúan estimated 25-40% of mild cases are overprescribed,‚Äù citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article‚Äôs supposed first author, told NOTUS that was an ‚Äúovergeneralization‚Äù of his research.<br><br>The updated report removed those figures. It now reads, ‚ÄúThere is evidence of overprescription of oral corticosteroids for mild cases of asthma.‚Äù<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [I00181 Fake academic reference reveals AI use in claim for $1.6m from NDIS](../../generated_pages/incidents/I00181.md) | <i>A behaviour support worker used artificial intelligence to create a National Disability Insurance Scheme report (T0166: AI-Generated Content) that contained fake academic references (T0163.001: Narrative Cites Nonexistent Academic Research) and data that did not exist, raising concerns about junk reports adding pressure to an already overwhelmed NDIS.<br><br>The Administrative Review Tribunal found Tenarra Campey used AI to ‚Äúgenerate ideas‚Äù for the report for an NDIS participant, which ‚Äúreferenced an academic journal article that does not exist and incorrectly cited another article‚Äù.<br><br>[...]<br><br>Campey wrote on November 11, 2024, that extra support would help prevent concerning behaviour by the applicant towards his three younger siblings, a claim supported by citations to academic articles on bullying.<br><br>But at the hearing, counsel for the NDIA revealed that one of the articles, purportedly titled ‚ÄúSiblings, conflict and bullying‚Äù and published in the journal Developmental Review, did not exist (T0163.001: Narrative Cites Nonexistent Academic Research).<br><br>[...]<br><br>Campey expressed opinions in a letter of September 1, 2025, that she said was based on three weeks of data, but Purcell found the ‚Äúdata ‚Ä¶ did not exist at the time‚Äù (T0164.001: Narrative Presents Fabricated Statistics as Genuine Data) because it was collected from August 28 to September 17. </i> |
| [I00182 No, CDC COVID-19 vaccine data doesn't link shot to shorter life span | Fact check](../../generated_pages/incidents/I00182.md) | <i>A Sept. 4 Instagram post (direct link, archive link) shows a screenshot of a post on X, formerly Twitter, that claims a particular vaccine has been linked to shorter lifespans.<br><br>"BREAKING: According to new CDC data, the Covid vaccine could take 24 years off of your life," reads the post.<br><br>The post was liked more than 1,000 times in two weeks. The X post appears to have been deleted.<br><br>There is no evidence any data from the Centers for Disease Control and Prevention shows a link between COVID-19 vaccines and lower life expectancy. The dataset cited in a related article wasn't released until months after the claim was made. The claim originated with a website that frequently publishes misinformation.<br><br>The claim appears to stem from a Feb. 8 article posted on a website called "The Expose," which has previously published vaccine-related misinformation.<br><br>The article cites a study that looked at the effectiveness of a bivalent COVID-19 vaccine among Cleveland Clinic employees. The study found the vaccine "afforded modest protection overall" when given to working-age adults. It did not address life expectancy (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>The article also claims the CDC's all-cause mortality data shows "each vaccine dose" increased mortality by 7% from 2021 to 2022. However, provisional mortality data for 2022 from the CDC wasn't released until May 5, nearly three months after the article was published (T0164.001: Narrative Presents Fabricated Statistics as Genuine Data). It says the "overall age-adjusted U.S. death rate decreased by 5.3% from 2021 to 2022."</i> |
| [I00183 Fact Check: Antarctic ice data cherry-picked to make false global warming claim](../../generated_pages/incidents/I00183.md) | <i>A misleading (T0160.004: Information is Misleading) comparison of Antarctic sea ice levels on two specific days 27 years apart has been used by people online to falsely (T0160.002: Information is False) claim that climate change warnings are a scam.<br><br>Two satellite images were published alongside each other in Facebook and X posts, pointing out that sea ice levels in the Antarctic were similar in 1997 and 2024.<br><br>They ended the posts with ‚Äú#climatescam‚Äù, implying the images are evidence global warming is a hoax.<br><br>Citing data from the University of Colorado Boulder‚Äôs National Snow and Ice Data Center (NSIDC), the users said Antarctica‚Äôs sea ice extent reached 3.08 million square kilometres on March 9, 1997, compared with 3.17 million square kilometres on the same date in 2024.<br><br>The NSIDC told Reuters via email these figures do not match its records (T0164.001: Narrative Presents Fabricated Statistics as Genuine Data). It said the five-day average sea ice extent was 2.76 million square kilometres on March 9, 1997, and 2.80 million square kilometres on March 9, 2024.<br><br>Two other climate experts who spoke to Reuters said ‚Äúcherry-picked‚Äù dates comparing Antarctica‚Äôs sea ice extent would also not provide an accurate picture by itself of Earth‚Äôs changing climate (T0164.002: Narrative Uses Selective Statistics to Support Claim). This is because it would fail to show long-term trends, but also because scientists are unsure how climate change is affecting sea ice in this area of the planet.<br><br>Antarctic sea ice levels in 1997 were, alongside 1993, at their lowest that decade, according to an interactive chart from the NSIDC.<br><br>Longer-term data show the sea ice extent slightly increased up until 2015 before dropping from 2017. In 2023 it hit a record low.<br><br>The drop since 2017 has caught experts‚Äô attention but it is not yet clear if this is related to climate change, said University of Exeter climate professor James Screen, who spoke to Reuters by telephone.<br><br>‚ÄúThe sea ice has been steady for several decades and then we have seen a sudden decline, and scientists are now trying to understand whether this is now a signal emerging of climate change-driven loss or whether this is another kind of temporary blip," he said.<br><br>Screen said the trend has puzzled scientists for a while, and it can‚Äôt be solved by looking at two single data points and saying one is higher than the other.<br><br>He said of the social media posts: ‚ÄúThey just cherry-pick a couple of random points on a time series and try and use that to make a claim that is unsubstantiated in a long-term perspective." (T0164.002: Narrative Uses Selective Statistics to Support Claim)</i> |
| [I00184 Fact check: Pfizer, Moderna vaccines don't pose 'any obvious safety signals' in pregnancy, study found](../../generated_pages/incidents/I00184.md) | <i>The different coronavirus vaccines have one thing in common: They've been met with skepticism by some Americans, who have often shared misinformation about their potential side effects.<br><br>Many of these false claims center around pregnant women.<br><br>"The CDC have been caught manipulating data to justify giving pregnant women the COVID jab," an image posted July 10 on Instagram says. "Their study stated just 12% of women suffered a miscarriage after having the jab, the actual figure was 82%."<br><br>The image is a screenshot of a July 6 tweet by the Daily Expose, a London-based alternative news site that has previously shared misinformation regarding vaccine safety. The tweet was retweeted nearly 2,000 times in the first week, and the Instagram post has received over 1,200 likes since it was posted on July 10.<br><br>The post refers to an April 21 study from researchers at the U.S. Centers for Disease Control and Prevention and published by the New England Journal of Medicine.<br><br>A similar Instagram post claimed women who got vaccinated within 30 days to 20 weeks of becoming pregnant had an 82% miscarriage rate. The image received more than 6,400 likes before it was deleted. <br><br>The study researched the safety of COVID-19 vaccines based on messenger RNA, or mRNA, in pregnant women between Dec. 14, 2020, and Feb. 28. <br><br>[...]<br><br>The study data released so far actually showed only 12.6 % of pregnant women in the study had a miscarriage ‚Äì not 82%. The study concluded that vaccinations weren't a safety risk for pregnant women.<br><br>The Daily Expose, along with social media users who've shared the claim, reached the errant conclusion by altering the data that scientists used, experts say. <br><br>[...]<br><br>A total of almost 4,000 women from the pregnancy registry participated in the study. Of that group, 827 participants completed their pregnancy by the end of the study, a category that included healthy births, miscarriages and any other complications that ended the pregnancy.<br><br>Among those 827 completed pregnancies, 712 gave birth and 104 had a miscarriage. One case resulted in a stillbirth, and 10 resulted in either induced abortion or extrauterine pregnancy.<br><br>Those completed pregnancies consisted primarily of women at the very beginning or very end of their terms. The live birth group was, of course, at the end, and the miscarraiges generally occurred among those early on.<br><br>Out of the 104 miscarriages, 94 occurred within the first 13 weeks of the pregnancy ‚Äì the first trimester, which is when most miscarriages happen according to the Mayo Clinic.<br><br>That works out to a miscarriage rate of 12.6%. According to Mayo Clinic, that percentage is within the average miscarriage rate for the population at large, which is between 10% and 20%.<br><br>[...]<br><br>The Daily Expose article linked on the image shared on Instagram claims women who were vaccinated within the first 20 weeks of their pregnancy had an 82% rate of miscarriage.<br><br>The publication altered the data used to calculate the miscarriage rate in order to get that percentage. Researchers divided 104 ‚Äì the number of miscarriages recorded ‚Äì by 827 ‚Äì the total number of completed pregnancies at the time of analysis.<br><br>But the article argued because the 700 women who had live births were vaccinated in their third trimester, when miscarriages are less common, they shouldn't be used as part of the sample to analyze the miscarriage rate.<br><br>So, the Daily Expose changed the denominator and divided 104 by 127 (T0164.002: Narrative Uses Selective Statistics to Support Claim), which it claims is the number of women who were pregnant and were vaccinated in their first or second trimester. That results in the claimed 82% miscarriage rate.<br><br>[...]<br><br>Victoria Male, a reproductive immunology lecturer at Imperial College, called the new percentage "a meaningless statistic" in a Twitter thread.<br><br>Male explained why the reasoning is flawed: Using that new denominator only takes into consideration completed pregnancies in the first and second trimester, in which a miscarriage is the only result possible.<br><br>"Presumably the only reason that rate isn't 100% is because some people vaccinated at the end of the 2nd trimester completed their pregnancies within three months by giving birth," she tweeted.</i> |
| [I00185 Fact check: Post wrongly claims 118,000 'died suddenly' after COVID-19 vaccine release](../../generated_pages/incidents/I00185.md) | <i>A Dec. 11, 2022, Instagram post (direct link, archived link) shows a headline purporting to report on a CDC admission.<br><br>"CDC quietly confirms at least 118k Children & Young Adults have ‚ÄòDied Suddenly‚Äô in the USA since the roll-out of the COVID Vaccines," reads the headline from the The Expose, a website that has previously published misinformation.<br><br>The Instagram post is a screenshot of a Dec. 10, 2022, tweet that was retweeted more than 1,000 times in a month. The Instagram version generated over 4,000 likes in less than a month.<br><br>[...]<br><br>The Expose article bases its claim on data collected by the Organization for Economic Co-operation and Development from publicly available CDC reports. The reports calculate estimated excess deaths potentially related to the COVID-19 virus by comparing death data in pre-pandemic years to during the pandemic. <br><br>The Nov. 30, 2022, Expose article claims that data shows six-figure tallies of sudden deaths, but those are not at all the same thing as excess deaths (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim).<br><br>The CDC defines excess deaths as "the difference between the observed numbers of deaths in specific time periods and expected numbers of deaths in the same time periods."  |
| [I00186 Fact check: No evidence of miscarriage surge since vaccine rollout](../../generated_pages/incidents/I00186.md) | <i>While vaccines are generally considered safe for pregnant women and new mothers, this group's exclusion from COVID-19 vaccine trials has left health care professionals with no clear data to guide their patients. <br><br>But a new study released by the American Journal of Obstetrics and Gynecology in late March found that the vaccines based on messenger RNA, or mRNA, conferred good protection against the virus to both pregnant and lactating women, and likely their newborns. <br><br>These encouraging results come on the heels of social media buzz claiming that, instead of protection, the COVID-19 vaccines are causing pregnant women to miscarry. <br><br>"Miscarriages skyrocket 350% in six weeks due to k*vid vacsines (sic)," writes one Facebook user in a March 29 post. <br><br>"Hmm... & y'all said it was safe. y'all said it was just 'conspiracy theories' when talking about the effect it would have on women's fertility," writes another user in a March 30 post that includes a screenshot of a headline from Natural News, a known conspiracy theory site, asserting the same claim as the March 29 post but with a figure of "366%." <br><br>[...]<br><br>So where does this 350% or 366% figure cited in the Facebook posts come from? <br><br>The source appears to be London-based alternative news site The Daily Expose, which, according to its website, markets its mission as "to report the facts that the mainstream refuse to."<br><br>In late March, The Daily Expose claimed that data from the U.K.'s Medicines and Healthcare products Regulatory Agency's Yellow Card Scheme (the British equivalent to the U.S. Vaccine Adverse Event Reporting System) showed an increase in miscarriages over a six-week period. <br><br>"Using data inputted to the MHRA Yellow Card Scheme (from Dec. 9, 2020) up to 24th January 2021 a total of 4 women had suffered a miscarriage as a result of having the Pfizer/BioNTech vaccine," that article claims, including two more after vaccination with AstraZeneca's vaccine.  <br><br>After Jan. 24 to March 7, the total count goes up to 28 miscarriages for both vaccines which, accordingly, is a 366% increase. USA TODAY was able to verify the exact numbers provided in the January and March reports. <br><br>While the Daily Expose's arithmetic isn't wrong, its conclusion isn't exactly right.<br><br>"There is no pattern to suggest an elevated risk of miscarriage related to exposure to the COVID-19 vaccines in pregnancy," the MHRA said in a statement to Reuters. <br><br>The agency explained the number of women vaccinated between December to March had to be considered alongside the expected frequency of miscarriage in a population.<br><br>"The numbers of people who have received a 1st dose COVID-19 vaccination increased from 1,340,043 to 4,322,791 for the same time frame. At least half of these would be expected to be women, so the number of women of child-bearing age (taking the vaccine) is estimated to have increased from 665,424 to 2,146,866 for the same time frame," the MHRA said. <br><br>It is estimated that as many as 26% of all pregnancies end in miscarriage, with nearly 80% of early miscarriages occurring in the first 12 weeks, or first trimester, according to the American College of Obstetricians and Gynecologists.<br><br>Given this, the MHRA acknowledged "some miscarriages would be expected to occur following vaccination purely by chance." <br><br>The Daily Expose article also claims two separate incidences of a premature birth and a stillbirth following vaccination. The MHRA explained that in the U.K., around 8 in 100 births are premature, so some would be expected to happen after vaccination also "purely by chance."  <br><br>The MHRA, however, disputed the claim of the stillbirth, telling Reuters some events can be incorrectly reported. It confirmed "no actual stillbirths" were reported to the agency at the time of its statement to Reuters in late March. <br><br>[...]<br><br>We rate this claim MISSING CONTEXT because without additional context it might be misleading (T0162: Reframe Context, T0160.004: Information is Misleading). Claims of miscarriages increasing by 366% over a six-week period originate from a U.K.-based alternative media site, The Daily Expose, citing data from the U.K.'s regulatory body, Medicines and Healthcare products Regulatory Agency. While the total counts of miscarriages are accurate, The Daily Expose article fails to take into account the increased number of women being vaccinated over the six-week time period (from December 2020 to March) alongside the expected frequency of miscarriages in the general population, which is around 26% (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim). The MHRA has stated there is no cause-and-effect association between COVID-19 vaccinations and the incidence of miscarriage. </i> |
| [I00187 Fact Check: False headline resurfaces to push COVID scam conspiracy](../../generated_pages/incidents/I00187.md) | <i>An inaccurate article has resurfaced on social media to wrongly suggest 99% of COVID-19 deaths were primarily caused by something other than the virus (T0160.002: Information is False).<br><br>Social media posts shared a screenshot of a long-since corrected Mail Online headline about weekly COVID fatalities in the U.S.<br><br>‚Äú99% of 'Covid deaths' not primarily caused by the virus, CDC data shows,‚Äù said the headline in the screenshot, referring to figures published by the U.S. Centers for Disease Control and Prevention for the week ending Aug. 19, 2023.<br><br>The posts suggested the screenshot was evidence the COVID pandemic was a scam.<br><br>‚ÄúWe, the allegedly dangerous conspiracy theorists, were right from the start. The measures were deadly, not the common cold itself,‚Äù said one Dec. 10, 2024, post sharing the screenshot on X, which received more than 315,000 views.<br><br>However, the original article and its headline misrepresented the CDC data (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim), which initially indicated 1.7% of total U.S. deaths in the week to Aug. 19, 2023, were primarily due to COVID.<br><br>The article, published on Aug. 28, 2023, interpreted the data as showing deaths where COVID was the primary cause as a percentage of all COVID-linked deaths.<br><br>The Mail Online updated the article on Aug. 29, 2023, with a new headline that said: ‚ÄúCovid to blame for just 1% of weekly deaths from all causes across the US, CDC data shows.‚Äù<br><br>At the bottom of the article, it said: ‚ÄúAn earlier version of this article claimed 99 percent of Covid deaths in the past week were not primarily caused by the virus. In fact, a footnote at the bottom of the CDC's Covid data tracker explains the percentage of all reported deaths attributed as Covid-19 is calculated based on the number of deaths from all causes. We have amended the article to reflect this.‚Äù<br><br>[...]<br><br>The CDC data showed 1.7% of all deaths - later updated to 1.6% - were primarily caused by COVID, not that 99% of COVID-related deaths were caused by something other than the virus. The Mail Online corrected its headline in 2023.</i> |
| [I00188 Fact check: CDC's data on COVID-19 deaths used incorrectly in misleading claims](../../generated_pages/incidents/I00188.md) | <i>The claim: Only 6% of reported COVID-19 deaths were the result of the coronavirus<br><br>A regular update of data on COVID-19 deaths by the Centers for Disease Control and Prevention has prompted a groundswell of claims that only a fraction of people have actually died directly from the novel coronavirus.<br><br>The misleading and inaccurate conclusions come from recently released statistics on comorbidities data in the CDC's weekly coronavirus update, with statistics as of Aug. 26 (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim).<br><br>‚ÄúFor 6% of the deaths, COVID-19 was the only cause mentioned. For deaths with conditions or causes in addition to COVID-19, on average, there were 2.6 additional conditions or causes per death,‚Äù the report reads.<br><br>The data, which is maintained by the CDC‚Äôs National Center for Health Statistics, is an aggregated list of comorbidities on death certificates reported to the agency.<br><br>[...]<br><br>The CDC defines a comorbidity as when ‚Äúmore than one disease or condition is present in the same person at the same time.‚Äù A comorbidity is often a chronic condition that a person can live with, such as arthritis, diabetes or obesity.<br><br>When a person dies, the cause and manner of death of death are determined separately from any comorbidities that may have been present. A person who takes his own life, for instance, has suicide listed for a cause of death, with any comorbidities he may have had documented separately.<br><br>The CDC regularly records comorbidities data for a range of diseases affecting Americans, including cancer and opioid overdoses.<br><br>As with COVID-19, it is possible that a person‚Äôs preexisting conditions exacerbated his or her death in the case of a drug overdose, for instance, but it is inaccurate to say the main cause of death was not the overdose itself.<br><br>‚ÄúIt seems clear that comorbidities, leading to compromised immune systems, play an important role in COVID-19 deaths, and in such cases attributing deaths to a single cause is at best questionable,‚Äù Roderick Little, a professor of biostatistics at the University of Michigan, told USA TODAY.<br><br>[...]<br><br>‚ÄúThese data come from death certificates, and the death certificate is designed to only capture information on causes of death,‚Äù Anderson said, explaining that COVID-19 would then not be "an incidental or trivial factor."<br><br>"The underlying cause of death is the condition that began the chain of events that ultimately led to the person‚Äôs death," Jeff Lancashire, acting associate director for communications at the National Center for Health Statistics, told PolitiFact.<br><br>"In 92% of all deaths that mention COVID-19, COVID-19 is listed as the underlying cause of death," Lancashire said.</i> |
| [I00189 Fact check: False claim COVID-19 vaccines caused 1.1 million deaths](../../generated_pages/incidents/I00189.md) | <i>Since COVID-19 vaccines first became available in late 2020, vaccine skeptics have twisted public health data to say the vaccines are actually harmful to recipients.<br><br>One such claim comes from a Nov. 24 article from The Expose, a website that has published multiple pieces of misinformation about COVID-19 vaccines.<br><br>‚ÄúSecret CDC Report reveals at least 1.1 Million Americans have 'Died Suddenly' since the COVID Vaccine roll-out & another Government Report proves the COVID Vaccines are to blame,‚Äù reads the article's headline.<br><br>The piece was shared more than 700 times on Facebook in a month, according to the social media analytics tool CrowdTangle.<br><br>But the article is incorrect (T0160.002: Information is False). The data source it cites does not list any cause for reported excess deaths in the U.S. Experts say COVID-19 infections make up most of those deaths (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim).<br><br>The Expose article is based on data collected by the Organization for Economic Co-operation and Development from publicly available CDC reports that show there were more than 1 million excess deaths during the course of the COVID-19 pandemic.<br><br>The report compares the average number of deaths in a given week from 2015-2019 to the period between December 2020, when vaccines first were authorized in the U.S., and the week of Sept. 25, 2022. It does show 1.1 million more deaths than would be expected historically.<br><br>There is, however, no publicly available evidence from the CDC or elsewhere to suggest those deaths were the result of the COVID-19 vaccines, according to Miguel Gorman, a spokesman for the organization, which compiles and tracks COVID-19 data.<br><br>‚ÄúWe don‚Äôt currently have information regarding cause of death to determine what caused the excess deaths,‚Äù Gorman said in an email.<br><br>Other experts pointed to COVID-19 itself as the culprit.<br><br>‚ÄúThe bulk of the excess deaths were a direct result of COVID-19 infections, but pandemics have major cascading impacts on all aspects of society,‚Äù Amesh Adalja, senior scholar at the Johns Hopkins Center for Health Security, told The Washington Post.<br><br>For example, Adalja suggested the pandemic could have indirectly led to a rise in drug overdoses, as drug users were less likely to seek treatment and more likely to be in isolation.<br><br>[...]<br><br>The post from The Expose also points to data from the United Kingdom it says bolsters its claim that the vaccine is causing deaths instead of saving lives, but that analysis falls apart as well.<br><br>The article cites a July data set from the Office for National Statistics, the U.K.‚Äôs national statistical institute. The Expose claims the data show that vaccinated people are more likely to die than the unvaccinated across all age groups, then tries to apply what it says are death rates from the data to the excess deaths in the U.S.<br><br>But that analysis makes several mistakes, according to Sarah Caul, head of mortality analysis at the office. Among them is the same mistake made with the data aggregated by the Organization for Economic Co-operation and Development ‚Äì the data doesn't show a causal link between the vaccines and deaths (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim).<br><br>A different Office for National Statistics data set shows just 59 deaths reported in England and Wales from March 2020 to November 2022 where vaccines were mentioned on the death certificate as an underlying or contributory cause of death. <br><br>The Expose‚Äôs article is also inconsistent with which subgroups are used to represent each age group, particularly when it comes to how long it has been since the last dose of a vaccine, she noted. It does not account for factors such as the health of vaccine recipients and the size of the subgroups.<br><br>USA TODAY has previously debunked claims published by The Expose on public health, including baseless assertions that the CDC manipulated data about the miscarriage rate in vaccinated women and that monkeypox is a side effect of COVID-19 vaccines.<br><br>AFP and PolitiFact also debunked the claim that COVID-19 vaccines were causing excess deaths.<br><br>Based on our research, we rate FALSE the claim that COVID-19 vaccines caused 1.1 million excess deaths in the U.S. Public health data points to the disease as a major contributor to excess deaths, not the vaccine. Research shows the vaccines to be safe and effective at preventing deaths. The claim originated with a website that has repeatedly made false claims about COVID-19. </i> |
| [I00190 Chris Christie Said Teachers Should Only Make Minimum Wage?](../../generated_pages/incidents/I00190.md) | <i>On 24 August 2015, the web site Business Standard News published a fake news article reporting that New Jersey governor (and Republican presidential hopeful) Chris Christie said that teachers were overpaid and should make minimum wage plus a bonus based on the number of students who pass standardized tests (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution):<br><br>‚Äú"Teachers are paid too much, that‚Äôs what‚Äôs bankrupting the system,‚Äù Christie said. ‚ÄúSome teachers make six-figure salaries and that‚Äôs not including retirement benefits.‚Äù<br><br>‚ÄúChristie suggested a new teacher pay structure. He proposed teachers get paid minimum wage and a bonus for every student that passes standardized tests.<br><br>‚Äú‚ÄúTeacher pay should be determined by how well their students are doing,‚Äù Christie said. ‚ÄúThey need to be held accountable. If students are failing, teachers are nothing more than glorified baby sitters.‚Äù‚Äù<br><br>The above-quoted story is a work of fiction from a satirical (i.e., fake news) site (T0160.005: Content Produced as Satire). A disclaimer on the Business Standard News (aka "B.S. News") site states that it was designed to "parody" real news (T0143.004: Parody Persona, T0097.202: News Outlet Persona):<br><br>‚ÄúThe Business Standard News is a satirical site designed to parody the 24-hour news cycle. The stories are outlandish, but reality is so strange nowadays they could be true.‚Äù<br><br>The Business Standard News has previously published fake stories about Fox News' hiring Stacey Dash to "attract old white dudes" and George Zimmerman's being engaged as a Fox News contributor.</i> |
| [I00191 Fact Check: False posts say AP reported on Trump ‚Äòchild molestation charges‚Äô](../../generated_pages/incidents/I00191.md) | <i>The Associated Press did not say prosecutors were ‚Äúreconsidering‚Äù bringing child rape and molestation charges against former U.S. President Donald Trump, contrary to baseless posts on social media (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>Facebook posts say, ‚ÄúBREAKING NEWS. Prosecutors Are Reconsidering Bringing Charges Against Former President Donald J. Trump On Child Rape And Molestation Charges. - AP News.‚Äù<br><br>Lauren Easton, a representative for the AP, said in an email that the agency did not report any such story.<br><br>No such article or alert exists on the AP website.<br><br>The posts surfaced days following the July 1 release of transcripts from the prosecution of disgraced financier Jeffrey Epstein in 2006  (T0068: Respond to Breaking News Event or Active Crisis). The transcripts, ordered by Florida Judge Luis Delgado, contain almost 200 pages of details about Epstein including first-hand reports from victims and settlements with the victims, the BBC reported.<br><br>While Trump has been mentioned in previous Epstein-related documents, he has not been accused of wrongdoing, the BBC reported.<br><br>There are no credible news reports about any child molestation charges against Trump (T0160.002: Information is False).</i> |
| [I00192 Fact Check: False Biden quote about NATO chief‚Äôs wife shared online](../../generated_pages/incidents/I00192.md) | <i>In remarks made during NATO‚Äôs 75th anniversary summit, U.S. President Joe Biden told NATO Secretary-General Jens Stoltenberg that he had spoken to Stoltenberg‚Äôs wife, not that he had had sex with her as suggested in social media posts sharing a wrong transcription of Biden‚Äôs remarks (T0162: Reframe Context).<br><br>On July 9 Biden pledged to defend Ukraine against Russia‚Äôs invasion in his remarks to NATO member states at the summit. He closed his speech by surprising Stoltenberg with the Presidential Medal of Freedom, the highest U.S. civilian award, and made the reference to Stoltenberg‚Äôs wife in the 15-second excerpt shared online.<br><br>The clip shows Biden behind a lectern with Stoltenberg standing to his right. Posts captioned the video, ‚ÄúHow is the media going to spin this?!‚Äù and quoted Biden as saying: ‚ÄúI realized I was fvckin‚Äô your wife?‚Äù<br><br>In fact, Biden said, ‚Äú‚Ä¶as I was talking to your wife‚Ä¶.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0162: Reframe Context)<br><br>Biden‚Äôs remarks can be seen in a White House video of the event. His words, starting at timestamp 29:38, are as transcribed by the White House : ‚Äú ... guided this alliance through one of the most consequential periods in its history. I realize I - as I was talking to your wife - I personally asked you to extend your service. (Laughs.) Forgive me. (Laughter.) And you put your...‚Äù<br><br>The White House video includes an inset with an American Sign Language interpreter, who also translated the quote as it was originally uttered by Biden, not as it was transcribed in social media posts, according to Jordan Wright, communications director of the Registry of Interpreters for the Deaf who reviewed the video at Reuters‚Äô request.<br><br>‚ÄúWe found the interpreting of President Biden‚Äôs remarks at the NATO summit on July 9th that you inquired about to be organic and appropriate,‚Äù Wright said in an email.</i> |
| [I00193 Fact Check: CNN did not report that Harris made 17 false claims during the debate](../../generated_pages/incidents/I00193.md) | <i>CNN did not report that Democratic presidential candidate Kamala Harris made 17 false claims in the first ten minutes of the Sept. 10 presidential debate with Republican rival Donald Trump (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). The misleading online narrative was originally shared as satire (T0162.011: Content Originally Produced as Satire Presented as Not Satire).<br><br>A day after the debate, Facebook accounts posted: ‚ÄúCNN's official fact-checker says Kamala lied 17 times in the first ten minutes.‚Äù<br><br>A CNN spokesperson declined to comment but the news outlet‚Äôs reporting makes no mention that Harris lied 17 times in the first 10 minutes, or across the duration of the debate, as the viral posts suggest.<br><br>A Sept. 11 CNN report analyzing several statements made by both candidates said Trump delivered more than 30 false claims during the debate while Harris made one.<br><br>The report identified Harris‚Äô claim that Trump left office ‚Äúwith the worst unemployment rate since the Great Depression,‚Äù as false.<br><br>According to CNN, Harris also made two misleading statements and three others that lacked context.<br><br>A video report featuring CNN fact-check reporter Daniel Dale echoes the same findings.<br><br>The viral narrative could be traced to a Sept. 10 post, on the Facebook page ‚ÄúAmerica - Love It Or Leave It.‚Äù<br><br>The page describes itself as a subsidiary of ‚ÄúAmerica's Last Line of Defense network‚Äù that posts satire (T0160.005: Content Produced as Satire) and parody content (T0143.004: Parody Persona). ‚ÄúNothing on this page is real,‚Äù the bio reads.</i> |
| [I00194 Fact check: False quote attributed to Nelson Mandela](../../generated_pages/incidents/I00194.md) | <i>Social media users have been sharing a meme featuring a picture of British government front-bench ministers alongside a quote on nationalisation attributed to Nelson Mandela. There is no evidence Mandela ever made the statement quoted (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>[...]<br><br>The meme shows a photo of British Prime Minster Boris Johnson sitting in the House of Commons alongside prominent cabinet ministers such as Rishi Sunak, Priti Patel and Matt Hancock.<br><br>Above this image is a quote that is attributed to South Africa‚Äôs first black head of state: ‚Äò‚ÄúWhen you see that the government and their relatives are looting the country, you have to disobey, rise up, get on the streets, drive them out of the country, send them in to exile, and nationalise their illegal gains.‚Äù Nelson Mandela‚Äô.<br><br>The meme features a watermark belonging to a Facebook account called ‚ÄúThe Ragged Trousered Philanderer,‚Äù which posted the image on Oct. 11, 2020 with a caption admitting there was no evidence to suggest the quote was genuine. That post was shared 10,000 times on Facebook.<br><br>The image was then shared by other social media users across multiple platforms. One example can be seen, shared by a retired boxer with a blue tick Twitter account and 128,300 followers.<br><br>Reuters found earlier examples of the quote being sourced to Mandela, but no indication is provided of where this statement was made (T0160.007: Claim Previously Fact Checked).<br><br>The Nelson Mandela Foundation told Reuters they could not authenticate the quote after searching their database of Mandela‚Äôs writings, interviews and speeches (www.nelsonmandela.org/). A web search for the phrasing leads only to the social media memes.</i> |
| [I00195 Fact check: False quote attributed to Michael Kors about African Americans ](../../generated_pages/incidents/I00195.md) | <i>Social media users have been sharing images online with a quote attributed to designer Michael Kors that says, ‚ÄúI‚Äôm tired of pretending that I like blacks.‚Äù This claim is false (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False). <br><br>[...]<br><br>Some versions of the post show a screenshot from a 2015 article on now-deleted website, nahadaily.com [...] The website discloses at the bottom of the page that it is satire (T0143.004: Parody Persona).  <br><br>Another version shows a screenshot of a tweet from an account called ‚ÄúTMZ Breaking News‚Äù, featuring the same wording from the satirical article (T0162.011: Content Originally Produced as Satire Presented as Not Satire). The account uses TMZ‚Äôs name and logo but discloses in its bio that it is not actually affiliated TMZ. (T0143.003: Impersonated Persona).<br><br>[...] Although the claim stems from a satirical article, it has since been taken seriously.  <br><br>A quote of this nature from well-known designer Michael Kors would have been reported on widely by major news organizations. Reuters found no evidence Kors made this statement. <br><br>Michael Kors released several statements in support of the Black Lives Matter movement on social media.</i> |
| [I00196 Fact check: False quote by Hank Williams Jr. about being offended ](../../generated_pages/incidents/I00196.md) | <i>Social media users have been sharing content featuring a quote attributed to musician Hank Williams Jr. saying, ‚ÄúIf you are offended by something then leave it alone‚Äù.  <br><br>[...]<br><br>The lengthy quote lists various things the reader may be offended by and tells them to ‚Äúleave it alone.‚Äù  <br><br>Towards the end, the quote adds: ‚ÄúWhy are the things you want so much more important than what I want? Are your demands greater than my likes? Is it because you use intimidation, blackmail and force to get your way- that you do? Let‚Äôs make a deal. I will leave what you like alone and you leave what I like alone and the world will be a better place for everyone. copy and paste or share (T0120: Incentivise Sharing) cause it‚Äôs exactly how I feel!! ENOUGH already people!!!‚Äù <br><br>Hank Williams Jr is an American musician and the son of country musician Hank Williams. When asked about the 2016 election, Williams told Rolling Stone: ‚Äú‚ÄôWhat about the election?‚Äô they say. I say, ‚ÄòI don‚Äôt give a shit about the election, I‚Äôve got a smash CD coming out!'‚Äù. Williams has been outspoken about politics in the past. <br><br>The posts do not give a time or place where Williams said this and Reuters could not find any evidence of the quote being authentic. The quote appears on meme pages, social media posts and blogs. It does not appear on Williams‚Äô social media or website.  <br><br>Williams‚Äô representative Ken Levitan confirmed to Reuters via email: "This is a fabricated quote.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False)</i> |
| [I00197 Fact check: False Theodore Roosevelt quote about liberals and conservatives](../../generated_pages/incidents/I00197.md) | <i>Posts circulated on social media attribute a quote on liberals and conservatives to the 26th President of the United States, Theodore Roosevelt. The quote, however, is falsely attributed to him (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False).<br><br>The posts show a photograph of Roosevelt alongside text which reads: ‚ÄúTo anger a conservative, lie to him. To anger a liberal, tell him the truth." The posts do not point to a source for the quotation, nor a possible date.<br><br>[...]<br><br>On social media, users have responded with conflicting reactions. Some have concurred with the sentiment (one posted: ‚ÄúGosh‚Ä¶that‚Äôs dead on‚Äù), while others have disputed the authenticity of the quotation.<br><br>Reuters found no mention of this quote among those compiled by the Theodore Roosevelt Association, a nonprofit organization dedicated to perpetuating his memory and ideals.<br><br>The quote did not appear among the list of notable quotations listed by the Theodore Roosevelt Center at Dickinson State University, another organization working to preserve Roosevelt‚Äôs legacy.<br><br>In the past, the quotation has been attributed to other statesmen like Winston Churchill. On other social media sites, the quotation has circulated with attribution to Roosevelt, as anonymous, or attributed to an ‚Äúunknown‚Äù source since 2009.<br><br>In 2013, Barry Popik, a contributor to the Oxford English Dictionary, the Yale Book of Quotations and the Dictionary of Modern Proverbs, found that the quote had been in print since about 2007, as first reported by Politifact. ‚ÄúWell after Roosevelt‚Äôs lifetime,‚Äù Popik noted in a blog entry.<br><br>Throughout the years, other fact-checkers have debunked this claim (T0160.007: Claim Previously Fact Checked).</i> |
| [I00198 Fact check: False Ruth Bader Ginsburg quote on age of consent](../../generated_pages/incidents/I00198.md) | <i>An image on social media makes the claim that Associate Supreme Court Justice Ruth Bader Ginsburg said ‚Äúthe age of consent for sexual acts must be lowered to age (sic) 12 years old.‚Äù Ruth Bader Ginsburg never said this (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False).<br><br>The alleged quote has its origin in 1993 hearings on Bader Ginsburg‚Äôs nomination to the Supreme Court by President Bill Clinton.<br><br>Susan Hirschmann presented this claim as fact at a July 1993 judiciary hearing. ‚ÄúThe age of consent must be lowered to 12 years old‚Äù is Hirschmann‚Äôs interpretation of a recommendation put forth by Bader Ginsburg in the report ‚ÄúSex Bias in the U.S. Code‚Äù, published by the U.S. Commission on Civil Rights (USCCR).<br><br>In August 1993, Thomas L. Jipping similarly presented a series of arguments against Bader Ginsburg‚Äôs nomination, among them referencing the same USCCR report co-authored by Bader Ginsburg. He argued that, in this report, Bader Ginsburg‚Äôs recommendations included drafting women into the military, legalizing prostitution, constitutionally protecting bigamy, and ‚Äúlowering the age of consent for sexual acts to 12 years.‚Äù<br><br>Sex Bias in the U.S. Code, the USCCR report, is visible online. It was published in 1977, a date also referenced in some of the Facebook claims.<br><br>A section beginning on page 95 described how the law stood at the time (‚ÄúUnder 18 U.S.C. 1153 and 2032, it is a crime for a person to have carnal knowledge of a female not his wife who has not reached 16 years of age‚Äù) and noted that the legal definition of rape assumed the offender was a man and the victim was a woman. The report argued: ‚ÄúThese provisions clearly fail to comply with the equal rights principle. They fail to recognize that women of all ages are not the only targets of sexual assault; men and boys can also be the victims of rape.‚Äù<br><br>On page 102, the report recommended removing the phrase ‚Äúcarnal knowledge of any female, not his wife who has not attained the age of sixteen years‚Äù and replacing it with ‚Äúa Federal, sex-neutral definition of the offense patterned after S. 1400 section 1633‚Äù.<br><br>The report goes on to quote directly from S. 1400, which was a proposed Senate bill under which a person would be guilty of an offense if they compelled someone else to take part in sex by force or threats, by drugging or intoxicating them, or if ‚Äúthe other person is, in fact, less than 12 years old‚Äù.<br><br>Bader Ginsburg was arguing in the report for a broader, gender-neutral definition of rape. The passage that her critics cite as evidence she favored lowering the age of consent is actually a quote from a proposed Senate bill (T0162: Reframe Context), not from Bader Ginsburg (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). The focus of Bader Ginsburg‚Äôs argument was not on the age of consent, but on removing an antiquated definition that assumed only women could be targets of sexual assault.<br><br>Bader Ginsburg had put forward the same argument in an earlier report, ‚ÄúThe Legal Status of Women under Federal Law‚Äù, co-authored with Brenda Feigen Fasteau in 1974 (see Title 18 section, page 78 of PDF). Once again, the paper quotes directly from the 1973 Senate bill, S. 1400, as providing ‚Äúa definition of rape that, in substance, conforms to the equality principle‚Äù.<br><br>Over time, the misinterpretations of Bader Ginsburg‚Äôs arguments in these reports have led to the type of claims seen in the Facebook posts where direct quotes are falsely attributed to her.<br><br>Slate tackled this misinformation when Senator Lindsey Graham repeated the claim in 2005, as has the Washington Post and Snopes (T0160.007: Claim Previously Fact Checked).<br><br>Other iterations of this claim include the attribution of the false quote ‚Äúpedophilia is good for the children‚Äù to Bader Ginsburg (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>False. This is not a direct quote by Ruth Bader Ginsburg. It stems from a false interpretation by third parties of legal recommendations presented by Bader Ginsburg in the 1970s.</i> |
| [I00199 Fact check: False Donald Trump quote on Africans  ](../../generated_pages/incidents/I00199.md) | <i>Social media users have been sharing images of a newspaper clipping of an article with an offensive quote about African people, falsely attributed to Donald Trump, in the headline (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False). <br><br>[...]<br><br>The newspaper article is dated October 27, 2015 and consists mostly of alleged quotes from Trump. The first two paragraphs read: ‚ÄúONCE AGAIN, US business magnate Donald Trump has expressed his deep disgust for Africans by referring to them as lazy fools only good at eating, lovemaking and thuggery. Speaking in Indianapolis, Trump who is also the republican Presidential torch bearer, reiterated his promise to deport Africans especially those of Kenyan origin including their son Barrack (sic) Obama.‚Äù <br><br>The newspaper article cites ‚Äúnewzimbabwe‚Äúat the end. A now-deleted online article on newzimbabwe.com (T0097.202: News Outlet Persona) ( archive.vn/PcNDE ) stems from a similarly deleted article published by a website called Politica on October 25, 2015 ( archive.vn/VXjGj ). Politica appears to be a now-defunct blog-style website (T0152.002: Blog Asset). <br><br>[...]<br><br>Reuters could not find any record of Trump making such remarks or of him speaking in Indiana in October 2015. At the time, he was the Republican front-runner for the 2016 election and any comments of the kind attributed to him in the article would have gained worldwide media attention. <br><br>VERDICT<br><br>False. This article was fabricated in 2015 on a now-deleted website. </i> |
| [I00200 Fact check: False Jimmy Carter quote on taxes, helping the poor and Christian values](../../generated_pages/incidents/I00200.md) | <i>Tens of thousands of users on social media are sharing a quote purportedly said by Jimmy Carter, regarding taxes helping the poor and Christian values. The quote has been wrongly attributed to the 39th president of the U.S. It was said by comedian John Fugelsang (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False).<br><br>Most iterations of the claim feature a photograph of Carter and read: ‚ÄúIf you don't want your tax dollars to help the poor, then stop saying that you want a country based on Christian values. Because you don't!"<br><br>[...]<br><br>Earlier iterations of the quote on social media dating back to 2013 do not refer to Jimmy Carter but to the actor and comedian John Fugelsang.<br><br>Fugelsang confirmed to Reuters via Twitter direct message that the quote is indeed his and not Carter's.<br><br>He made this remark on the talk show Viewpoint, featured on Current TV. Reuters was unable to find the segment, which is no longer available on Current TV‚Äôs website. Snopes, who debunked the claim back in 2014 (T0160.007: Claim Previously Fact Checked), reported that the segment aired on May 29, 2013 and provides a transcript of Fugelsang‚Äôs full quote .<br><br>During the segment, Fugelsang made this remark to criticize Tennessee congressman Stephen Fincher, who had recently quoted a Bible passage to allegedly justify billions of dollars in cuts to the Supplemental Nutrition Assistance Program (cs.pn/2Z3AGOP ).</i> |
| [I00201 Fact check: False quotes attributed to Nancy Pelosi, Joe Biden and Alexandria Ocasio-Cortez  ](../../generated_pages/incidents/I00201.md) | <i>Social media users have been sharing images online that attribute various quotes to Speaker of the House Nancy Pelosi, Democratic presidential candidate Joe Biden and Rep. Alexandria Ocasio-Cortez. Reuters could not find evidence that these any of these quotes are accurate (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>[...]<br><br>The image reads: ‚Äú‚ÄôYou need to vote for the Democrats, otherwise the illegal aliens will lose their rights.‚Äô ‚Äì Nancy Pelosi (2019), ‚ÄòNo ordinary American cares about Constitutional rights.‚Äô ‚Äì Joe Biden, And my personal favorite: ‚ÄòOwning guns is not a right. If it were a right, it would be in the Constitution.‚Äô ‚Äì Alexandria Ocasio Cortez (2018)‚Äù<br><br>The image doesn‚Äôt say where or when any of the alleged comments were made.<br><br>[...]<br><br>Reuters could not find any records of Speaker Pelosi saying ‚Äú‚ÄôYou need to vote for the Democrats, otherwise the illegal aliens will lose their rights. (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)‚Äù<br><br>A Google search brings up pages using this quote with no attribution to a source.<br><br>In a press conference in June 2018, Pelosi told a reporter that using terminology like ‚Äúillegal alien‚Äù is not constructive. The video is available on C-SPAN. Her exact words can be heard around the 15:30 mark. It is therefore unlikely that Pelosi would have used the terminology in the alleged quote. <br><br>[...]<br><br>Reuters found no evidence former Vice President Biden ever said the words ‚ÄúNo ordinary American cares about Constitutional rights. (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)‚Äù <br><br>Some forums and posts featuring the article link to a Breitbart article that reads, ‚ÄúBiden: No Ordinary American Cares About Their Constitutional Rights, Facebook Questions are Plants‚Äù. The article mentions a February 21, 2013 forum at Western Connecticut State University in Danbury where then Vice President Biden spoke about gun control.<br><br>A video on the New York Times website shows Biden‚Äôs speech at the university. The closest to the quote in the claim can be seen round the 11:10 mark: ‚ÄúNo law-abiding citizen in the United States of America has any fear that their constitutional rights will be infringed in any way. None, zero.‚Äù<br><br>The actual quote is significantly different to the quote in this claim. Biden did not say ‚ÄúNo ordinary American cares about their constitutional rights‚Äù. (T0162: Reframe Context, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>[...]<br><br>‚ÄúOwning guns is not a right. If it were a right, it would be in the constitution.‚Äù<br><br>Reuters could not find any record of Ocasio-Cortez saying this. The social media posts do not provide any context or other information about the purported quote (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>Ocasio-Cortez clarified her stance on the second amendment on the right to keep and bear arms while answering questions on a Reddit thread when she was running as a candidate for Congress in 2017: <br><br>‚ÄúCombating domestic terrorism? Keep the severely mentally ill and people with a history of domestic abuse from purchasing guns. I am a believer in the second amendment, but this is where I draw the line.‚Äù<br><br>On her website, she calls for ‚Äúcommon sense gun reform with the goal of eliminating gun violence and saving lives‚Äù<br><br>It is unlikely that, as a Congresswoman, Ocasio-Cortez would not be aware of the existence and meaning of the second amendment.<br><br>Reuters previously debunked the Biden and Ocasio-Cortez quotes (T0160.007: Claim Previously Fact Checked), along with other false quotes attributed to prominent Democrats.</i> |
| [I00202 Fact check: False Mel Gibson quote on Hollywood   ](../../generated_pages/incidents/I00202.md) | <i>Social media users have been sharing content online that attributes a quote exposing Hollywood elites as pedophiles to actor Mel Gibson. This claim is false; the quote is fabricated (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0160.002: Information is False). <br><br>[...]<br><br>Some posts use this quote: ‚ÄúHollywood is an institutionalized pedophile ring. It is a den of parasites who feast on the blood of children. Every studio in Hollywood is bought and paid for with the blood of innocent children.‚Äù <br><br>The comment seems to stem from an article originally published on NewsPunch.com (T0097.202: News Outlet Persona), previously ‚ÄúYour News Wire‚Äù. It has since been deleted, but archived versions can be found  archive.vn/FyeKl ; and  archive.is/SuTTt ; . <br><br>The article claims that Gibson explained to guests in a green room after appearing on the Graham Norton Show about the ‚Äúreal nature of Hollywood elites‚Äù.  <br><br>A well-known actor speaking to a crowd of people about this topic would have gathered a lot of attention and been reported by major news organizations. However, a Google search of the quote only brings up social media posts, blogs and meme pages. <br><br>The original article including the quote was written by Baxter Dmitry, a possible pseudonym (T0143.002: Fabricated Persona) for an author who has been criticized for writing fake news articles. The website YourNewsWire has also‚ÄØbeen criticized for producing inaccurate content. Dmitry‚ÄØ is still a frequent contributor to NewsPunch. <br><br>The article has been copied or slightly altered (T0165: Edited Content) and published to other websites and blogs, which has led to the wide circulation of the quote. <br><br>[...]<br><br>A spokesperson for Mel Gibson told Reuters via email that the claims are ‚Äú100% fake‚Äù. <br><br>VERDICT<br><br>False. This quote attributed to Mel Gibson originated from a fabricated article.</i> |
| [I00203 Fact check: Viral post backing Trump came from imposter Hallie Biden account](../../generated_pages/incidents/I00203.md) | <i>Nearly two years after the 2020 presidential election, former President Donald Trump and his supporters continue to claim the election was rigged. On Aug. 29, Trump demanded to either be instated as the rightful president of the U.S. or have the country perform "a new election, immediately!"<br><br>A recent social media post claims President Joe Biden's daughter-in-law is backing the debunked claim that Trump won in 2020 (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>An Aug. 30 Facebook post claims Hallie Biden tweeted support for Trump and his performance in the 2020 presidential election.<br><br>"President Trump won that election and my entire family knows it," the supposed tweet reads.<br><br>The post garnered more than 90 likes in two weeks. A previous version garnered more than 100 likes in its first four days before it was deleted. Screenshots of other tweets purportedly shared by Hallie Biden have been published on Instagram, Facebook and Twitter.<br><br>USA TODAY and other fact-checkers have repeatedly debunked false and misleading claims that Trump won the 2020 election, as an array of recounts and audits by election officials from both parties have confirmed the legitimacy of Biden's win. <br><br>The purported Hallie Biden tweet is also false ‚Äì in both its content and attribution.<br><br>A spokesperson for Hallie Biden said the Twitter account was run by an imposter (T0143.003: Impersonated Persona), not Hallie Biden. It has since been suspended.<br><br>[...]<br><br>The tweet in question did not come from Hallie Biden, as she doesn't use the platform.<br><br>"Mrs. Hallie Biden does not have a Twitter account," a spokesperson for the Beau Biden Foundation, a charity organization for which Hallie Biden is a board chair, told USA TODAY in an emailed statement. "Any account bearing her name is fraudulent." <br><br>The account was suspended sometime between Aug. 31 and Sept. 2, according to online archives. </i> |
| [I00204 Fact check: Statement on migrants at the southern border falsely attributed to Ted Cruz](../../generated_pages/incidents/I00204.md) | <i>A viral screenshot claims to show a purported quote from Sen. Ted Cruz, R-Texas, about the condition of migrants arriving at the southern U.S. border. <br><br>The April 2 Facebook post says Cruz questioned why "illegals arrive at the Border clean and refreshed after traveling 1,500 miles on foot." <br><br>"Why don't they ever carry food or blankets? How do they charge their cell phones?" the post continues. "Almost makes one think the Alien Invasion is orchestrated." <br><br>Above the text is an image of Cruz. The post has over 25,000 shares and over 900 reactions. <br><br>Similar versions of the claim with hundreds of shares have been shared by users across Facebook. USA TODAY reached out to the users for comment. <br><br>Steve Guest, a spokesperson for Cruz, confirmed to USA TODAY via email that Cruz never made that statement and that it is a "categorically false quote that is being wrongly attributed to him on social media." (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>The quote does not appear on Cruz's personal Twitter account, @tedcruz, or his official Twitter page, @SenTedCruz. <br><br>There is also no record of the statement on ProPublica's Politwoops database, which tracks deleted tweets by elected officials. <br><br>On Facebook, users shared an image with the same text coming from the Twitter account @jonmichaelolse1, revealing where the quote first originated. <br><br>The Twitter user shared the tweet on March 27 and someone likely took a screenshot of the quote, cropping out the Twitter username (T0165.004: Source Edited Out of Content, T0165.002: Cropped Content) and adding the false attribution to Cruz (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>The image of Cruz included in the posts was taken by pool photographer Jonathan Newton on June 24, 2020, during an oversight hearing held by the Senate Committee for Commerce, Science and Transportation. </i> |
| [I00205 Fact check: Image claiming to show 2016 Ted Cruz tweet on climate change and Texas is fabricated](../../generated_pages/incidents/I00205.md) | <i>After Texas Sen. Ted Cruz's trip to Cancun, Mexico, as his home state dealt with an unprecedented winter storm, an image went viral purporting to show a tweet on climate change made by the Republican in 2016. <br><br>The post, shared on Facebook by many users across the platform, claims Cruz, R-Texas, tweeted, "I'll believe in climate change when Texas freezes over." (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>The post claims the tweet was sent at 5:44 p.m. on Sept. 8, 2016, and the profile picture and blue checkmark seen in the image matches the one displayed on Cruz's official Twitter profile. <br><br>"There's always a tweet," one Facebook user wrote in a post that has more than 400 shares. "This 2016 Ted Cruz Tweet did not age well, and it has also frozen solid," another user wrote Feb. 19 in a post with more than 800 shares.<br><br>USA TODAY was unable to reach the Facebook users who shared the image for comment. <br><br>A search for the purported climate change tweet on Cruz's personal account, @tedcruz, and his official Twitter page, @SenTedCruz, results in no matches.<br><br>There is no record of the tweet on ProPublica's Politwoops database that tracks deleted tweets by elected officials. <br><br>The tweet does not appear in archived versions of Cruz's Twitter page from September 2016 using the Internet Wayback Machine. <br><br>Steve Guest, a communication adviser for Cruz, confirmed to Factcheck.org in an email that the tweet is fabricated. USA TODAY was unable to reach Cruz's office for comment. </i> |
| [I00206 Fact check: Barack Obama tweet about Donald Trump's Twitter ban is fake](../../generated_pages/incidents/I00206.md) | <i>President Donald Trump's permanent suspension from Twitter sent shock waves through the internet, bringing forth a mix of reactions from lawmakers, celebrities and beyond (T0068: Respond to Breaking News Event or Active Crisis).<br><br>While former President Barack Obama has not publicly commented on Trump's ban from the social media platform, one Facebook post claims to show his reaction.<br><br>"Man do I love having this Twitter account, the ability to connect to so many millions of people, to express my feelings and views, to know that all major media outlets and world leaders are reading these words, that they are recorded for posterity. How awesome is that?" reads a purported screenshot of a tweet from Obama.<br><br>The screenshot, which has over 1,000 shares, was shared by the Facebook page Boycott All Things Trump with the caption, "#ForeverPresident Barack Obama trolls the orange user not to be found."<br><br>The post claims the tweet was shared on Jan. 9 at 6:52 a.m., with 2,000 likes and 250 retweets. The tweet appears to have a blue checkmark and the profile image matches the one seen on Obama's official Twitter profile. However, there is no evidence that the tweet is authentic (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>The Facebook page does not have a way to be contacted.<br><br>USA TODAY found no reports of the purported tweet; it does not appear on Obama's Twitter timeline and the 44th president has not released any statement regarding Trump‚Äôs ban from the platform.<br><br>As of Jan. 11, Obama's latest tweets appear to be on Jan. 8 on the response of Capitol police to the violence that took place in Washington, D.C. <br><br>An archived page from the Internet Archive Wayback Machine of Obama's official Twitter page on Jan. 9, the day the post claims the tweet was posted, also shows no such post.</i> |
| [I00207 Fact check: Claim comparing cost of border wall to HealthCare.gov misattributed to actor Tim Allen](../../generated_pages/incidents/I00207.md) | <i>Actor Tim Allen is once again the subject of a misattributed quote that makes a false comparison between the cost of the Affordable Care Act website and the cost of former President Donald Trump's border wall (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>A meme that contains the misattributed quote and an image of Allen stems from another false post that USA TODAY debunked in May (T0160.007: Claim Previously Fact Checked). <br><br>‚ÄúPresident Trump‚Äôs Wall Costs Less Than The Obamacare Website. Let that sink in, America,‚Äù reads the meme, which a Trump fan page posted to Facebook on March 20.<br><br>Eric Trump, the former president's son, also shared the meme, which has circulated on social media for several years, to Instagram in September 2019. <br><br>Neither the attribution nor the claim is true. <br><br>The Facebook page that posted the meme did not respond to USA TODAY‚Äôs request for comment.<br><br>In May, USA TODAY debunked a similar claim that criticized Democrats and was falsely attributed to Allen. The recently shared meme features a partial quote from that longer false claim (T0160.007: Claim Previously Fact Checked, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>USA TODAY found that the post was authored by another man named Tim Allen in August 2019. Social media users confused the author, who has the same name as the actor, with Tim Allen from ‚ÄúLast Man Standing‚Äù and ‚ÄúHome Improvement.‚Äù<br><br>"This statement was not written by my client, the actor Tim Allen," Allen‚Äôs publicist, Marleah Leslie, told USA TODAY. <br><br>Leslie confirmed in an email to USA TODAY that the actor did not author the shorter quote, either.<br><br>Aside from the misattribution, the claim itself is false. While exact costs for Healthcare.gov, the Obamacare website, and Trump‚Äôs border wall, are unknown, estimates put the cost of the wall much higher than the Obamacare website. <br><br>An August 2014 report by the HHS Office of Inspector General estimated contracts for HealthCare.gov totaled $1.7 billion. Bloomberg put the cost slightly higher at $2.1 billion in a September 2014 article. <br><br>These estimates are dwarfed by the proposed costs of the border wall.<br><br>According to a Department of Homeland Security report obtained by Reuters in February 2017, the border wall could cost up to $21.6 billion if completed. This number is much higher than the $12 billion dollar estimate Trump pushed on the campaign trail.<br><br>In January 2020, U.S. Customs and Border Protection reported that the ‚Äúborder wall system‚Äù had cost $11 billion so far, according to NPR. <br><br>By the end of 2020, the Trump administration had spent an estimated $16 billion on the wall, none of it paid for by Mexico, as Trump had promised, according to The Arizona Republic.</i> |
| [I00208 Fact check: Actor and comedian Tim Allen did not write viral conservative statement](../../generated_pages/incidents/I00208.md) | <i>A statement arguing a list of conservative views attributed to ‚ÄúTim Allen‚Äù has recently resurfaced on Facebook.<br><br>‚ÄúTim Allen is credited with writing this ... Here are some interesting points to think about prior to 2020, especially to my friends on the fence,‚Äù the post begins. ‚ÄúWe are one election away from open borders, socialism, gun confiscation, and full-term abortion nationally. We are fighting evil,‚Äù it also says.<br><br>The statement, which argues conservative stances on immigration, voter ID laws and criticizes Democratic leaders, accompanies the actor's photo and encourages others to copy, paste and share. The post also makes several false claims about the ‚ÄúObamacare‚Äù website, Chelsea Clinton‚Äôs NBC salary and Alexandria Ocasio-Cortez‚Äôs ‚ÄúGreen New Deal.‚Äù<br><br>A jewelry technician from Virginia also named Tim Allen posted an identical statement  Aug. 25, 2019. Allen from Virginia did not respond to USA TODAY's request for comment.<br><br>"This statement was not written by my client, the actor Tim Allen," Marleah Leslie, publicist for the "Last Man Standing" and "Home Improvement" actor, told USA TODAY Wednesday (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>The viral statement also expresses adamant disapproval for 2016 presidential candidate Hilary Clinton that the actor has not shared in the past. <br><br>‚ÄúI wake up every day and I am grateful that Hillary Clinton is not the president of the United States of America,‚Äù the post reads.  <br><br>‚ÄúI‚Äôve met the Clintons and I think she‚Äôs a capable and wonderful choice,‚Äù Allen told Fox Business at President Donald Trump‚Äôs inauguration in 2017. ‚ÄúI was more concerned about the people she brought with her.‚Äù<br><br>Allen is known for his personal and on-screen conservative politics. He's famously called his ‚ÄúLast Man Standing‚Äù character ‚ÄúArchie Bunker but college educated.‚Äù<br><br>In the sitcom, Allen plays a conservative husband and father to three daughters. Many interviewers have asked Allen whether he shared his politically incorrect character‚Äôs views.<br><br>‚ÄúMy politics are really irrelevant,‚Äù Allen told Entertainment Weekly in 2018.  <br><br>Allen has commented on Hollywood‚Äôs dominant liberal and politically correct nature.<br><br>‚ÄúIf you don‚Äôt believe what everybody believes, this is like '30s Germany,‚Äù Allen joked on "Jimmy Kimmel Live" in March 2017.  ‚ÄúI‚Äôm not ignorant, but sometimes I play it.‚Äù<br><br>Allen endorsed and voiced a campaign ad for Republican presidential candidate John Kasich in 2016.<br><br>Allen was one of the few Hollywood stars to attend Trump‚Äôs inauguration.<br><br>‚ÄúAt one point, Trump said things that made sense to me,‚Äù Allen told Fox Business. ‚ÄúAt some points, the Democrats didn‚Äôt, and that‚Äôs it.‚Äù<br><br>Allen has described his fiscally conservative values in several interviews, but has not publicly endorsed Trump.<br><br>‚ÄúI‚Äôm just watching the theater of it and trying to keep my personal opinions out of it,‚Äù Allen told Entertainment Weekly when asked whether he supported Trump. ‚ÄúMy political party is that I‚Äôve never liked taxes, period, so whatever that means.‚Äù<br><br>We rate the claim that actor and comedian Tim Allen wrote a viral conservative statement FALSE (T0160.002: Information is False) because it is not supported by our research. The statement was posted by another man named Tim Allen and misattributed to the well-known actor.<br><br>The same misattributed statement was posted and debunked by FactCheck.org, PolitiFact, Truth or Fiction and Snopes in September (T0160.007: Claim Previously Fact Checked).</i> |
| [I00209 Fact check: False excerpt from Trump‚Äôs ‚ÄòArt of the Deal‚Äô on ‚Äònever admitting defeat‚Äô](../../generated_pages/incidents/I00209.md) | <i>Users on social media are sharing a purported excerpt from ‚ÄúTrump: Art of the Deal‚Äù, a book authored by U.S. President Donald Trump and Tony Schwartz that was published in 1987. The alleged excerpt shows Trump describing his approach on ‚Äúnever admitting defeat‚Äù. This is false (T0160.002: Information is False). This quotation is not part of the ‚ÄúArt of the Deal‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>[...]<br><br>The claim has attracted interest because Trump has not conceded the Nov. 3 election (T0068: Respond to Breaking News Event or Active Crisis) and repeatedly made unfounded allegations of massive voter fraud.   <br><br>The alleged extract starts with: ‚ÄúArt of the Deal. Chapter 45. Never admit defeat. You win. If you don‚Äôt win claim they cheated. They rigged it. They stole it.‚Äù It adds: ‚ÄúYou are the aggrieved, but you are the true winner always. Threaten to sue and/or die. No one will remember the outcome. Just that you fought to defend your honor. This is essential to keep your brand healthy and keep the millions of chumps and losers, your base.‚Äù  <br><br>While the claim says the excerpt is from ‚Äúchapter 45,‚Äù the book has only 14 chapters. <br><br>A search of key terms brought up no results of the alleged excerpt in the book. There‚Äôs no mention of ‚Äúnever admit defeat‚Äù , ‚Äúcheated‚Äù nor ‚Äúchumps and losers‚Äù. <br><br>The fabricated quote includes words like ‚Äúrigged‚Äù and ‚Äústole,‚Äù which Trump has used repeatedly to refer to the Nov. 3rd election and to baselessly claim widespread voter fraud took place.</i> |
| [I00210 False: An Italian news channel used visuals from the film Deep Impact to depict an exodus of people from Kyiv.](../../generated_pages/incidents/I00210.md) | <i>Since Russia invaded Ukraine, misinformation relating to the conflict has spread. A recent tweet claimed that media outlets were using clips from the movie Deep Impact to report the war. The tweet compared two similar images which show people running on a highway packed with cars. One image had text which said Deep Impact, and the other with the logo of TGcom24, an Italian news channel, with the text "Escape from Kiev."<br><br>After analyzing TGcom24's reports of Ukraine and Russia, it is clear that the news channel did not use footage from the movie. After comparing the image from the tweet with video footage from TGcom24, Logically found that the image is altered. The logo in the tweet is hazy and pixelated. The logo's format and placement were also different in the tweet. Official footage from TGcom24 shows their logo usually on the bottom right of the screen and not on the top right (T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>We could not find where this edited image first appeared through a reverse image search. Most results showed stock images from the movie. The Twitter post in question was made on June 19, 2022. TGcom24 has never published any reports using the image in question.<br><br>The image in the tweet has been digitally altered to appear as though TGcom24 used a still from the film Deep Impact. There are no reports from the Italian news outlet that includes this image. Hence we have marked this claim as false (T0160.002: Information is False).</i> |
| [I00211 FALSE: Greenhills hostage taker ‚Äòfound dead in jail cell‚Äô](../../generated_pages/incidents/I00211.md) | <i>Claim: Alchie Paray, the sacked security guard who recently (T0068: Respond to Breaking News Event or Active Crisis) held dozens of people hostage at Greenhills Shopping Center, was found dead inside his jail cell.<br><br>The claim was posted on the website randomnames.club using the headline: ‚ÄúABS CBN NEWS: Hostage Taker na si Archie (sic) Paray Natagpuang Patay sa Loob ng Kanyang Selda.‚Äù<br><br>The article only contained a video with a thumbnail of a blurred black-and-white photo of a man. The video also used the ABS-CBN logo. (T0161.001: Impersonated Content, T0097.202: News Outlet Persona)<br><br>Separately, another video posted by Youtube channel Latest Trending claimed that Paray committed suicide inside his jail cell. The video‚Äôs thumbnail appeared to be a GMA News report with a photo of Paray (T0161.001: Impersonated Content, T0097.202: News Outlet Persona). <br><br>The article and the video were both flagged by Facebook Claim Check, a monitoring tool that detects viral posts with potentially false information. According to social media tool Crowdtangle, the article was posted 9 times on Facebook by pages and groups with a collective total of over 1.1 million followers.<br><br>Rating: FALSE<br><br>The facts: ABS-CBN News and GMA News did not report on Paray‚Äôs death, nor did any other legitimate news sites.<br><br>The website that posted the false claim used a video report of the death of Genesis Argoncillo which occurred in 2018 (T0162.003: Historic Content Incorrectly Presented as Current). Argoncillo died in police custody after being beaten to death.<br><br>Upon checking the Youtube link of the video, it redirected (T0149.004: Redirecting Domain Asset) to ABS-CBN‚Äôs TV Patrol report on the death of Argoncillo in 2018.</i> |
| [I00212 FACT CHECK: Kuya Kim is still alive ](../../generated_pages/incidents/I00212.md) | <i>Claim: Television host Kim Atienza, popularly known as Kuya Kim, died on Monday, June 3, 2024. <br><br>Rating: False (T0160.002: Information is False)<br><br>Why we fact-checked this: The TikTok video bearing the claim has gained 1,615 likes, 697 favorites, and over 208,300 views, as of writing. <br><br>‚ÄúHanggang sa muli Kuya Kim #viral #fyp #sad #death,‚Äù the caption reads. <br><br>The post includes a black-and-white photo of Atienza, the GMA Integrated News logo, and a message expressing gratitude to Atienza. The visual design of the post resembles the social media cards used by GMA News to announce someone‚Äôs passing (T0161.001: Impersonated Content).<br><br>The Facts: Atienza refuted these claims, stating that the posts about his death were a hoax.<br><br>‚ÄúAabot din tayo diyan, but not today,‚Äù Atienza said in a Facebook post on Monday, June 3.  (We will get there, but not today.)<br><br>The next day, June 4, Atienza even appeared in the GMA news program 24 Oras to host his segment ‚ÄúKuya Kim, Ano Na?‚Äù<br><br>Fake account: The false claim was made by the TikTok account ‚Äúgmanewsbalita_156.‚Äù This is not an official account of GMA News. The news outlet‚Äôs official account on TikTok has three million followers and 103.8 million likes.<br><br>Debunked: In 2020, Rappler also fact-checked some posts claiming that Atienza was in critical condition after supposedly being shot by robbers in his home.</i> |
| [I00213 No, USAID didn't pay Hollywood actors millions to visit Ukraine | Fact check](../../generated_pages/incidents/I00213.md) | <i>A Feb. 5 Facebook post shows what appears to be a news report from E! News claiming the U.S. Agency for International Development paid Hollywood actors to visit Ukraine.<br><br>"USAID sponsored American celebrity visits to Ukraine after Russia's full-scale invasion began," reads text on the video, which is accompanied by a voiceover narration. "Angelina Jolie, $20,000,000. Sean Penn, $5,000,000. Jean-Claude Van Damm (sic), $1,500,000. Orlando Bloom, $8,000,000. Ben Stiller, $4,000,000."<br><br>The video uses the E! News logo throughout.<br><br>The Facebook post was shared more than 300 times in five days. The video was also spread by Donald Trump Jr. on X.<br><br>The video is a fabrication that wasn't reported or published by E! News, a company spokesperson said (T0161.001: Impersonated Content). The video is consistent with material created by a Russia-aligned influence campaign, disinformation experts said.<br><br>President John F. Kennedy created USAID by executive order after signing the Foreign Assistance Act in 1961. The global aid agency has been targeted for apparent dismantling by billionaire Elon Musk (T0068: Respond to Breaking News Event or Active Crisis), the head of President Donald Trump's Department of Government Efficiency. Trump told reporters he loved "the concept" of USAID but claimed the agency "turned out to be radical left lunatics."<br><br>The video in the Facebook post plays into the scrutiny of the agency by linking USAID to Hollywood celebrities and U.S. support for Ukraine. But the video is not a real news report from E! News. It's a fabrication that deceptively uses the branding of the entertainment news outlet to spread a false narrative (T0160.002: Information is False).<br><br>An E! News spokesperson who declined to provide a name for attribution said the video is not authentic and not from E! News.<br><br>Multiple actors mentioned in the video also said the claims USAID paid them to visit Ukraine were false. Bloom said in an Instagram post that such reports were "untrue." Stiller wrote in an X post that his trip to Ukraine was "completely self-funded." Mathew Rosengart, Penn's longtime lawyer, similarly said in an email that Penn's travel to Ukraine was "self-funded" and not sponsored by USAID. A spokesperson for Angelina Jolie, who declined to be identified, also told USA TODAY the payment claim was wrong and Jolie has personally covered the cost for all her humanitarian missions.<br><br>Patrick Warren, co-director of the Media Forensics Hub at Clemson University and researcher of online disinformation, said he's "confident" the video traces back to Russia because it has indicators consistent with the ongoing Russia-aligned Matryoshka influence campaign, including the mimicking of an authentic news outlet, the use of an AI voiceover (T0166: AI-Generated Content) and the targeting of Ukraine.<br><br>This type of Russian malign influence commonly takes advantage of "current news ‚Äì like U.S. AID ‚Äì to insert anti-Ukraine messaging into the discourse," Warren said in an email. He added the video in the Facebook post was first shared on X by an account that "regularly initiates" narratives propagated by the Kremlin-aligned Storm-1516 influence network. The narrative in the fabricated video appeared on Russian state media around the same time the X post was shared and appeared on pro-Russian Telegram channels, Warren said.<br><br>Darren Linvill, another co-director of Clemson's Media Forensics Hub, noted in an X thread that Musk and Trump Jr. amplified the video and that it had "every indication of being a Russian-fabricated video" that never appeared on E! News. Attempts to reach Musk and Trump Jr. for comment were not immediately successful.<br><br>USA TODAY reached out to the Facebook user who shared the post for comment but did not immediately receive a response. Attempts to reach Van Damme for comment were also unsuccessful.<br><br>¬ß</i> |
| [I00214 False: An Italian news channel used visuals from the film Deep Impact to depict an exodus of people from Kyiv.](../../generated_pages/incidents/I00214.md) | <i>Since Russia invaded Ukraine, misinformation relating to the conflict has spread. A recent tweet claimed that media outlets were using clips from the movie Deep Impact to report the war. The tweet compared two similar images which show people running on a highway packed with cars. One image had text which said Deep Impact, and the other with the logo of TGcom24, an Italian news channel, with the text "Escape from Kiev."<br><br>After analyzing TGcom24's reports of Ukraine and Russia, it is clear that the news channel did not use footage from the movie. After comparing the image from the tweet with video footage from TGcom24, Logically found that the image is altered (T0165: Edited Content). The logo in the tweet is hazy and pixelated. The logo's format and placement were also different in the tweet. Official footage from TGcom24 shows their logo usually on the bottom right of the screen and not on the top right (T0161.001: Impersonated Content).<br><br>We could not find where this edited image first appeared through a reverse image search. Most results showed stock images from the movie. The Twitter post in question was made on June 19, 2022. TGcom24 has never published any reports using the image in question.<br><br>The image in the tweet has been digitally altered to appear as though TGcom24 used a still from the film Deep Impact. There are no reports from the Italian news outlet that includes this image. Hence we have marked this claim as false (T0160.002: Information is False).</i> |
| [I00215 MATRYOSHKA: A pro-Russian campaign targeting media and the fact-checking community](../../generated_pages/incidents/I00215.md) | <i>Since late 2023, VIGINUM has observed and documented a malicious campaign that could affect French-speaking public debate online. <br><br>[...]<br><br>Active on X since at least September 2023, the Matryoshka campaign is an operation conducted in two stages: <br><br>- A first group of accounts, known as ‚Äúseeders‚Äù, posts fake content on the platform (see section 2.2); <br><br>- A second group of accounts, known as ‚Äúquoters‚Äù, then shares a seeder‚Äôs post in response to posts by media outlets, public figures and fact-checkers (T0039.001: Collaborating Assets Seed and Ping). <br><br>The quoters contact targeted individuals or organizations to ask them to check the authenticity or veracity of content posted by the seeders (T0039.002: Solicit Production of Fact Check). Since September 2023, Matryoshka operators have conducted at least 90 successive operations, during which they adapted and tested different methods to disseminate content and bring it to the attention of their targets.<br><br>2.1.1 Content dissemination pattern<br><br>Matryoshka operations generally involve two or three seeders that initially post the content on X within a few minutes of each other. Some 30 to 40 minutes later, two or three quoters start to share the seeders‚Äô posts, commenting on the target‚Äôs latest post. Quoters frequently add text, an emoji, or simply mention the target, this addition being unique for each operation and quoter. This second phase usually lasts several hours, with an average of 45 seconds between each quote (T0039.001: Collaborating Assets Seed and Ping).<br><br>[...]<br><br>Furthermore, the X accounts involved in the Matryoshka campaign are used to conduct several successive operations, a quoter‚Äôs account almost systematically becoming a seeder‚Äôs account and vice versa. For example, @wosuhitsu1972 was involved in at least five Matryoshka operations from 14 to 28 March 2024, in turn as a quoter, a seeder, a seeder, a quoter then a seeder (T0039.001: Collaborating Assets Seed and Ping). The account has since been suspended.<br><br>Moreover, from September 2023 to February 2024, most seeders‚Äô and quoters‚Äô accounts seem to have been bought from a specialized company (T0146: Account Asset, T0150.006: Purchased Asset). Some weeks before they were involved in the Matryoshka campaign, some accounts suggested in their display name that they were part of the WebMasterMarket company, which sells X accounts. They also share common features, including links with people from Asia, past creation dates, and recent posts promoting crypto assets such as Memecoin. <br><br>Created shortly before the operations (T0150.001: Newly Created Asset), all the accounts used have digitally generated profile pictures, do not mention a biography or location, and do not have any subscribers or subscriptions. As it stands, VIGINUM considers that the accounts used by Matryoshka could have been bought in different pools by the campaign, and even possibly compromised.<br><br>[...]<br><br>2.2.1 Misleading and untruthful content aimed at discrediting Ukraine and its allies <br><br>This campaign first seems to have appeared on September 5, 2023,27 with a first post sharing a report impersonating Fox News (T0161.001: Impersonated Content, T0097.202: News Outlet Persona) and asking various media outlets to check the information (T0039.002: Solicit Production of Fact Check). <br><br>The Matryoshka campaign then primarily involved dissemination of false graffiti and posts spoofing the visual identity of Western media outlets, institutions and NGOs using the above procedure (T0161.001: Impersonated Content). To date, this content has been published in French, English, Italian, German, Russian and Ukrainian (T0101: Create Localised Content). <br><br>Matryoshka spreads three types of content impersonating media outlets, institutions and NGOs: <br><br>- Video reports with untruthful content using the visual identity and fonts of the organization (T0161.001: Impersonated Content). These false reports appear to be produced using royalty-free stock images and music;<br><br>- False screenshots presenting an excerpt of an article, an Instagram story or a short YouTube video from a media outlet (T0161.001: Impersonated Content), organization or individual (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution); <br><br>- Fake official documents impersonating a government body (T0085.004: Develop Document, T0161.001: Impersonated Content). <br><br>Other posts involve disseminating fake images of graffiti in the streets of major Western cities (T0161.003: Falsified Graffiti or Signage). These graffiti images are produced using a montage of a photograph of a real place and a caricature targeting Ukrainian or European figures. These manipulated images may also spoof the visual identity of street artists, like the French artist Lekto (T0161.001: Impersonated Content, T0097.100: Individual Persona). <br><br>The narratives pushed by these publications primarily target Ukraine and aim to discredit its government and President, or else to criticize the arrival of Ukrainian refugees in Western countries. For example, many instances of fake graffiti, often anti-Semitic, depict Volodymyr ZELENSKYY as a beggar or war criminal, and several false reports have presented Ukrainian refugees in Europe in an unfavourable light.<br><br>2.2.2 France, a key target of the Matryoshka campaign<br><br>Other narratives aim to discredit European governments, and particularly their policy supporting Ukraine. As such, France constitutes a key target for the campaign‚Äôs operators. <br><br>The media outlets BFMTV, Le Parisien, Lib√©ration, Le Monde and La Montagne (T0161.001: Impersonated Content, T0097.202: News Outlet Persona), as well as the Banque de France, the city of Paris and the Directorate-General for Internal Security (DGSI) (T0161.001: Impersonated Content, T0097.206: Government Institution Persona) have been impersonated by the Matryoshka campaign. Moreover, several cases of fake graffiti (T0161.003: Falsified Graffiti or Signage) purported to be situated in and around Paris. This false content targeting France spread several narratives aimed notably at sowing distrust of French institutions and government members.</i> |
| [I00216 Rats, bedbugs, tuberculosis, and financial problems. Russian propaganda disinformation about the Paris Olympics](../../generated_pages/incidents/I00216.md) | <i>[Translated from original in Russian] Russian propaganda began actively attacking France and its President Emmanuel Macron in connection with a different news story. On February 27, Macron stated that he wouldn't rule out sending French troops to Ukraine, and subsequently reiterated his stand. Soon after, in Telegram channels that regularly publish disinformation, the French president temporarily eclipsed his Ukrainian counterpart, Volodymyr Zelenskyy: it was Macron, for example, who was the subject of the majority of fake magazine covers in the first months of 2024 (T0068: Respond to Breaking News Event or Active Crisis).<br><br>By the end of March, the fakers' attention had shifted to a project of great importance to Macron and his government: the Paris Olympics.<br><br>Bedbugs and rats<br><br>In the second half of 2023, the internet was flooded with reports of bedbugs in France. They were found everywhere: in apartments, hospitals, schools, and public transportation. But only some of the reports were true. The problem does exist: for example, BBC Paris correspondent Hugh Schofield cited experts in one of his reports who said it occurs at the end of every summer. But the reports about the bedbug infestation, which allegedly led to the possible cancellation of the Games and the widespread closure of hospitals, turned out to be false (T0160.002: Information is False). They were created by pro-Kremlin Telegram channels.<br><br>A striking example of this mixture of truth and lies is the news about a bedbug-infested hospital in the northern French city of Arras. On April 9, 2024, the newspaper La Voix du Nord reported that the patient found with bedbugs had been isolated, while the other patients were transferred to nearby healthcare facilities. Major French media outlets, including Le Figaro , later reported on this story . On April 11, the Telegram channel " Shkvarka 2.0 " expanded on this story, claiming that 11 hospitals and 24 emergency rooms had allegedly closed in France in recent weeks for similar reasons. The post was accompanied by a video featuring the Le Figaro logo, emphasizing that this all happened approximately 100 days before the Olympic opening ceremony (T0161.001: Impersonated Content, T0068: Respond to Breaking News Event or Active Crisis, T0097.202: News Outlet Persona).<br><br>The French outlet did release a video about the events in Arras on April 9, but it made no mention of hospitals being closed nationwide. The fake copy included fragments containing false information, and one frame immediately featured captions in English.<br><br>To illustrate the fake, the creators used a stock photo from the internet. The Alamy photo agency website states that it is of an emergency room in London.<br><br>The day after the video with the fake fragments was published, a report appeared on the PoliTube_news Telegram channel claiming that French intelligence services had allegedly initiated an investigation into the journalists who reported on the closure of the Arras hospital. The post was also accompanied by a video, this time featuring the BFMTV logo.<br><br>The video claimed that the French Ministry of the Interior's General Directorate of Internal Security (DGSI) was investigating reporters from BFMTV, La Voix du Nord, and France 3. Le Point contacted all the journalists mentioned in the article, who said they were hearing about it for the first time and laughed at the news. Editorial management took the fake news more seriously: BFMTV CEO Marc-Olivier Faugiel promised to sue for illegal use of the company's logo and announced that La Voix du Nord would join the lawsuit (T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0160.002: Information is False).<br><br>The video's fake nature is evidenced not only by the journalists' statements but also by technical details. BFMTV's short videos on social media are designed completely differently, from the font to the color of the captions. The creators of the fake video chose a font similar to the one the publication uses in its video previews, but the text in the BFMTV videos themselves is formatted differently.<br><br>The creators of Olympic fakes haven't forgotten another favorite tactic: fake graffiti (T0161.003: Falsified Graffiti or Signage). Over the past two years, "Verified" has debunked nearly two dozen fake street art posts, all created using the same principle: digitally overlaying a drawing onto a real photo.<br><br>For example, on March 29, the Telegram channel " Shkvarka 2.0 " published a photo of a mural depicting the "symbol of the Paris Olympics"‚Äîa bug twirling gymnastics hoops in the colors of the Olympic rings. A week and a half later, on April 9, the channel " V Ruka Kremlya Z " showed a new mural on the wall of another building, a photo allegedly sent by a subscriber. "What are the Paris Olympics based on? Three sewer rats and a giant bug. This is the attitude of the French towards the upcoming Olympics depicted on a huge mural in Paris," the post read.<br><br>Unfortunately, it was impossible to find the exact locations of the murals based on the images (the photographers took pains to avoid including any identifying marks), but there's no doubt that street art like this has never existed on the streets of the French capital (T0161.003: Falsified Graffiti or Signage). Firstly, the images were posted by channels that have been repeatedly caught spreading fakes. Secondly, there are no photos of these murals from other angles online. And finally, no French media outlets covered them. However, these fakes didn't garner much interest from Russian-speaking Telegram users either: according to "Verified" estimates based on TGStat data, both stories received a combined total of approximately 150,000 views.<br><br>From late March to early April, pro-Kremlin channels generated numerous such stories. For example, a video about elevated levels of a pesticide used to kill rats and bedbugs in Parisian tap water was allegedly released by Le Figaro (T0161.001: Impersonated Content, T0097.202: News Outlet Persona), citing Professor Yves L√©vy (the professor is real, but the citation is not available in open sources (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)). Or the story about the bedbug-infested Olympic Village‚Äîthis time, the post was accompanied by a fragment of an unrelated Euronews broadcast, with a feed from the AFP agency (the agency that allegedly broke the news) shown on the right side of the screen (T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0084.002: Plagiarise Content).<br><br>In one of the stories, the authors of the fake attempted to combine several themes: bedbugs, rats, political scandal, and the general negative attitude toward the Olympics. It was first published on April 18 by the Telegram channel " Primorsky zov ." It claimed that former French Economy Minister Jean-Louis Borloo compared the Paris Olympics to "a gypsy camp that comes to the city, provides a momentary joy, and leaves behind financial losses, dirt, disease, and rats." A video featuring the TF1 logo claimed that the former minister's remark sparked a scandal: Norman Rudevich, head of the International Roma Union, allegedly condemned Borloo for discrimination.<br><br>This fake is a mixed bag. Firstly, it's unlikely that a French television channel would have presented Borloo as a former Minister of the Economy ‚Äì he held that post for exactly a month in 2007, but spent three years as Minister of Ecology, Energy, and Sustainable Development. Secondly, he never uttered such a phrase (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). In February 2023, Senator Herv√© Marcel, head of Borloo's Union of Democrats and Independents (UDI) party, did indeed refer to a "gypsy camp ," but this inappropriate comparison was made to the left-wing France Insoumise party's behavior in the National Assembly during the debate on pension reform, not to the upcoming Olympics. Marcel later apologized , but Borloo had nothing to do with this story. Finally, the video itself is fake. TF1 has never produced a video like this (T0161.001: Impersonated Content, T0097.202: News Outlet Persona), and the footage of Borloo is excerpts from his interview with France Inter radio on June 22, 2023 (T0084.002: Plagiarise Content), in which he didn't mention the Olympics, gypsies, rats, or bedbugs.<br><br>Pollution of the Seine<br><br>The Seine plays a key role in the symbolism of the 2024 Olympics. The opening ceremony, scheduled for July 26, will take place on the river. The delegations of the participating teams will be carried on 160 boats past all the main Parisian landmarks: Notre Dame, the Eiffel Tower, and the Louvre. However, the Seine has never been known for its cleanliness, and the French authorities are doing everything possible to improve this. Macron has repeatedly promised to swim in the Seine himself before the Olympics to prove the safety of the water in Paris's main river‚Äîand he's not alone. Mayor Anne Hidalgo and IOC President Thomas Bach have also made similar statements.<br><br>In March and April, fakers began to exploit the Seine's pollution theme (T0068: Respond to Breaking News Event or Active Crisis). On April 16, the Telegram channel " Z_O_V " reported that ironic graffiti had appeared on the Seine embankment: it depicted the three-eyed fish Blinky (who lives in ponds near the nuclear power plant in The Simpsons) jumping out of the river and claiming the water in the Seine is absolutely clean. This story turned out to be one of the most viral of all Olympic fakes studied by "Verified," garnering over 250,000 views on Telegram (largely due to a post on the major channel " Two Majors "). We determined the exact location from which the photograph that served as the basis for the fake was taken: it is the Left Bank of the Seine, and the embankment where the graffiti is supposedly located is opposite, on the √éle de la Cit√©, not far from Notre-Dame Cathedral. However, it is unlikely that Parisians saw this drawing, since not a single French media outlet reported on it, and local residents and numerous tourists did not post photos on social media (T0161.003: Falsified Graffiti or Signage).<br><br>Pro-Kremlin Telegram channels also exploited the Seine's polluted water theme using another of their traditional methods : a fake cover (T0161.001: Impersonated Content). The story about Macron's promise to swim in the river was depicted using a fake cover from the German satirical magazine Titanic. "I will prove that the water in the Seine is absolutely safe for the Olympic Games," Macron says in the cartoon, then dives in and emerges a mutant.<br><br>The real April issue of Titanic has a completely different cover (with Hitler smoking marijuana)‚Äîthe magazine typically covers German news, not events relevant to other countries. Furthermore, the authors never corrected the errors found on other fake covers of the publication: for example, the word "issue" (Ausgabe) is misspelled (Aufgabe).<br><br>Ukrainians and tuberculosis<br><br>Sanitary and epidemiological fakes about the Paris Olympics aren't limited to bedbugs, rats, and dirty water in the Seine. Another popular story concerns tuberculosis, which Ukrainians are bringing to France. One claim claims that 85% of Ukrainian Armed Forces personnel who arrived in France for rehabilitation were diagnosed with the disease, and that the doctors treating them also became infected. A post that first appeared on the Shkvarka News channel on March 27 cited screenshots from videos featuring the logos of France 24 and RFI as evidence (T0161.001: Impersonated Content, T0097.202: News Outlet Persona). Four days later, the same channel reported that French authorities would introduce mandatory tuberculosis testing for Ukrainian refugees, and on April 18, they updated the story, claiming that $2 billion had been allocated for testing, and that luggage would also be checked for parasites. In all three cases, the primary sources were the Telegram channels "Shkvarka News" and "Shkvarka 2.0," where posts appeared simultaneously. The last two publications about Ukrainian refugees included videos with the RFI logo as evidence (T0161.001: Impersonated Content, T0097.202: News Outlet Persona). The latter is the most detailed and deserves a closer look.<br><br>The video cites a document allegedly from the French Ministry of Labor and Health, which outlines stricter controls for Ukrainians arriving in the country. It concludes by stating that those who refuse to undergo testing for tuberculosis, hepatitis A, B, and C, HIV, and other diseases, including COVID-19, will be deported within 48 hours.<br><br>The ministry's website doesn't list such an order in the official documents section , and no documents signed by the minister were issued on April 17 (the date indicated on the document in the video) (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). Searching the website for the words "tuberculosis" and "Ukraine" reveals only a memo drafted in the spring of 2022, after Russia's full-scale invasion of Ukraine began. It states that the fight against tuberculosis in Ukraine is not as intensive as in France, so adults are advised to get tested. Testing is mandatory for children. However, no penalties are stipulated for refusal, and the document has not been updated since the summer of 2022.<br><br>However, as in previous cases, the problem exists: in its March report, the French National Health Agency noted the rise in registered tuberculosis cases in the country and cited statistics showing that the incidence rate among Ukrainian refugees is 197 per 100,000 people (for comparison, in France as a whole, it is no more than 10 per 100,000 people) (T0068: Respond to Breaking News Event or Active Crisis). Nevertheless, vaccination remains voluntary, the report emphasizes.<br><br>Finally, the videos themselves have no connection to RFI or France 24. There are no videos about Ukrainians and tuberculosis on either RFI's website or its YouTube channel. Furthermore, on March 28, the company issued a special statement condemning the use of its logo to create a fake news story about allegedly infected Ukrainian soldiers. And fact-checkers at France 24 mentioned this series of fakes in one of their analyses.<br><br>"The Games Will Be a Disaster": Sponsor Withdrawal and Spectator Problems<br><br>The leitmotif of Olympic fakes is the impending failure of the Games. The stories described above described problems that would allegedly seriously complicate the competition. In the reality created by the fakers, this was confirmed by the decisions made by the competition sponsors and tourists planning to attend.<br><br>On April 8, a post appeared on the Telegram channel "Mariupol Party Girl" claiming that three companies‚ÄîSpotify, Lego, and Hochland‚Äîhad disappeared from the sponsors section of the 2024 Olympics website. Attached to the post was a video featuring the TF1 logo , which supported the news with screenshots of the website and a commentary by economist Jacques Attali.<br><br>These companies are indeed not listed in the "Partners" section of the official Olympics website , which is not surprising: none of them have publicly announced any such contracts with the IOC or the Paris Organizing Committee (T0160.002: Information is False). The Ukrainian fact-checking project " Gvara Media " received comments from official representatives of Spotify, Lego, and Hochland, who assured that they have never been partners of the Paris Olympics. Jacques Attali also responded to the inquiry, stating that he neither said nor thought the words attributed to him in the video (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). Finally, the video is also not available on TF1 platforms (T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>Pro-Kremlin Telegram channels claim that not only sponsors but also spectators are abandoning the Olympics. On March 27, the aforementioned " Shkvarka 2.0 " channel posted a video featuring the France 24 logo, claiming that Airbnb would automatically make Olympic bookings in France non-refundable due to mass cancellations. In March alone, Airbnb users allegedly canceled over 23,000 bookings, costing the service over ‚Ç¨600,000 in losses, the video claimed.<br><br>Fact-checkers from France 24's Les Observateurs project verified that their own channel had not produced such a video (T0161.001: Impersonated Content, T0097.202: News Outlet Persona) and then contacted Airbnb and the expert mentioned in the video for comment. The short-term rental service stated that the cited figures were fictitious and that there were no mass cancellations during the Olympics (T0164.001: Narrative Presents Fabricated Statistics as Genuine Data, T0160.002: Information is False). Meanwhile, the British company Verdant Leisure stated that their tourism expert, Francesca Holdsworth, had not made any such comments to anyone (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>Finally, a logical plotline in this alternative Olympic universe is the persecution of those who "tell the truth." At the very beginning, we reported on the fake investigation that French intelligence services allegedly launched into journalists who wrote about the closure of the Arras hospital. But such a story is not unique.<br><br>On April 2, the same Shkvarka News reported on censorship imposed on French journalists‚Äîthey were allegedly banned from reporting on issues related to the preparation and hosting of the Olympics. A video confirming this news features the BBC logo, and the investigative project Bellingcat is cited as the source.<br><br>This scheme (a BBC video citing Bellingcat) has become as commonplace as fake magazine covers. At the end of 2023, "Verified" already wrote in detail about the flood of fakes under the British Broadcasting Corporation's brand. This video is no different (T0161.001: Impersonated Content, T0097.202: News Outlet Persona). It consists of stock photos and videos, excerpts from speeches (in this case, TF1 journalist Samira El Ghadir and the channel's executive director, Rodolphe Bellmer) (T0084.002: Plagiarise Content), and screenshots of earlier Olympic fakes are shown as "true reports." Fact-checkers from " Gvara Media " received a comment from Bellingcat founder Eliot Higgins, who called the video a fake. Bellmer's speech about the alleged censorship introduced in 2024, used in the video, is his speech at the launch of one of TF1's projects in 2023 (T0162.003: Historic Content Incorrectly Presented as Current).<br><br>An obvious continuation of the story about the French authorities "concealing the truth" is their crackdown on criticism, including visual criticism. On April 27, the latest fake we've discovered, related to street art and the Olympics, appeared. Simultaneously, the Telegram channels " Shkvarka News " and " Shkvarka 2.0 " reported that an unnamed Parisian municipality had destroyed a supposed ‚Ç¨2 million Banksy work. "A drawing of an athlete throwing a rocket appeared in Paris. The word '2024' is written on the athlete's chest. Apparently a reference to the Olympics. But the French are so obsessed with threats surrounding the Olympics that they painted over the graffiti," the posts stated, which garnered only 7,500 views.<br><br>The drawing is based on a real work by Banksy, which he created back in 2012 in the run-up to the London Olympics (T0161.003: Falsified Graffiti or Signage, T0161.001: Impersonated Content).<br><br>Since the photographs this time showed parts of other buildings, "Verified" was able to pinpoint the exact location . The photo, which was then overlaid with a reworked Banksy mural, was taken in a location already familiar to our regular readers‚Äîthe commune of Montreuil, east of Paris. Specifically, it was taken on the wall of a building at 31 rue Voltaire. This is the eighth graffiti we've analyzed that was allegedly created in the same area (although it was once claimed to be from Brussels).<br><br>To further enhance the credibility of the story, the fakers also fabricated screenshots of Instagram stories and website pages from the French newspapers Le Parisien and Le Monde, respectively. Neither the former nor the latter published such materials (T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>[...]<br><br>In this article, "Verified" covered only a portion of the fakes about the Paris Olympics created by pro-Kremlin Telegram channels from late March to late April. There are over three dozen of them‚Äîand these are just the ones we were able to find. They employed the full arsenal of tools actively used by fakers over the past two years: fake videos from European media outlets, magazine covers, and non-existent graffiti. Some fakes are multi-layered: one media outlet supposedly quotes another, which in turn cites the opinion of a real expert (who, of course, never actually said those words). Clearly, the production line for creating such fakes is well-established.<br><br>However, there's a mystery. None of these fakes went truly viral, with only a few amassing hundreds of thousands of views combined: for example, the post about tuberculosis and Ukrainian refugees only had 130,000. The rest barely circulated online, including in French-language posts on X.<br><br>The goal of these fakes is likely to gain critical mass and create the impression that the Paris Olympics are a surefire failure. Real problems (bedbugs and rats, pollution in the Seine, the cost of the Olympics) are greatly exaggerated in these publications and supplemented with false details.</i> |
| [I00217 False Authority: How the BBC Became a Favorite Brand for Fake Video Creators](../../generated_pages/incidents/I00217.md) | <i>[Translated from original in Russian] The mass production of fake videos featuring the logos of prominent international media outlets is one of the hallmarks of Russia's information war against Ukraine. Since the start of the full-scale invasion, creators of such fakes have released dozens of videos imitating the designs of Al Jazeera , Euronews , Deutsche Welle , Reuters , Fox News , and other major foreign media outlets  (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). But the British Broadcasting Corporation is a particular favorite among disinformationists. "Verified" explains how the BBC's disinformation videos are made, what they cover, and how to distinguish genuine content from fake.<br><br>What the fake videos say<br><br>At first glance, the videos purporting to be BBC reports focus on events unfolding in various countries around the world (T0068: Respond to Breaking News Event or Active Crisis), but all of them portray Ukraine and Western countries in a negative light. "Verified" is aware of nine such recordings, but there could have been more: some may have failed to resonate and escaped the attention of fact-checkers. <br><br>The most recent of these videos appeared in mid-December 2023. It reported that over 10,000 cameras had been discovered in Ukraine, the data from which had been transmitted to Russia and stored on Russian servers since 2004. Although this video is not available on the BBC's website or official social media accounts, it is based on a genuine journalistic investigation published several days earlier by the " Schemes " project. Its authors discovered that TRASSIR cameras were supplied to Ukraine, the data from which was transmitted through Russian servers and could potentially be accessed by the FSB.<br><br>This is the only case known to "Verified" where a fake BBC video was based on credible information (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0160.001: Information is Verified). The video likely served only a supporting role‚Äîit was distributed alongside a fake screenshot of a news story from the British Broadcasting Corporation (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona), which quoted US Congressman Matt Gaetz as saying, "For two years, we supported a country that didn't even bother to get rid of Russian cameras in its capital. The question remains, who is dumber: us or the Ukrainians?" Gaetz has consistently opposed further American aid to Kyiv, and his statements on this topic are regularly quoted by Russian state media.<br><br>Another video released in December claimed that Bellingcat investigators had information about Ukraine selling weapons to the terrorist group Hamas, which attacked Israel on October 7. David Arakhamia, chairman of the Servant of the People faction in the Verkhovna Rada, was accused of organizing the deal, and the information was allegedly confirmed by BBC Verify journalist Shayan Sardarizade and Bellingcat founder Eliot Higgins. According to TGStat data, this video (which Reuters fact-checkers found to be fabricated (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona)) has been viewed over 1 million times in the Russian-language segment of Telegram alone. <br><br>Similar claims (also citing Bellingcat) were present in a video analyzed by "Verified" in October 2023. At the time, the Ukrainian Ministry of Defense was accused of selling weapons to militants. The video, which garnered roughly the same number of views as the later one, featured the BBC logo and intro, as well as other distinctive design elements, but this video also turned out to be fake (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>A third report based on Bellingcat's "findings," allegedly produced by the BBC, circulated in late November. The video, citing the investigative group, claimed that former adviser to the Ukrainian Presidential Office Oleksiy Arestovych spent $500,000 on private jets over six months. Bellingcat founder Eliot Higgins denied conducting such an investigation. The video also appears to be missing from the BBC's website (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>In early October 2023, a video disguised as a BBC story featuring a prediction by essayist Nassim Taleb circulated. Taleb allegedly claimed that the United States was planning to leave NATO and stop aiding Ukraine. The video, which had garnered several hundred thousand views on Telegram, was nowhere to be found on the British media corporation's website or social media accounts (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona), and the writer himself denied ever making such a statement (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>On August 30, fact-checkers at Logically Facts published an analysis of yet another video released on behalf of the BBC, which claimed that Yevgeny Prigozhin did not die in the plane crash in the Tver region, but that his death was allegedly staged by the Kremlin with the consent of the head of the Wagner PMC. The video became popular not only among Russian-speaking internet users (over 3 million views on Telegram) but also in the English-speaking segment (for example, one of the posts on X was viewed over 380,000 times). As in other cases, the video turned out to be a fake, and the BBC had no connection with it (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>The videos described above were edited vertically, but some fake videos were also disguised as more familiar horizontal footage. For example, in early August 2023, a video circulated claiming that "Britain is suffering from sanctions" imposed by London on Russia due to its aggression against Ukraine. However, by changing the video's format, its creators stumbled‚Äîthe BBC does not use such a format for such videos (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). The viral video, which consists of a montage of photos and reports unrelated to the topic of sanctions, is not available on the broadcaster's website or social media. The experiment was relatively unsuccessful‚Äîthe fake story failed to generate much interest in either English or Russian. <br><br>In May 2022, a video widely circulated on social media X , purporting to show BBC journalists showing an order to send Polish troops to Ukraine (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). Fact-checkers from the Polish project Demagog discovered that the document was falsified (T0085.004: Develop Document, T0161.001: Impersonated Content, T0097.206: Government Institution Persona), while their colleagues at Reuters analyzed the video, which mimicked the BBC design, and came to a similar conclusion. In Russian, the image of the order gained more popularity , while in English, the fake footage gained more popularity. <br><br>In April 2022, a video of a missile strike on a train station in Kramatorsk , killing 61 civilians , went viral . The video, purportedly released by the BBC, blamed Ukraine for the attack (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). The subtitles stated that Russia does not use Tochka-U missiles, and the serial number on a shell fragment found at the site of the tragedy indicated that the missile belonged to Ukraine. <br><br>This ‚ÄúBBC story‚Äù was broadcast on Russian state television channels (in particular, on ‚ÄúRussia 24‚Äù), and also formed the basis for numerous publications in the Russian media .<br><br>Last spring, journalists, fact-checkers, and media researchers first noticed the emergence of several fabricated stories about the war in Ukraine, disseminated under the branding of the BBC and other well-known media outlets. BBC correspondent Joe Inwood wrote on his X account (then Twitter): "...Someone appears to have obtained BBC video tools and created a convincing, but fake, video." <br><br>The fake videos described above were designed identically, using the BBC News banner, the British television channel's logo, and a consistent design code. The vast majority of these videos are vertical and mimic the style of BBC social media posts. The fake videos are primarily edited together from video fragments available in the public domain or produced by other media outlets (T0165: Edited Content, T0084.002: Plagiarise Content), accompanied by subtitles and background music. Similarly, as noted above, over the past year and a half, videos have been faked and attributed to media outlets around the world.<br><br>Along with the fake videos, equally fake screenshots of publications on the websites of various foreign media outlets, including the BBC , were also distributed (T0086: Develop Image-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). Sometimes, they even became elements of disinformation videos. For example, a screenshot of a non-existent BBC news story was featured in a video purportedly from Reuters, which reported on appeals from boxer Mike Tyson and actor Elijah Wood to Ukrainian President Volodymyr Zelenskyy (the Western stars allegedly asked the politician to begin drug addiction treatment (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)).<br><br>Fake reports attributed to the BBC have appeared before, but they were far fewer in number, presented differently, and, in most cases, apparently were not created with the intention of disinformation. For example , in 2018, a video about an imminent nuclear war between NATO and Russia went viral (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona) ( a second wave of distribution occurred in January 2022 (T0160.006: Content Previously Fact Checked)). The video purported to be a clip from a BBC television broadcast, but it turned out that the recording was created independently of the British broadcaster and "purely for entertainment purposes." Neither "Verified" nor other fact-checking projects were able to find any similar disclaimers for more recent videos.<br><br>There's no trace of any of the videos described above, which appeared in 2022‚Äì2023, on the BBC website or on the corporation's social media accounts, and the media group's official representatives have repeatedly been forced to claim that the stories are fabricated. These claims are independently confirmed by fact-checking projects operating in various countries. However, by this point, the fake publications have accumulated millions of views and influenced the opinions of many people about the global situation. <br><br>Who is behind the falsifications?<br><br>Many of the videos described above began spreading on social media shortly after the publication of statements by Russian officials and speakers collaborating with state media containing the same points (T0068: Respond to Breaking News Event or Active Crisis). For example, the narrative about Ukrainian weapons in the hands of Hamas militants emerged on October 7, the day of the group's attack on Israel. This was reported in a commentary to Argumenty i Fakty , without citing a source, by orientalist Yevgeny Satanovsky, until recently a regular guest on TV host Vladimir Solovyov's broadcasts. On October 9, Deputy Chairman of the Russian Security Council Dmitry Medvedev made the same statement on his Telegram channel , and the very next day, a fake video about a non-existent Bellingcat investigation began circulating in the Russian-language segment of Telegram (T0087: Develop Video-Based Content, T0068: Respond to Breaking News Event or Active Crisis, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). The report that the attack on the Kramatorsk train station was carried out by Ukrainian troops, not Russian ones, was widely disseminated in Russia by federal media on the day of the attack, April 8, 2022. Soon, a video with the BBC logo, repeating the official Russian narrative, appeared on social media (T0087: Develop Video-Based Content, T0068: Respond to Breaking News Event or Active Crisis, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). <br><br>[...]<br><br>Almost all the videos examined by "Verified" and other fact-checkers mimic the design of the videos the BBC uses on its X social media account and sometimes on Instagram , making them difficult to immediately distinguish from the real thing. However, sometimes it's enough to simply visit the website or the official BBC News accounts on X , Instagram , YouTube , or TikTok to confirm that such information has not been published there.<br><br>There are also a number of significant differences in the design of fake and real videos, which make it possible to determine whether a video is fabricated or not, even without specialized tools or video content verification skills. <br><br>Firstly, the fake videos feature background music, while the originals rarely use it (if music is present, it's overlaid on the main audio track). The so-called caption videos (videos with embedded subtitles) published by the BBC use the original audio from the footage used in the videos , while the fake videos, on the contrary, lack it.<br><br>Secondly, when a genuine BBC report shows a clip of someone speaking, their voice is not muted. For example, in the fabricated December video about Ukraine selling arms to Hamas, the footage of Eliot Higgins and Shayan Sardarizadeh allegedly making this statement is accompanied only by background music, and the British journalist's lip movements make it clear that he is speaking a completely different phrase than the one in the caption. Footage of Nassim Taleb was used in exactly the same way in a report about the impending US withdrawal from NATO. <br><br>Thirdly, although the disinformation campaigners use elements of the BBC video design (the opening title, the logo, the characteristic vertical red stripe near the title), the font in the fake videos does not match the company's.<br><br>Fourth, native speakers may note that the captions of such fake videos regularly contain spelling, grammar, and stylistic errors that are virtually impossible to find in a reputable media corporation. For example, an October video about a Bellingcat investigation includes the phrase "Ukrainian Defense Prime Minister," while the captions of a video about Nassim Taleb's forecast sometimes lack the necessary articles.<br><br>These recommendations apply to any videos of this kind, regardless of whose logo you see in the frame. And they're worth taking on board, as there's no reason to believe that the current method of fake news stories, which generates millions of views, will become tiresome for disinformation makers anytime soon.</i> |
| [I00218 BBC News video claiming Prigozhin death was staged is a fake](../../generated_pages/incidents/I00218.md) | <i>Context<br><br> <br><br>An alleged BBC News social media video containing claims that the Kremlin staged the death of Yevgeny Prigozhin and that the Wagner leader is still alive is circulating on social media. "BBC is going full conspiracy theory 'It was all staged - Prigozhin is alive,'" reads one post containing the video, uploaded to X (formerly Twitter) on August 29, 2023, that has amassed 335,000 views.  <br><br>The video is edited in the style of videos uploaded to the BBC's social media channels, using the BBC News logo. However, the video is a fake . The BBC has not reported that Prigozhin's death was staged or that he is still alive. On the contrary, the BBC has cited Russian authorities sources that Prigozhin died in a plane crash on August 23, 2023, and have reported from the cemetery where he is believed to have been buried on August 29  (T0087: Develop Video-Based Content, T0068: Respond to Breaking News Event or Active Crisis, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). <br><br>In fact<br><br>BBC Verify journalist Shayan Sardarizadeh debunked the video on X (formerly Twitter). "A fake video with the logo and branding of BBC News is being shared online, claiming that Wagner chief Yevgeny Prigozhin's death was staged by the Kremlin. The video is completely fake. BBC News has never published such a video," wrote Sardarizadeh in a post. <br><br>A BBC spokesperson told Logically Facts: "We are aware of this fake video, and our lawyers are urgently looking into it. In a world of increasing disinformation, we urge everyone to check links and URLs to ensure they are getting news from a trusted source."<br><br>[...]<br><br>The verdict<br><br>A fake video using the BBC's logo and style claims that Yevgeny Prigozhin's death was staged by the Kremlin. The video has never been available on BBC media channels and has been debunked by a BBC journalist. No reporting corresponding with the video can be found on BBC's website. Therefore, we have marked the video as fake.</i> |
| [I00219 Fact Check-BBC did not report Poland preparing to send troops to Ukraine](../../generated_pages/incidents/I00219.md) | <i>A clip that purports to show content published by the British Broadcasting Corporation (BBC) reporting that Poland is preparing to send troops to Ukraine is digitally altered and was not published by the outlet (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona) .<br><br>The clip opens with the BBC logo and a clip of a helicopter landing. The video mimics the formatting of the broadcaster‚Äôs social media clips, and incorporates the same font, caption placement and graphics. The clip suggests Poland is preparing to send forces into Ukraine, with visuals of military vehicles and personnel in the background.<br><br>Examples of the clip shared by users with captions in numerous languages (T0101: Create Localised Content), including Russian, English, and French.<br><br>No such video was published by the outlet, however, and the clip is an example of ‚Äòimposter content‚Äô - a video or image that impersonates a legitimate organization, such as a news outlet, to push forward a particular narrative or claim.<br><br>A spokesperson for the BBC told Reuters that no such video was published by the outlet, adding that ‚Äúit‚Äôs best to check the BBC News website to verify stories.‚Äù<br><br>[...]<br><br>The clip also features an alleged letter as ‚Äúproof‚Äù of Polish forces preparing to ‚Äúinvade Ukraine.‚Äù (T0085.004: Develop Document, T0161.001: Impersonated Content, T0097.206: Government Institution Persona) The Twitter account for the Polish General Command of the Armed Forces called the image of the alleged letter ‚Äúfake.‚Äù<br><br>‚ÄúThis is a false order of the Polish General Staff. The whole document is FAKE! We observe more and more such counterfeit military documents in Polish mass media. Please, DO NOT share this FAKE NEWS,‚Äù the tweet reads.<br><br>[...] <br><br>The BBC did not release a video claiming that Poland was preparing to send troops to Ukraine. The clip was never published by the BBC and is an example of ‚Äòimposter content‚Äô - a video or image that impersonates a legitimate news source.</i> |
| [I00220 Fact Check: Fake BBC clip on Ukrainian politician selling arms to Hamas](../../generated_pages/incidents/I00220.md) | <i>The BBC did not publish a video claiming that a Ukrainian politician sold arms to Hamas as suggested by images circulating online that carry BBC branding (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona) and cite investigative group Bellingcat (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). The clip is fake, according to representatives of both BBC and Bellingcat.<br><br>The clip as well as screengrabs from the video, which surfaced on X and Facebook, have the appearance of a BBC social media video, with the BBC logo visible at the top left corner.<br><br>Text overlaid on the video at the start of the clip reads: ‚ÄúA Ukrainian politician may be involved in arms sales to Hamas.‚Äù<br><br>Further text cites Bellingcat as having reported that the International Criminal Court is ‚Äúpreparing a case‚Äù against David Arakhamia, the leader of Ukraine‚Äôs Servant of the People political party, although the video caption incorrectly spells his name.<br><br>The video (timestamp 00:48s) cites BBC journalist Shayan Sardarizadeh as saying: ‚ÄúWe are dealing with serious allegations.‚Äù<br><br>Sardarizadeh said on X that the quote attributed to him is ‚Äúfake‚Äù and ‚Äútotal nonsense.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>A BBC spokesperson told Reuters in a Dec. 6 email, ‚ÄúThis is not a BBC News video.‚Äù<br><br>Founder of Bellingcat, Eliot Higgins also said in an email that Bellingcat "has done no investigation that supports the claims in the videos.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>[...]<br><br>BBC News did not publish the video, a spokesperson said to Reuters. Bellingcat did not release any related report, the founder of the organization said.</i> |
| [I00221 Expert says Russian influence campaign behind video making false claims about Zelensky](../../generated_pages/incidents/I00221.md) | <i>We have been examining a viral video which baselessly claims Ukrainian President Volodymyr Zelensky has secretly acquired Russian citizenship.<br><br>The video, which bears the logo of the genuine Ukrainian news outlet the New Voice of Ukraine, has amassed hundreds of thousands of views on platforms like X and Threads.<br><br>It falsely claims (T0160.002: Information is False) that secret documents obtained by the hacking collective Anonymous indicate Zelensky owns a luxury apartment in Moscow. The clip also includes a purported copy of a Russian passport issued in 2015 bearing the name and photo of Zelensky.<br><br>There‚Äôs no evidence for either of those claims.<br><br>New Voice of Ukraine editor-in-chief Yulia McGuffie tells BBC Verify that the clip is ‚Äúa complete fake‚Äù and linked to ‚ÄúRussian-backed‚Äù actors.<br><br>‚ÄúWe have never published anything like that on any of our platforms,‚Äù she adds (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>Independent researchers told us the video is likely to be the work of a Russian disinformation campaign known as Storm-1516, which has a history of making fake videos aimed at undermining Western support for Ukraine.<br><br>Darren Linvill, a media forensics specialist at Clemson University in the US, says "there is no question this is Storm-1516".<br><br>"Russian influence campaigns have used Anonymous as a vehicle to sow lies in the past, so that is not surprising."</i> |
| [I00222 How Russia is trying to disrupt the 2024 Paris Olympic Games](../../generated_pages/incidents/I00222.md) | <i>In the summer of 2023, a curious set of videos crept into social media platforms. Telegram feeds that normally promoted pro-Kremlin narratives suddenly began promoting a film called ‚ÄúOlympics Has Fallen.‚Äù Users were encouraged to scan a QR code that directed them to a Telegram channel of the same name (T0153.004: QR Code Asset, T0122: Direct Users to Alternative Platforms, T0151.007: Chat Broadcast Group). Upon arriving at this channel, viewers encountered a feature-length film (T0087: Develop Video-Based Content) with a similar aesthetic and a play on the title of the American political action movie ‚ÄúOlympus Has Fallen,‚Äù released more than a decade earlier. AI-generated audio impersonating the voice of film actor Tom Cruise (T0166.001: Deepfake Impersonation, T0088: Develop Audio-Based Content, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution) narrated a strange, meandering script disparaging the International Olympic Committee‚Äôs leadership.<br><br>Nearly a year later and with less than 80 days until the opening of the 2024 Paris Olympic Games, the Microsoft Threat Analysis Center (MTAC) has observed a network of Russia-affiliated actors pursuing a range of malign influence campaigns against France, French President Emmanuel Marcon, the International Olympic Committee (IOC), and the Paris Games. These campaigns may forewarn coming online threats to this summer‚Äôs international competition.<br><br>[...]<br><br>Old world tactics meet the age of AI<br><br>Starting in June 2023, prolific Russian influence actors‚Äîwhich Microsoft tracks as Storm-1679 and Storm-1099‚Äîpivoted their operations to take aim at the 2024 Olympic Games and French President Emmanuel Macron. These ongoing Russian influence operations have two central objectives: to denigrate the reputation of the IOC on the world stage; and to create the expectation of violence breaking out in Paris during the 2024 Summer Olympic Games.<br><br>The ‚ÄúOlympics Has Fallen‚Äù website (T0152.004: Website Asset) and video became the first in many videos MTAC encountered from Storm-1679. The video, which falsely purported to be a Netflix documentary (T0087: Develop Video-Based Content, T0161.001: Impersonated Content) narrated by the familiar voice of American actor Tom Cruise, clearly signalled the content‚Äôs creators committed considerable time to the project and demonstrated more skill than most influence campaigns we observe. Further analysis confirmed the fake documentary used AI-generated audio resembling Cruise‚Äôs voice to imply his participation, spoofed Netflix‚Äôs iconic intro scene and corporate branding, and promoted bogus five-star reviews from reputable media outlets like the New York Times, the Washington Post, and the BBC, all amid slick computer-generated special effects. Social media accounts associated with Storm-1679 promoted the documentary across several platforms, attempting to reach US and European social media users. As previously reported in MTAC‚Äôs December 2023 report, Storm-1679 deceived US celebrities into recording short videos on Cameo (T0010: Cultivate Ignorant Agents), a popular website where users can pay for personalized video messages from celebrities, and deceptively edited the videos into anti-Ukrainian propaganda (T0165: Edited Content). Among the edits to those Cameo videos included advertisements for Olympics Has Fallen and QR-code links to the Telegram channel where it was hosted, giving the false impression that American celebrities were endorsing and promoting the film (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>Videos, fakes, and false flags sow anticipation of violence in Paris<br><br>The ‚ÄúOlympics Has Fallen‚Äù video signalled the first glimpse of what would prove to be an extensive campaign by Storm-1679. The Paris 2024 Summer Olympic Games has been one of Storm-1679‚Äôs primary objectives since at least the summer of 2023, but MTAC has previously observed this actor heavily targeting the Ukrainian refugee community living in the US and Europe. Their principal tactic involves creation of spoofed content that mimics reputable media outlets (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona), particularly in short-form video clips posted on social media.<br><br>Storm-1679‚Äôs Olympics-focused disinformation goes beyond defaming the IOC and seeks to foment public fear to deter spectators from attending the Games. Over the past year, Storm-1679 has consistently produced a collection of deceptive videos, suggesting that legitimate and trusted sources are conveying accurate information regarding expected violence during the Paris Games. In a short video, masquerading as a clip from Brussels-based media outlet Euro News (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona), Storm-1679 falsely claimed that Parisians were buying property insurance in anticipation of terrorism surrounding the Games (T0160.002: Information is False). In another spoofed news clip, this time impersonating French broadcaster France24 (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona), Storm-1679 falsely claimed that 24% of purchased tickets for Olympic events had been returned due to similar fears of terrorism (T0164.001: Narrative Presents Fabricated Statistics as Genuine Data). In a more explicit attempt to dissuade citizens from attending the Games, Storm-1679 produced fake video press releases posing as the American Central Intelligence Agency (CIA) and the French General Directorate for Internal Security (DGSI) (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.206: Government Institution Persona) warning potential attendees to stay away from the Paris 2024 Olympics due to the alleged risk of a terror attack.<br><br>Prior to spring 2024, Storm-1679 primarily launched disinformation operations in English, only occasionally producing content in French and German, among other European languages (T0101: Create Localised Content). MTAC tracked a notable increase in Storm-1679‚Äôs French-language content as the Olympics campaign gained steam, possibly signaling an effort to target the French public more directly or set the scene for alleged unrest in the lead-up to the Games.<br><br>The most worrisome disinformation advanced by pro-Russian actors has sought to impersonate militant organizations and fabricate threats to the Games amidst the Israel-Hamas conflict (T0068: Respond to Breaking News Event or Active Crisis). During fall 2023, Storm-1679-linked social media accounts posted images claiming to show graffiti painted in Paris that threatened violence against Israelis attending the 2024 Games (T0161.003: Falsified Graffiti or Signage). In several of the images, Storm-1679 referenced the attacks at the 1972 Munich Olympics, where members of Black September‚Äîan affiliate of the Palestine Liberation Organization‚Äîkilled 11 members of the Israeli Olympic team and a West German police officer. Microsoft did not observe any independent confirmation that the graffiti physically exists, suggesting the images were likely digitally generated. Separately, in November 2023, a Russian-language X (formerly Twitter) account posted a video purportedly produced by the Turkish ultranationalist organization the Grey Wolves, which included imagery from the 1972 Munich Games attacks, hinting at similar attacks at Paris 2024. The video caused significant confusion online in the hours after it was posted, many assuming it to be authentic, and even prompted a response from the Israeli Olympic Committee. Microsoft does not presently have enough information to attribute the video to a specific actor, but its heavy amplification by pro-Russian bot accounts suggests the video may be another operation in the broader Olympics campaign.<br><br>Storm-1679 is not the only Russia-aligned actor with a particular‚Äîand growing‚Äîinterest in undermining the 2024 Paris Games. The Russia-affiliated influence actor Microsoft tracks as Storm-1099‚Äîbetter known as ‚ÄúDoppelganger‚Äù‚Äîhas also increased its anti-Olympics messaging in the past two months. Articles published by the actor‚Äôs core disinformation outlet Reliable Recent News (RRN), as well as any of its 15 unique French-language ‚Äúnews‚Äù sites, echo claims of rampant corruption within the IOC and warn of potential violence at the Games. Similarly, in Storm-1099‚Äôs spoofs of reputable media outlets‚Äîpro-Russian clones of legitimate media websites peddling propaganda (T0152.004: Website Asset, T0143.003: Impersonated Persona, T0097.202: News Outlet Persona)‚Äîforgeries of French outlets Le Parisien and Le Point also raise the specter of violence and castigate Macron‚Äôs government. Posts in this network of forgeries criticize Marcon‚Äôs showmanship around the Games and emphasize his indifference to the socio-economic hardships faced by French citizens.</i> |
| [I00223 "Matriochka", la nouvelle campagne de d√©sinformation anti-ukrainienne √† destination des m√©dias occidentaux](../../generated_pages/incidents/I00223.md) | <i>[Translated from original in French] "Please verify this information" (T0039.002: Solicit Production of Fact Check): the modus operandi is well-rehearsed and is part of an operation nicknamed "Matriochka" (or "Russian dolls") by the "Antibot4Navalny" collective ( @antibot4navalny on X) which tracks influence operations related to Russia on this social network. <br><br>For example , an internet user, "K√§the", commented on X on December 4 on a BFM publication to ask the channel to verify a video that looks like a report from the German television Deutsche Welle claiming that a "Ukrainian artist sawed down the Eiffel Tower" (T0116: Comment or Reply on Content, T0039.002: Solicit Production of Fact Check).<br><br>"I see this kind of information every day. The official media don't talk about it, what am I supposed to believe?" she asks.<br><br>Within a few hours, this profile attracted the attention of dozens of French media outlets such as Le Progr√®s, Paris Match, Mediapart, Le Point, Franceinfo, Le Figaro, RFI, Le Parisien...<br><br>The account then remained inactive until December 20: it then posted a graffiti of Zelensky caricatured as a homeless person in Los Angeles, an image that another account would in turn ask media outlets to verify, and so on (T0039.001: Collaborating Assets Seed and Ping).<br><br>The videos that need to be verified are, in the vast majority of cases, mimicking the style of major French and international media outlets (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). [...] <br><br>The data provided by "Antibot4Navalny," which AFP investigated, documents the existence of dozens, if not hundreds, of profiles using this strategy of mass media harassment. Most of these are accounts abandoned by their owners and then hacked (T0150.005: Compromised Asset). A sign that these accounts have been taken over by bots is that posts sometimes appear at a rate of one per minute, effectively flooding social media. <br><br>AFP's analysis also revealed that accounts that ask media outlets to verify fake news will themselves spread it some time later.<br><br>Misappropriation of military aid to Ukraine, truncated or invented Zelensky graffiti (T0161.003: Falsified Graffiti or Signage) and fake advertisements on Times Square: these publications always implicate Ukrainians and seek to stir up the idea of  |
| [I00224 Photofake: Anti-Ukrainian Billboards Appear in Poland](../../generated_pages/incidents/I00224.md) | <i>The photograph circulated by Russian media and pro-Kremlin social media users as proof of anti-Ukrainian advertising in Poland, is an example of basic photoshop manipulation (T0165: Edited Content). There are scores of identical photos on the web which containe different advertisements on the billboards.<br><br>[...]<br><br>Kremlin media and social media users are spreading claims that anti-Ukrainian billboards have appeared in Poland. As evidence, a photograph of a billboard is attached (T0161.003: Falsified Graffiti or Signage), which depicts the legs of a man in military garb with a prosthesis against the background of the Ukrainian and Russian flags and the inscription ‚ÄúThis is not our war.‚Äù<br><br> ‚ÄúThey want to develop this campaign throughout Poland by placing billboards. This is how the authorities want to influence the thinking of local residents who support Ukraine in the Special Military Operation,‚Äù writes Pravda.ru, and points out that the prosthetic leg on the billboard is featured over the Ukrainian flag.<br><br>StopFake factcheckers decided to check the authenticity of the photo featured on Pravda.ru. The image was was edited in a photo editor (T0165: Edited Content, T0162.008: Context Reframed by Edits to Media). A TinEye reverse photo search showed that there are many photos of this billboard on the Internet, taken from an identical angle, but with different advertisements on the billboard.<br><br>After a little online searching, we found the site [...] which allows you to insert any image on this billboard for free. The fact that this particular photo was used to create a fake about alleged ‚Äúanti-Ukrainian advertising‚Äù in Poland is evidenced by the identical placement of cars and clouds in the doctored fake Russian photo, as in the distributed stock photo.</i> |
| [I00225 Operation Overload Impersonates Media to Influence 2024 US Election](../../generated_pages/incidents/I00225.md) | <i>Target Audience 1: Media Organizations, Fact-Checkers, and Researchers <br><br>US and international media organizations, fact-checkers, and researchers are almost certainly the primary target audience of Operation Overload, based on industry research, recent credible testimonials from various media organizations, and Operation Overload automated social media accounts (T0146.007: Automated Account Asset) that tag media organizations, request fact-checking verification (T0039.002: Solicit Production of Fact Check), and reply to social media posts from these organizations with spam (T0049.008: Generate Information Pollution). These individuals and organizations themselves are at risk of brand abuse and potential impersonation. Similarly to the Russia-linked influence operation Doppelg√§nger, Operation Overload frequently abuses logos and stylized wordmarks of US and international media organisations (T0161.001: Impersonated Content). Known tracked organizations impersonated in Operation Overload per Insikt Group‚Äôs investigation are available in Appendix A. <br><br>According to CheckFirst, between August 2023 and May 2024, Eastern European fact-checking and counter-propaganda outlets, particularly in Ukraine, were among those ‚Äúmost actively engaged‚Äù in publishing debunks of Operation Overload content. Though the reported trend demonstrated Eastern European-based researchers as at the forefront of combatting Russian propaganda, there were concerns about whether these organizations were effectively collaborating on incoming leads. <br><br>Objective 1: Overwhelm Target‚Äôs Research Resources <br><br>CheckFirst research indicated that Operation Overload aims to overwhelm journalists and fact-checkers with persistent, spam-like requests to verify the operation‚Äôs inauthentic content (T0039.002: Solicit Production of Fact Check), diverting resources from pursuing legitimate investigations and debunking Russian disinformation. The operation almost certainly seeks to inundate media, fact-checkers, and researchers‚Äô resources ‚Äî both time and personnel ‚Äî causing time-sensitive, critical, legitimate investigations to be overlooked or delayed (T0049.008: Generate Information Pollution). This tactic creates confusion and distraction, ensuring that key narratives or investigative avenues slip through unnoticed, enabling mis- and disinformation to spread unchallenged. By leading journalists and fact-checkers into pursuing fake fact-checking leads or failing to verify legitimate information, the operation exhausts the capacity of the aforementioned organizations, diminishing their ability to verify accurate information. <br><br>Objective 2: Use Target to Launder Disinformation into Mainstream Discourse <br><br>In pursuing Objective 1, Operation Overload very likely also hopes to use media organizations as a means of information laundering; more specifically, to inject malign content and pro-Kremlin narratives into mainstream political discourse via trusted parties. Should a member of Target Audience 1 3 unintentionally fact-check and publish a report using Operation Overload's fake leads, there is an added risk of giving additional legitimacy to the false story and lending credibility to the operation‚Äôs broader themes, increasing the likelihood that audiences will believe it, and pushing malign influence deeper into mainstream discourse (T0039: Bait Influencer).<br><br>[...]<br><br>Insikt Group has observed several instances of Operation Overload content negatively depicting Vice President Harris, the Harris campaign, and members of Vice President Harris‚Äôs immediate family in a manner we assess is very likely aimed at undermining her candidacy for president (T0135: Undermine). We have also observed Operation Overload negatively depicting the candidacy of former President Trump, though based on our ongoing collection of Operation Overload content, content negative of Vice President Harris is at least four times more frequent than negative portrayals of former President Trump. Targeting each candidate with content intended to damage their reputation is consistent with historic Russia-linked influence operations against major US elections. We assess that the significant disparity between the level of targeting of each candidate likely indicates whose policies the Kremlin perceives to be better aligned with its strategic objectives. <br><br>To illustrate how Operation Overload targets Vice President Harris‚Äôs campaign, one video, impersonating the BBC on Telegram, falsely claimed her campaign lost over $1.2 million on ‚Äúnon-existent advertising services‚Äù (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>[...]<br><br>Operation Overload has also produced limited content that discredits the Trump campaign with false claims. For instance, one video impersonating the BBC falsely claimed that former President Trump had received an endorsement from Norwegian neo-Nazi Anders Behring Breivik (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>In another instance, a video impersonating a Fox News brief published on the ‚ÄúGagauz Republic‚Äù Telegram account falsely claimed that Yum! Brands filed a lawsuit against the Harris campaign to prevent it from using Kentucky Fried Chicken (KFC) in its advertisements (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). The video cited an inauthentic quote attributed to Lexington, Kentucky attorney, J. Dale Golden, who suggested the Trump campaign pressured Yum! Brands to file the fictitious lawsuit, alleging Trump was concerned Harris would attract African-American voters with the KFC brand. The video, using Golden‚Äôs likeness, further attempted to reinforce racial stereotypes by stating, ‚Äúand we all know who enjoys KFC more than anyone else‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>[...]<br><br>Operation Overload-affiliated social media accounts, as well as Russian-language Telegram accounts posting inauthentic videos deceptively attributed to reliable ‚Äúsources‚Äù, have published clips that vilify Ukrainian refugees in the US as purveyors of crime, drugs, and disease (T0135.001: Smear). For example, video segments impersonating US-based CNN recently suggested that a ‚ÄúUkrainian woman with HIV infected more than 200 American men‚Äù and that ‚Äúan influx of Ukrainian refugees‚Äù resulted in an increase in violence in Los Angeles‚Äôs Skid Row neighborhood (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona). In a video impersonating USA Today, Operation Overload assets claimed that Ukrainian refugees ‚Äúare being used to transport fentanyl‚Äù into the US (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona).<br><br>[...]<br><br>Operation Overload very likely attempts to provoke negative sentiment in the US toward the LGBTQIA+ community (T0135.004: Polarise), using disinformation to perpetuate discriminatory beliefs around transgender individuals, perceived behavioral issues, gender transition and reassignment surgeries, and pharmaceutical treatments. One video published in September 2024, impersonating The Washington Post, claimed that LGBTQIA+ families are more likely to be approved for adoption than traditional families, citing fabricated data from the National Children's Bureau (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.202: News Outlet Persona, T0164.001: Narrative Presents Fabricated Statistics as Genuine Data).<br><br>[...]<br><br>Professional Imposter <br><br>Operation Overload deploys a range of professional and multi-layered techniques to build credibility and disseminate influence content. The operation‚Äôs initial layer to build credibility stems from its repeated misuse of a media organization's brand, including abuse of an organization‚Äôs logo, emulation of the brand‚Äôs graphics packages for visual media, and attempts to attribute its videos to journalists employed by the impersonated media organization (T0161.001: Impersonated Content). <br><br>The video content from Operation Overload is crafted as if it was found on a legitimate organization‚Äôs social media account. The videos are short (one to two minutes in length), incorporate background news audio (sometimes referred to as a ‚Äúnews bed‚Äù), and incorporate real footage and static images pulled from publicly available sources. For example, an impersonation of the BBC published on September 20, 2024 (Figure 3), recycled a photograph Vice President Harris posted to her social media account on September 8, 2024, to commemorate National Grandparents Day in the US (T0084.002: Plagiarise Content). <br><br>Further, the videos often cite inauthentic research or investigation findings from reputable organizations and fabricated testimonials attributed to professionals in the corresponding field to falsely appeal to authority and add a layer of legitimacy to the video content (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). For example, many documented videos we attribute to Operation Overload have cited fake research from Bellingcat, almost certainly attempting to exploit the organization‚Äôs established credibility as a reputable investigative outlet. <br><br>Insta-imposed <br><br>As demonstrated in Figures 18‚Äì23, Operation Overload creates static media that appears to be screenshots taken of Instagram Stories from real media organizations. These fabricated screenshots often include a tagged article link edited onto the image to provide an additional layer of legitimacy (T0122: Direct Users to Alternative Platforms). <br><br>Instagram overlays were mostly used to produce content covering fake graffiti (T0161.003: Falsified Graffiti or Signage) from multiple angles; these images are similar to other fake graffiti campaigns purportedly across Europe, documented in AFP‚Äôs initial analysis as well as in CheckFirst research, which found the ‚Äúgraffiti paintings are deceptively superimposed onto authentic photos of urban landscapes ‚Ä¶ from places in Germany and France‚Äù. Recent examples targeting US audiences, as shown in the following figures, indicate Operation Overload's attempts to attribute the graffiti to legitimate artists, likely to bolster the images‚Äô credibility (T0161.001: Impersonated Content). <br><br>There is a persistent risk that Operation Overload may expand to impersonating additional online video-centric platforms. In early October 2024, Insikt Group located an Operation Overload video on a mainstream social media platform impersonating a YouTube Short posted by US telecommunications company AT&T (T0087: Develop Video-Based Content, T0161.001: Impersonated Content, T0097.205: Business Persona). The AT&T impersonation video urged ‚Äúviewers‚Äù of the YouTube Short to ‚Äúvote for Harris if you want kids‚Äô [expletive] to be cut‚Äù and ‚Äúvote for Trump if you want prices to be cut‚Äù.<br><br>Amplification and Automation Operation <br><br>Overload very likely seeks to amplify its content through CIB, documented email campaigns, and promotion through pro-Kremlin social media personalities. Operation Overload also uses hashtags, such as #BBC, #FoxNews, or #USATODAY, in its posts to increase content views (T0015.001: Use Existing Hashtag). Likewise, we located accounts that directly tag media or other related media organizations and political figures (T0039: Bait Influencer), including persistent requests to verify the videos (T0039.002: Solicit Production of Fact Check), with the likely goal of supplementing the operation‚Äôs email campaigns. Notably, the method of reply spam activity is strongly similar to Insikt Group‚Äôs observations of the Doppelg√§nger network in the fall of 2023 and into 2024. <br><br>Insikt Group assesses that many individual social media accounts engaged in persistent social media amplification and CIB to promote Operation Overload content were likely compromised and repurposed for automated engagement boosting (T0146.007: Automated Account Asset, T0150.005: Compromised Asset). This analysis is based on Operation Overload using ‚Äúaged‚Äù accounts, including some that had their original posts from several years ago, often in local languages. The accounts would go dormant for several years and then are suddenly reactivated (T0150.002: Dormant Asset). The social media accounts frequently boost content unrelated to Operation Overload, including likely engagement farming related to cryptocurrency and NFTs. Currently, Insikt Group assesses that the automated accounts are likely services provided by outsourced entities commissioned by the administrators of Operation Overload that advertise social media promotion and engagement boosting. However, additional research on these networks engaged in CIB will be required for further confirmation.<br><br>AI Voiceover Usage <br><br>Insikt Group has observed some videos we attribute to Operation Overload employing the use of AI-generated voiceovers to narrate its content, stylistically similar to a news broadcast or reporting from a legitimate journalist (T0166: AI-Generated Content, T0088: Develop Audio-Based Content). More specifically, the administrators of Operation Overload appear to prefer combining AI-generated voiceovers with content impersonating USA Today, as well as content in Ukrainian. Operation Overload may attempt to integrate AI-generated voiceovers into other content impersonating other media entities.<br><br>QR-edibility <br><br>Social media ‚Äúseed‚Äù accounts that initially post Operation Overload content regularly incorporate the use of QR codes in their posts, almost certainly to aid in establishing credibility for the inauthentic video. The QR codes (T0153.004: QR Code Asset), often marked as ‚Äúverified‚Äù, redirect to the homepage of a trusted organization (such as the BBC, VIGINUM, and so on). Some of these QR codes have previously redirected to a seemingly benign payload hosted on the IP address 127.255.255[.]254. Although benign at the time of writing, we recommend avoiding scanning instances of such QR codes identified in the wild, as they can be leveraged in the future for delivering malware or other malicious activities.<br><br>Insikt Group can further confirm CheckFirst‚Äôs own analysis of the QR codes, including Operation Overload‚Äôs use of the QR code management tool, me-qr[.]com. Further analysis from CheckFirst indicated that the QR codes were possibly managed by an individual connected to Otri, a Russian marketing agency.</i> |
| [I00226 Headline misrepresents a California reproductive health bill](../../generated_pages/incidents/I00226.md) | <i>CLAIM: A California bill would allow mothers to kill their babies up to seven days after birth.<br><br>AP‚ÄôS ASSESSMENT: False. A bill in the California legislature, AB 2223, is being falsely represented. It was introduced to provide legal protection to women who lose their child due to pregnancy complications, and those who have abortions. It does not legalize the killing of infants (T0160.002: Information is False).<br><br>THE FACTS: Social media users are sharing a headline that falsely suggests a proposed California bill will legalize ‚Äúinfanticide,‚Äù the killing of an infant (T0160.002: Information is False).<br><br>‚ÄúCalifornia introduces new bill that would allow mothers to kill their babies up to 7 days after birth,‚Äù reads the erroneous headline on a story published on the Miami Standard, a conservative website.<br><br>‚ÄúTo everyone saying it‚Äôs fake because it was posted on 4/1 just do some research. 99% of y‚Äôall don‚Äôt stay in Cali. It‚Äôs called The infanticide bill,‚Äù claimed a Facebook post sharing a screenshot of the headline on April 1 with over 11,000 reshares.<br><br>But the posts misrepresent the purpose of the bill and its potential impact. The bill eliminates a requirement that a coroner must investigate deaths related to suspected self-induced or criminal abortion. Coroner statements on certificates for a fetal death could not be used to pursue a criminal case against the mother.<br><br>The aim of the bill, introduced Assemblywoman Buffy Wicks, a Democrat representing the East Bay, is to protect women who end a pregnancy or have a miscarriage from being investigated, persecuted or incarcerated. Erin Ivie, a spokesperson for Wicks told The Associated Press in an email. ‚ÄúThe bill is specific to pregnancy and pregnancy-related outcomes, and does not decriminalize the ‚Äòmurder of babies‚Äô in the weeks after birth,‚Äù Ivie said.<br><br>Social media users making the false claim cite a line in the bill stating that a person would not have criminal liability in the event of ‚Äúperinatal death,‚Äù a period of time following a birth.<br><br>The section of the bill states: ‚ÄúNotwithstanding any other law, a person shall not be subject to civil or criminal liability or penalty, or otherwise deprived of their rights, based on their actions or omissions with respect to their pregnancy or actual, potential, or alleged pregnancy outcome, including miscarriage, stillbirth, or abortion, or perinatal death.‚Äù<br><br>The bill does not establish a time frame around ‚Äúperinatal.‚Äù The Miami Standard article defines the period as ‚Äúup to seven days or more.‚Äù The outlet wrote in a response to the AP that, ‚ÄúPerinatal is defined by the Oxford Dictionary as at or around the time of birth. This could extend up to 28 days after the infant has been born.‚Äù The outlet included statements by several attorneys from pro-life organizations and firms arguing that the wording could decriminalize killing infants.<br><br>But the term ‚Äúperinatal death‚Äù in the bill is intended to mean the death of an infant caused by complications in pregnancy, according to Ivie. To clarify the term, Wicks added a new amendment to the bill on Monday to change the wording to, ‚Äúperinatal death due to a pregnancy-related cause.‚Äù<br><br>Even without the new amendment, the bill wouldn‚Äôt have allowed for ‚Äúinfanticide‚Äù or a murder of an infant days after it‚Äôs born, since homicide is illegal, according to Farah Diaz-Tello, senior counsel and legal director at If/When/How: Lawyering for Reproductive Justice.<br><br>Before the bill was introduced, a ‚Äúlegal alert‚Äù was issued earlier this year by California Attorney General Rob Bonta, which clarified criminal liability around the death of a fetus, saying prosecutors should not charge women with murder when a fetus dies, even if their behavior may have contributed to the death. He issued the advisory after San Joaquin Valley‚Äôs Kings County charged two women, Chelsea Becker and Adora Perez, with ‚Äúfetal murder,‚Äù alleging their drug use led to stillbirths. Becker‚Äôs case has been dismissed, and Perez‚Äôs conviction was cleared, although her case is ongoing.<br><br>Diaz-Tello says Wicks‚Äô bill could have ensured legal protections for these women from the outset.<br><br>Outside of the two cases, another example where the bill might apply would be a pregnant woman who exhibited signs of preterm labor, but could not afford to be on bed rest, Ivie said. While there could be a chance that the delivery results in a stillborn, the bill would ensure the woman couldn‚Äôt be prosecuted if that did occur, Ivie explained.<br><br>‚ÄúAnti-abortion activists are peddling an absurd and disingenuous argument that this bill is about killing newborns when ironically, the part of the bill they‚Äôre pointing to is about protecting and supporting parents experiencing the grief of pregnancy loss,‚Äù Wicks said in an email to the AP.<br><br>On Tuesday, the bill passed through the Assembly Judiciary committee with the new amendment and is moving to the health committee hearings.</i> |
| [I00227 False claim CDC reported spike in HIV-related diseases after COVID-19 vaccines | Fact check](../../generated_pages/incidents/I00227.md) | <i>A May 22 Instagram post (direct link, archive link) shows a screenshot of a tweet that claims a government agency reported a massive increase in certain illnesses after the COVID-19 vaccines were introduced.<br><br>"BREAKING: AIDS-associated diseases and cancers have increased by 338x since the rollout of the (COVID-19 vaccine), according to the CDC and foreign government bodies," reads the tweet.<br><br>The post was liked more than 1,000 times in two days. The original tweet has been retweeted more than 3,000 times.<br><br>[...]<br><br>The claim appears to have originated on a website called "The Expose," which has previously published vaccine-related misinformation. The headline of an article from May 11 reads, "COVID Vaccine roll-out caused 338x increase in AIDS-associated Diseases & Cancers in 2021 says CDC."<br><br>The CDC, though, says that claim isn't accurate (T0160.002: Information is False).<br><br>"To date, CDC has not detected any unusual or unexpected patterns for HIV or AIDS-associated diseases and cancers following immunization that would indicate that COVID vaccines are causing or contributing to these conditions," CDC spokesperson Scott Pauley told Lead Stories.<br><br>[...]<br><br>The article published by "The Expose" cites numbers from the Vaccines Adverse Event Reporting System, a government database that contains unverified reports of adverse effects possibly related to vaccines.<br><br>The database's website says anyone can submit a report, including the general public, and it has a disclaimer that states, ‚ÄúVAERS reports may contain information that is incomplete, inaccurate, coincidental or unverifiable. Reports to VAERS can also be biased. As a result, there are limitations on how the data can be used scientifically."</i> |
| [I00228 Rosie Holt: the satirist whose ‚ÄòTory MP‚Äô video had so many fooled](../../generated_pages/incidents/I00228.md) | <i>The video was, according to former Ukip leader Henry Bolton, evidence of the declining quality of MPs. Anthony Grayling, the philosopher, described her as a ‚Äúbald-faced emetic‚Äù and Philip Pullman, the author, said he was ‚Äúaghast‚Äù.<br><br>Their collective outrage was directed at the words of Rosie Holt who, asked by an interviewer whether she attended any of the Downing Street parties, said that until Sue Gray completes her report ‚Äúyour guess is as good as mine: I don‚Äôt know whether I attended the party‚Äù (T0068: Respond to Breaking News Event or Active Crisis).<br><br>Holt added: ‚ÄúIf there was a party in lockdown when we told everyone they couldn‚Äôt even attend funerals, but no one knew about it, was there a party?‚Äù<br><br>At a glance, Holt may be hard to distinguish from the declining number of Tory MPs prepared to stand up for the prime minister, but she is in fact a satirist (T0097.110: Party Official Persona, T0143.004: Parody Persona) ‚Äì an actor and comedian with a strong line in parodies of the political speech that veers into drivel. This video sketch (T0087: Develop Video-Based Content, T0160.005: Content Produced as Satire) has taken off ‚Äì 6 million views on Twitter so far ‚Äì partly because ‚Äúan awful lot of people‚Äù think it‚Äôs real, she said.<br><br>‚ÄúI don‚Äôt go in there to hoodwink people,‚Äù she told the Observer. ‚ÄúI get a bit unnerved when lots of people think it‚Äôs real because that‚Äôs not what I‚Äôm trying to do.<br><br>‚ÄúBut there‚Äôs also an awful lot of people who do get it. And I‚Äôm quite good at screening out the negative stuff. Some people say ‚Äòoh, you shouldn‚Äôt joke about things like this ‚Äì it‚Äôs a serious subject‚Äô.<br><br>‚ÄúBut I‚Äôm a big believer in laughing at things that make you sad and angry. And there‚Äôs so many things happening with this government at the moment there is always just so much material.‚Äù<br><br>This particular video was created by splicing Holt‚Äôs footage with questions from a Sky News reporter to Boris Johnson in which he dodged questions about whether he had gone to the 20 May 2020 garden party (T0162.002: Edits Made to News Report which Reframe Context).</i> |
| [I00229 No, Elon Musk isn‚Äôt investing $1 billion into an ‚Äòun-woke‚Äô film studio](../../generated_pages/incidents/I00229.md) | <i>Posts claiming Elon Musk has invested $1 billion or more ‚Äúinto Mel Gibson and Mark Wahlberg‚Äôs ‚Äòun-woke‚Äô film studio committed to traditional family values‚Äù are circulating on social media. But they aren‚Äôt true, and originated from a satirical website (T0160.005: Content Produced as Satire).<br><br>Several other fact checkers debunked similar claims earlier this year (T0160.006: Content Previously Fact Checked). Many of the posts use similar language to the headline in a satirical article, which said: ‚ÄúBreaking: Elon Musk Invests $1 Billion in Mel Gibson and Mark Wahlberg‚Äôs New Un-Woke Production Studio‚Äù.<br><br>The website this article appeared on describes itself as a ‚Äúone-stop destination for satirical news and commentary about the United States of America‚Äù which is ‚Äúdedicated to bringing you the latest and greatest in fake news and absurdity‚Äù. (T0097.202: News Outlet Persona, T0143.004: Parody Persona)<br><br>The article page itself is also labeled ‚Äúsatire‚Äù.</i> |
| [I00230 American Pickers' host Mike Wolfe is not in jail](../../generated_pages/incidents/I00230.md) | <i>In March and April 2025, videos surfaced on social media alleging ‚Äî without any evidence to corroborate the claim ‚Äî that Mike Wolfe, host of the History Channel TV show "American Pickers," had been arrested. The assertion was false (T0160.002: Information is False).<br><br>The rumor stemmed from videos uploaded by purported celebrity gossip channels on YouTube. The videos had titles like "Mike Wolfe Sentenced For Frank Frits's Death, Goodbye Forever," implying that Wolfe had been arrested (and charged, and sentenced) for the death of his former co-host, Frank Fritz. Fritz died on Sept. 30, 2024, according to an Instagram post Wolfe made the following day.<br><br>Artificial intelligence video-creation tools seemingly helped create the videos' narration, scripting, and sequencing (T0166: AI-Generated Content). Without naming an explicit source for its information, one video's narrator said:<br><br>‚ÄúMike Wolfe's sentencing stunned viewers across America when the "American Pickers" star was led away in handcuffs. Everyone thought his arrest was related to some dispute over antiques or perhaps a business disagreement with the History Channel. According to breaking news reports, Wolfe was arrested because the death of his former co-host, Frank Fritz, wasn't from natural causes as initially reported. Instead, investigators now classify it as premeditated murder. Medical reports indicate Fritz was found with lethal amounts of medication in his system. He was deliberately overdosed, and prosecutors grabbed Wolfe for the murder because it was calculated not accidental.‚Äù<br><br>That YouTube video had been viewed more than 272,000 times, as of this writing. Other YouTube videos sharing the same claim, or variations on it, also had large view counts. Posts on social media sites like Facebook also spread the supposed news.<br><br>There was no evidence to support the assertion that Wolfe had been arrested, and certainly no evidence he was arrested on charges of killing Fritz. If there was even a sliver of truth to the claim ‚Äî for instance, if Wolfe had been arrested on different charges ‚Äî reputable entertainment media outlets would have interviewed parties involved and documented the ordeal. That had not happened.<br><br>In short, the claim appeared to be made up from whole cloth for the purpose of gaining views online. A disclaimer at the bottom of the YouTube video read: <br><br>"Context of Information: The views and information shared in Niwra's videos are drawn from current news, reports, and personal insights. They are provided for educational and informational purposes only and may not always reflect the latest developments or offer a full perspective on the topics discussed."<br><br>Despite the fact that the video's underlying claim was false, the clip about Wolfe could seem believable because Fritz did die in September 2024, from the long-term effects of a stroke. Additionally, an unrelated Oregon man named Michael Wolfe was sentenced to life in prison in 2022 for murdering a woman and her child.<br><br>That small amount of truth inside a mountain of misleading information, combined with emotionally charged language, is a recipe that allows baseless celebrity rumors to go viral, and generate hundreds or thousands of comments from YouTube users. Some of those messages indicate that people interpret the videos to be real news.</i> |
| [I00231 Naga Munchetty: Scammers spread fake nude pictures of me on social media](../../generated_pages/incidents/I00231.md) | <i>"Naga Munchetti: This is the most humiliating day of my life. Yesterday's news shocked the whole of the UK."<br><br>The headline was enough to make me want to read more (T0167.001: Use of Clickbait)  ‚Äì but the fact they had spelled my name wrong made me immediately question the credibility of the journalism involved ‚Äì if there was any.<br><br>I'm used to seeing misleading articles about myself online, but the screenshots I've been sent by friends and followers on social media in recent weeks are a lot more insidious than most.<br><br>Paid-for advertisements (T0114: Deliver Ads) are popping up across X and Facebook, some including crudely mocked-up images of me naked ‚Äì my face badly photoshopped onto someone else's body (T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material).<br><br>I was both mortified and bemused, curious about who would pay good money to spread such obvious nonsense. And what was their motive? Is it something malicious? Someone with an axe to grind?<br><br>I discussed it with my 5 Live production team, and we began to dig into it more. It soon became apparent that my name and image were being used by scammers to try to hoodwink people out of money.<br><br>Clicking on the adverts took you through to a fake news article, complete with BBC logo and imagery (T0161.001: Impersonated Content, T0097.202: News Outlet Persona).</i> |
| [I00232 Celebrity death hoaxes alive and thriving on social media](../../generated_pages/incidents/I00232.md) | <i>Celebrity death hoaxes are spreading rapidly across social media in what experts suspect is an attempt to generate engagement for online advertising. <br><br>Dozens of pages on Facebook are making false claims that celebrities including action movie star Jason Statham and TV personality Simon Cowell have died in accidents over recent weeks.<br><br>The posts have generated tens of millions of views combined, but experts warn the fake stories are most likely engagement bait. <br><br>[An image in the article shows a screenshot of a post made to Facebook by a Page (T0151.001: Social Media Platform, T0151.003: Online Community Page) called ‚ÄúCelebrity Cars‚Äù. It has made a post with the following text: ‚ÄúSEE MORE: [url redacted] SAD NEWS: 30 minutes ago / The family announced the sad news of Action movie legend Jason Statham‚Ä¶‚Äù (T0167.001: Use of Clickbait). The post has an image which shows pictures of Jason Statham (including one of him in a gown on a hospital bed, presumably taken from an appearance in film or TV), and a picture of a coffin with the words ‚ÄúRest in peace‚Äù overlaid]<br><br>One post shared by a page called Celebrity Cars falsely claims Statham‚Äôs family released a public statement announcing ‚ÄúSAD NEWS‚Äù, accompanied by a composite image of the actor next to a separate photo of a coffin and the words ‚Äúrest in peace‚Äù (T0165: Edited Content).<br><br>The post links through to a website laden with advertising that includes several fake quotes attributed to Statham‚Äôs family (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>One of the quotes reads: ‚ÄúWith heavy hearts, we confirm the unfortunate news regarding Jason. We ask for privacy and respect during this difficult time.‚Äù<br><br>However, the statement, which can‚Äôt be found in any credible news reports, does not specifically claim Statham died (T0160.002: Information is False). <br><br>The article also includes an image of Statham next to a coffin and the words ‚Äúrest in peace‚Äù, continuing: ‚ÄúAs of now, there is no confirmed report on whether this news pertains to a personal tragedy, a serious health issue, or even the shocking possibility of Jason‚Äôs passing.‚Äù<br><br>There is no record of Statham‚Äôs family making the statement attributed to them, nor has there been any significant announcement regarding his health, death or career. <br><br>The same post and article on a different website have been shared by other pages on Facebook, including one called Buzz Network.<br><br>Statham was pictured, alive and apparently well, alongside Albanian-born football star Granit Xhaka in an image shared to Xhaka‚Äôs Instagram page and reported by the Albanian Post on July 8, 2025, the same day the Celebrity Cars post appeared. <br><br>Another death hoax that‚Äôs been shared widely recently targets British celebrity Simon Cowell, claiming [‚ÄúSAD NEW : 39 Minutes Ago In Chigaco, ‚ÄúAmerica‚Äôs Got Talent‚Äù Simon Cowell, He Has Been Confirmed‚Ä¶.‚Äù (T0167.001: Use of Clickbait)]. <br><br>While the Facebook post doesn‚Äôt specifically mention a death, Cowell‚Äôs photo is juxtaposed next to a coffin in a composite image that also depicts a crushed, burnt out vehicle (T0165: Edited Content).<br><br>A reverse image search of the damaged car reveals the photo was taken in Texas in 2023 after a head-on collision, as reported by NBC News (T0162.003: Historic Content Incorrectly Presented as Current).<br><br>Cowell‚Äôs personal Instagram account has been active in recent days, sharing clips from his TV show‚Äôs account, America‚Äôs Got Talent.</i> |
| [I00233 FACT CHECK: US President Donald Trump is alive](../../generated_pages/incidents/I00233.md) | <i>Claim: United States President Donald Trump has been confirmed dead after he was assassinated.<br><br>Rating: FALSE (T0160.002: Information is False)<br><br>Why we fact-checked this: The Facebook post bearing the false claim has garnered 1,300 shares, 118 reactions, and is circulating in several Facebook groups. <br><br>The caption states, ‚ÄúBreaking news!! Sad news just confirmed the passing of‚Ä¶ See more,‚Äù (T0167.001: Use of Clickbait) with nothing after. The person being referred to is implied to be Trump, as the post attaches photos of the US president bloodied and weak along with scenes of police containing the [ US protests ongoing as of the time of writing (T0068: Respond to Breaking News Event or Active Crisis)]. <br><br>In the comments, the post‚Äôs author includes a link and photos of Trump stumbling up a flight of stairs.<br><br>The facts: Trump is alive, and was last seen in person at the Kennedy Center for a ‚ÄòLes Miserables‚Äô performance on June 11. He also posted a video clip of the event on his Truth Social account a few hours after the viral post about his supposed death was published.<br><br>No other media reports have confirmed Trump‚Äôs supposed death or an assassination attempt occurring at the time the false posts were published.<br><br>The photos used in the false post come from past events and are taken out of context (T0162.003: Historic Content Incorrectly Presented as Current). Pictures showing Trump injured come from a failed attempt to assassinate him on July 13, 2024, while the scenes with police come from the Los Angeles protests against immigration raids. <br><br>Phishing link checker EasyDMARC also flagged the URL commented by the post‚Äôs author as suspicious. <br><br>Los Angeles protests: The false claim began circulating amid ongoing protests that erupted in downtown Los Angeles on June 6 over Trump‚Äôs immigration raids.</i> |
| [I00234 FACT CHECK: Justin Bieber is alive](../../generated_pages/incidents/I00234.md) | <i>Claim: Canadian pop singer Justin Bieber died in a car accident.<br><br>Why we fact-checked this: The Facebook post containing the claim was posted on September 6 and has 103 shares, 319 reactions, and 145 comments as of writing. Several posts circulating online contain the same claim too. <br><br>One of the three photos in the post shows a black and white image of Bieber with the text, ‚ÄúRIP‚Äù and ‚ÄúRest In Peace.‚Äù (T0165: Edited Content)<br><br>Other photos show a red car that crashed. <br><br>The facts: No news outlets have reported Bieber‚Äôs supposed death, and the Canadian singer can be seen in a September 11 post of American television personality Karlie Redd (T0160.002: Information is False). <br><br>Redd‚Äôs post was captioned ‚ÄúJustin Bieber Mommy & Daddy üë∂üèª üíñ‚Äù and shows Bieber and his wife, Hailey, carrying their newborn son. <br><br>Photo taken out of context: A reverse image search also revealed that the photo of the red car was from a 2022 car accident in Poland (T0162.003: Historic Content Incorrectly Presented as Current, T0162.004: Content Incorrectly Presented as Depicting Another Location). The driver of the Ferrari lost control of the car and hit the protective barriers while passing by the Wyrzysk bypass on S10 road on February 11, 2022.</i> |
| [I00235 FACT CHECK:  Marcos did not issue an order for Karen Davila to leave the country](../../generated_pages/incidents/I00235.md) | <i>Claim:  President Ferdinand Marcos Jr. ordered Karen Davila to leave the Philippines.<br><br>Rating: FALSE (T0160.002: Information is False)<br><br>Why we fact-checked this: As of writing, the video posted by YouTube channel PINAS NEWS INSIDER, has gained 24,000 views and 1,100 likes.<br><br>The bottom line: No statements were released on Marcos‚Äô official Facebook and Twitter accounts, Radio Television Malaca√±ang (RTVM), or the Office of the President indicating that he had issued an order for Davila to leave the country. Content on these platforms consisted mostly of netizens‚Äô comments urging Marcos to take action against Karen Davila.<br><br>These comments were triggered by a supposed statement by Davila, suggesting she would migrate to another country if Marcos won the presidential election (T0068: Respond to Breaking News Event or Active Crisis). Davila never made such a statement (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>During an episode of ANC‚Äôs Headstart, Senator Imee Marcos jokingly said that she thought Davila would leave if her family won: ‚ÄúIt‚Äôs nice to see you‚Äôre still here in the Philippines. Akala ko magma-migrate ka ‚Äòpag nanalo ang Marcos.‚Äù Davila responded by saying, ‚ÄúHoping always for the best for the country.‚Äú<br><br>Moreover, the video only provided clips of Marcos during an interview at the 125th anniversary celebration of the Philippine Navy last May 26, 2023. None of the questions had any relation to the claim (T0167.002: Title Misrepresents Content).</i> |
| [I00236 FACT CHECK: No Marcos order for Mel Tiangco to leave the country](../../generated_pages/incidents/I00236.md) | <i>A YouTube video falsely claims that Marcos issued the order after the veteran TV anchor supposedly insulted the President in relation to his State of the Nation Address<br><br>Claim:  President Ferdinand Marcos Jr. ordered news anchor Mel Tiangco to leave the Philippines.<br><br>Rating: FALSE (T0160.002: Information is False)<br><br>Why we fact-checked this: The claim was made in the title and thumbnail of a YouTube video posted on July 30, which has 251,000 views, 8,600 likes, and 2,359 comments as of writing. The channel that posted the video has 746,000 subscribers and has been fact-checked by Rappler multiple times.<br><br>The video‚Äôs title reads: ‚ÄúJUST IN: Mga MEDIA Natameme Grabe! ang NANGYARI kay MEL TIANGCO Nataranta sa UTOS ng PRES MARCOS!‚Äù<br><br>(Just in: The media was left speechless! Mel Tiangco was shocked by the order of President Marcos!) (T0167.001: Use of Clickbait)<br><br>The video thumbnail also shows pictures of Marcos and Tiangco with the text ‚ÄúPinalayas sa Pilipinas‚Äù (Expelled from the Philippines).<br><br>The bottom line: No official statements were released on Marcos‚Äô Facebook or Twitter accounts, on Radio Television Malaca√±ang, or the Office of the President regarding any supposed order for Tiangco to leave the country. Instead, the content of the video mainly showed comments from netizens urging Marcos to take action against the journalist for supposedly insulting him (T0167.002: Title Misrepresents Content).<br><br>These comments were triggered by a viral clip of Tiangco during her July 24 coverage of Marcos‚Äô State of the Nation Address (SONA) on the GMA news program 24 Oras, where she supposedly uttered the word ‚Äúpwe‚Äù which was interpreted as a sign of disrespect towards Marcos (T0068: Respond to Breaking News Event or Active Crisis).<br><br>Video from the GMA Integrated News YouTube channel does not show Tiangco making the exclamation. </i> |
| [I00237 Apple urged to axe AI feature after false headline](../../generated_pages/incidents/I00237.md) | <i>A major journalism body has urged Apple to scrap its new generative AI feature after it created a misleading headline about a high-profile killing in the United States.<br><br>The BBC made a complaint to the US tech giant after Apple Intelligence, which uses artificial intelligence (AI) to summarise and group together notifications, falsely created a headline about murder suspect Luigi Mangione.<br><br>The AI-powered summary falsely made it appear that BBC News had published an article claiming Mangione, the man accused of the murder of healthcare insurance CEO Brian Thompson in New York, had shot himself. He has not (T0166: AI-Generated Content, T0167.002: Title Misrepresents Content, T0160.002: Information is False, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0097.202: News Outlet Persona).<br><br>Now, the group Reporters Without Borders has called on Apple to remove the technology. Apple has made no comment.<br><br>Apple Intelligence was launched in the UK last week.<br><br>Reporters Without Borders, also known as RSF, said it was was "very concerned by the risks posed to media outlets" by AI tools.<br><br>The group said, external the BBC incident proves "generative AI services are still too immature to produce reliable information for the public".<br><br>Vincent Berthier, the head of RSF's technology and journalism desk, added: "AIs are probability machines, and facts can't be decided by a roll of the dice.<br><br>"RSF calls on Apple to act responsibly by removing this feature. The automated production of false information attributed to a media outlet is a blow to the outlet's credibility and a danger to the public's right to reliable information on current affairs."<br><br>[...]<br><br>The BBC does not appear to be the only news publisher which has had headlines misrepresented by Apple's new AI tech.<br><br>On 21 November, three articles from the New York Times were grouped together in one notification - with one part reading "Netanyahu arrested", referring to the Israeli prime minister.<br><br>It was inaccurately summarising a report about the International Criminal Court issuing an arrest warrant for Netanyahu, rather than any reporting about him being arrested (T0166: AI-Generated Content, T0167.002: Title Misrepresents Content, T0160.002: Information is False, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution, T0097.202: News Outlet Persona).<br><br>The mistake was highlighted on Bluesky, external by journalist Ken Schwencke with the US investigative journalism website ProPublica.<br><br>Mr Schwencke told BBC News that he took the screenshot and confirmed it was real. The New York Times has declined to comment.</i> |
| [I00238 Telegraph headline misrepresents estimate of foreign aid spent within UK](../../generated_pages/incidents/I00238.md) | <i>A Daily Telegraph article reporting on newly released figures for UK foreign aid spending is headlined ‚ÄúHalf of foreign aid spent in UK owing to asylum claims‚Äù.<br><br>The article also appears online under the headline ‚ÄúUK houses so many asylum seekers that over half the foreign aid budget is spent in Britain‚Äù.<br><br>These headlines are misleading (T0160.004: Information is Misleading). They‚Äôre based on a think tank estimate that the amount of UK foreign aid spent in the UK itself accounted for half of bilateral aid spending in 2023, rather than all foreign aid spending (T0164.003: Narrative Uses Misinterpreted Statistics to Support Claim). This distinction is made clearer later in the Telegraph article (T0167.002: Title Misrepresents Content). The amount of foreign aid spent in the UK would actually equate to around a third of the total.<br><br>Headline-writers should ensure headlines are accurate and support the body of the article. If headlines are inaccurate or don‚Äôt reflect the body of the article, it is possible that people will be misled. We‚Äôve contacted the Daily Telegraph for comment and will update this article if we receive a response.</i> |
| [I00239 ‚ÄúDeceptive‚Äù clickbait headlines: Relevance, intentions, and lies](../../generated_pages/incidents/I00239.md) | <i>2. Classic clickbaiting strategies<br><br>In Scott (2021: 54), clickbait was defined as ‚Äúonline content that is specifically designed to entice a reader to click on a link, but which offers very little reward for doing so‚Äù. (T0167.001: Use of Clickbait) When we click through and read a clickbait article, we are likely to feel disappointed. In relevance theory terms, the cognitive effects that we derive from reading the article are less than those we were promised, and we may feel that it has not been worth our while to do so.<br><br>The focus of much of the existing work on the pragmatics of clickbait has been on what Jod≈Çowiec (2022:7) calls ‚Äúclassic‚Äù clickbait headlines. These are sensationalist and hyperbolic headlines which often use a formulaic structure, vocabulary, and style. These headlines entice readers to click by creating an information gap (Blom and Hansen, 2015; Scott, 2021). They imply that some relevant information exists, and they indicate that the readers will gain access to that information if they click through and read the article. The headlines in (3) to (8) are typical examples of this and they illustrate some techniques that are commonly used to create an information gap.<br><br>(3) These gadgets will make you believe in the future of food<br><br>(4) Now THIS is why I voted for Barack Obama<br><br>(5) This goat has been bullying his Tiger Friend<br><br>(6) 12 Mind-blowing ways to eat Polenta<br><br>(7) Which TV female friend group do you belong to?<br><br>(8) How intuitive are you really?<br><br>The headlines in (3) to (5) use proximal demonstrative pronouns (‚Äúthese‚Äù, ‚Äúthis‚Äù) as referring expressions. The reader must click through to find out what ‚Äúthese gadgets‚Äù, ‚Äúthis goat‚Äù or ‚ÄúTHIS‚Äù refers to. Furthermore, in (4), the use of upper-case letters encourages an emphatic or contrastive interpretation of ‚ÄúTHIS‚Äù (Scott and Jackson, 2020; Scott, 2021). The reader will expect there to be something new, special, or particularly interesting about the reason the writer voted for Obama, but he must click through to find out what that unusual or special reason is. Headline (6) is a ‚Äúlistacle‚Äù (Temmerman and Vandenabeele, 2018) and uses a superlative (‚Äúmind-blowing‚Äù) to tempt the reader to click. The examples in (7) and (8) ask the readers a direct question, implying that if they click through, they will learn something relevant about themselves. Blom and Hansen (2015) discuss a range of other forward referring techniques and devices used in clickbait including imperatives, ellipsis, and implicit reference, and Chen et al. (2015) identify use of reverse narrative as a further technique for creating an information gap. In Scott (2021), it was demonstrated that there are a number of significant differences in the vocabulary used in these headlines when compared with non-clickbait headlines from traditional news publications. For example, there is a significant difference in the use of superlatives and intensifiers, and classic clickbaits also use forward-referring expressions such as demonstratives and personal pronouns more often than non-clickbait newspaper headlines.<br><br>Furthermore, headlines in traditional news media tend not to leave information gaps, but rather, they provide a summary of the associated story. As such, they function as autonomous texts (Dor, 2003; Ifantidou, 2009). Classic clickbait headlines are not autonomous in this way. They are dependent on the associated article, and the readers must click through if they are to form an understanding of the story or content of the article and fill the information gaps.<br><br>Some of the strategies used in clickbait (lists, questions, reverse narratives) and formulaic phrases (‚ÄúYou won't believe ‚Ä¶‚Äù, ‚Äú‚Ä¶ and what happened next is unbelievable!‚Äù) have become red flags alerting users to the presence of clickbait. Furthermore, software for clickbait detection and avoidance tends to, at least partly, depend on the use of ‚Äúclickbait language‚Äù to identify likely clickbait (Chen et al., 2015; Chakraborty et al., 2016; Potthast et al., 2016, 2018). As users and software become more effective at recognising classic clickbait techniques, new strategies have emerged or become favoured. As Jod≈Çowiec (2022: 14) puts it, authors of clickbait headlines are ‚Äútrying hard to camouflage deception‚Äù. One such strategy that has emerged is the use of what Jod≈Çowiec (2022) calls ‚Äúdeceptive clickbaits‚Äù (T0167.002: Title Misrepresents Content). In the next section, I outline the properties of deceptive clickbaits, following and building on Jod≈Çowiec's analysis.<br><br>3. Deceptive clickbaits<br><br>On the surface, deceptive clickbaits appear much more like traditional headlines than the classic clickbaits that we saw in the previous section. Some examples of deceptive clickbaits are given in (9) to (11).<br><br>(9) The Polish national team coach dies (Popularne, discussed in Jod≈Çowiec 2022:8)<br><br>(10) Alex Scott worries about sharing ‚Äútoo much information‚Äù in very candid health update (Beale, 2022)<br><br>(11) Chaos for Cheryl's West End debut as star pulls out and is replaced by Eastenders Legend (Domachowski, 2023)<br><br>The headline in (9) is an example discussed by Jod≈Çowiec (2022), and the examples in (10) and (11) were collected from the Google Discover news feed and appeared in UK online tabloid newspapers. Each of these headlines appears to provide a summary of the accompanying story in a way that parallels traditional news headlines. These headlines function as autonomous texts, and we can form a hypothesis about the intended meaning and hence the content of the story, based on the headline alone.<br><br>Applying the relevance-theoretic interpretative processes (outlined in Section 1) to the deceptive clickbait headlines allows us to predict likely interpretations. To interpret the headline in (9) the reader must assign reference to the definite description ‚ÄúThe Polish national team coach‚Äù. This headline appeared on a Polish news website and, as Jod≈Çowiec points out, the most accessible interpretation is quite clearly the coach of the current Polish national men's soccer team. Such an interpretation would lead to cognitive effects for the Polish target audience, and the death of such a high-profile individual would be highly relevant to many readers. The audience is therefore likely to settle on this as the intended interpretation of the referring expression.<br><br>At this point, the headline appears to be functioning much like a traditional news headline. In Dor's (2003: 703) relevance-theoretic analysis of headlines he characterises them as optimising ‚Äúthe relevance of the story by minimizing processing effort while making sure that a sufficient amount of contextual effects are deducible within the most appropriate context possible‚Äù. That seems to be the case here. If the coach of the Polish national soccer team has died, then the headline in (9) appears to be an optimally relevant way to communicate this information to a national audience.<br><br>Furthermore, this interpretation of the headline in (9) aligns with expectations that readers might have about the newsworthiness of stories in national news media. The optimally relevant interpretation of this headline has high news value in the sense outlined by Bednarek and Caple (2018). Stories with high news value tend to be negative, with impactful consequences, and significance from a human-interest perspective. They also tend to feature prominent individuals and they describe events which are both recent and geographically/culturally close to the experiences of the intended readership. The optimally relevant interpretation of the headline in (9) fulfils all these criteria and leads us to infer that the article contains more details about what would be a highly newsworthy story.<br><br>The classic clickbait headlines that we saw in Section 2 leave us with a gap that needs to be filled. We are unable to fully interpret the headline unless we fill that gap, and it is this that arouses our curiosity. With deceptive clickbaits, however, our curiosity is aroused precisely because our expectations of relevance lead us to a highly relevant, highly newsworthy interpretation and we want to know more. Why then might we consider these headlines deceptive? And why might they fall under the definition of clickbait?<br><br>While the way in which they arouse curiosity is different, like classic clickbait, the ‚Äúdeceptive‚Äù clickbait headlines are ultimately disappointing. As Jod≈Çowiec discusses, when users click through, they find that their interpretation of the headline does not align with the content of the story in the full article. For example, the ‚ÄúPolish national coach‚Äù referred to in (9) is not the national men's soccer team coach. On reading the article, we discover that the story is about the much less well-known coach of the Polish junior volleyball team. The news value of this story is much lower. The individual concerned is less well known and while still a negative story with human-interest, far fewer cognitive effects will follow from this news than from the story that the reader was expecting.<br><br>In example (10) we have even more contrast between the optimally relevant interpretations of the headlines and the content of the stories that they accompany. The wording of the headline in (10) leads us to expect that the former England women's footballer Alex Scott has been given a worrying update about her health. This interpretation has high news value. It is negative and unexpected, and it is likely to lead to significant consequences. It would be a highly relevant story from which many cognitive effects would follow for the average readers of the publication in which it appears. However, when the readers click through the headline in (10), they find a report about an Instagram video posted by Scott. In the 16 s video she says ‚ÄúGuys, this may be a bit too much ‚Ä¶ TMI ‚Ä¶ but [laughs] ‚Ä¶ I am off for a colonic. My first colonic of 2022 [laughs]‚Äù. While this is strictly speaking a health update, it is a far more mundane and less newsworthy story than the readers have been led to believe.<br><br>Finally, the headline in (11) appeared after the first performances of a play in London's West End featuring the high-profile singer and celebrity Cheryl. To interpret the headline, the reader must infer who the ‚Äústar‚Äù is who has pulled out and been replaced. Given the fact that she has just been mentioned and that her appearance in this production has been long anticipated, the most accessible and relevant referent for the referring expression ‚Äústar‚Äù is Cheryl. This interpretation would justify the use of the word ‚Äúchaos‚Äù to describe the opening night of the play, and it would represent a highly newsworthy story. If Cheryl had pulled out of the production many ticketholders would be left disappointed. However, when reading the full article, readers discover that it is in fact Cheryl's much less famous co-star Hugh Chegwin who has had to withdraw from the production due to illness.<br><br>Notice that in each of these cases, the actual stories are not entirely lacking in newsworthiness. Jod≈Çowiec's example in (9) deals with a sad event with human interest. The story behind the headline in (10) is taken from a celebrity social media post. Presumably, the followers of this account would find this information interesting, and the fact that it is about a prominent figure gives it at least some news value. Finally, the news story behind the update in (11) may well be of interest to theatregoers and ticketholders. However, the success of online news headlines depends on large numbers of users clicking on the links to access the pages. To optimise revenue, the headlines must appeal to as wide an audience as possible. Online headlines are competing for the user's attention with a wide array of other inputs. While the actual stories behind headlines like (10) may be relevant enough to attract the attention of a fan as they scroll through the Instagram feed of a favourite celebrity, they are unlikely to have the same effect when competing against high news value stories and other content such as advertisements. Similarly, while the headline in (11) may be relevant to those who are planning to see the play, the high news value ‚Äúdeceptive‚Äù interpretation has a much wider potential audience, comprising of anyone with any interest in Cheryl's activities or career.<br><br>The headlines in (10) and (11) appeared on the Google discovery news feed. They are designed to attract users‚Äô attention when Google is opened on mobile devices. Presumably, the user has opened Google for a specific purpose such as running a search or checking for updates. To be successful, the headlines must seem more relevant than whatever the user was planning to do. According to relevance theory, our ‚Äúattention tends automatically to go to what is most relevant at the time‚Äù (Wilson and Sperber, 2012: 6) and ‚Äú[w]hat makes an input worth picking out from the mass of competing stimuli is not just that it is relevant, but that it is MORE relevant than any alternative input available to us at that time‚Äù (Wilson and Sperber, 2004: 609). Therefore, to attract the attention of a wide audience and to arouse curiosity in the members of that audience, the headlines need to present the associated article as likely to yield a wealth of cognitive effects for as many people as possible.<br><br>Of course, it is a function of all headlines to attract attention and to arouse interest (Iarovici and Amel, 1989; Ifantidou, 2009; Jod≈Çowiec, 2022), and, as Ifantidou (2009: 700) discusses, they ‚Äúconsistently underrepresent, or overrepresent, and hence misrepresent‚Äù the news stories with which they are associated. For example, a study by Andrew (2007: 24) established that in headlines about politics there is ‚Äúa considerable difference between articles and their headlines in terms of emphasis and issue salience‚Äù. Deceptive clickbait headlines, however, do not simply provide a different emphasis or focal point to the main article. They encourage the reader to retrieve an interpretation which is significantly at odds with the story described in the article. In her discussions, Jod≈Çowiec (2022) provides a relevance-theoretic analysis of how such deceptive clickbait headlines do so.<br><br>As Jod≈Çowiec (2022: 10) explains, with deceptive clickbait headlines, following the relevance-theoretic comprehension procedure leads readers to an interpretation that is ‚Äúpotentially rich in cognitive effects‚Äù. This is the interpretation with high news value. This interpretation turns out to be ‚Äúincongruent with the content of the text‚Äù (p. 6) and the reader is left disappointed. Jod≈Çowiec likens these headlines to jokes or puns where the hearer is led down one inferential path only to realise that another, often absurd interpretation is intended. Consider, for example, the joke in (12), where encyclopaedic information about goldfish (i.e., that they sometimes live in glass water tanks) makes one meaning of the word ‚Äútank‚Äù highly accessible. The punchline forces the hearer to reassess this disambiguation, as they realise that a different meaning is intended.<br><br>(12)	Two goldfish in a tank. One says to the other, ‚ÄúHow do you drive this thing?‚Äù<br><br>As Jod≈Çowiec points out, while the mechanism may be similar, there are no humorous effects or ‚Äúintellectual satisfaction associated with getting the punchline‚Äù (p. 15) for the victims of deceptive clickbait. There is only disappointment that their expectations have not been met and a feeling that their time and effort has been wasted or that they have been tricked or deceived.<br><br>According to Jod≈Çowiec, however, these deceptive clickbait headlines are not lies. She follows Dynel's (Dynel, 2018) characterisation of what it means to lie, and argues that ‚ÄúUnlike lies, which are intended to produce a false belief in the target (Dynel, 2018:16), deceptive clickbaits merely misrepresent the content of the article by projecting a certain meaning which is later discovered to be unintended‚Äù (Jod≈Çowiec, 2022:7). Furthermore, she suggests that ‚Äúresponsibility for recovering a certain meaning, though cleverly ‚Äòengineered‚Äô by the website editor, rests with the reader‚Äù (p. 15). In the rest of this paper, I will show that as least some deceptive clickbait headlines fall within existing definitions of what it means to be a lie. They mislead the reader at the explicit level, and they are intended to produce a false belief in the reader. Indeed, it is this false belief that drives the reader to click on the headline link. Furthermore, if we assume the relevance-theoretic pragmatic framework of utterance interpretation, responsibility for the interpretation sits not with the reader, as Jod≈Çowiec suggests, but firmly with the writers. This issue of where responsibility lies has consequences in terms of journalistic ethics and practices. For example, the United Kingdom independent regulator for newspapers and magazines (IPSO) Code of Practice specifies that, ‚ÄúThe Press must take care not to publish inaccurate, misleading, or distorted information or images, including headlines not supported by the text‚Äù (IPSO, 2021). If responsibility for the interpretation of clickbait headlines is placed with the readers, then journalists have a clearer defence against accusations that they have breached the code by publishing ‚Äúmisleading‚Äù or ‚Äúinaccurate‚Äù information. If we can show, however, that the deception is intentional, any defence is likely to be much less convincing.</i> |
| [I00240 FALSE: Bela Padilla dies after being gunned down in car](../../generated_pages/incidents/I00240.md) | <i>Claim: Actress Bela Padilla was declared dead on arrival after she was gunned down in her car.<br><br>Multiple links, most of them as different pages of gmanewsupsimelda.blogspot.com, contained variations of this claim.<br><br>Once clicked, they redirect (T0149.004: Redirecting Domain Asset) to randomnames.club. There, it contains a headline on Padilla‚Äôs death. It also contains a ‚ÄúTrending News‚Äù name and logo, yet some of the links cited GMA News in the headline (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). Some also misspelled Padilla‚Äôs name.<br><br>The pages also embedded a video with the 24 Oras logo in the thumbnail (T0161.001: Impersonated Content, T0097.202: News Outlet Persona). Once clicked, it plays for a few seconds before prompting the viewer to share the video before continuing to watch it.<br><br>These links were flagged by Facebook Claim Check, the platform‚Äôs tool for monitoring potentially false content.<br><br>Rating: FALSE<br><br>The facts: Padilla is alive and has been active on social media platforms. The link to the embedded video on the websites leads to a GMA News report on a shooting incident of a different woman (T0167.002: Title Misrepresents Content).<br><br>GMA reported that the woman who was shot in her car was identified as a certain Consuelo Go (T0162: Reframe Context). The video was uploaded on January 26, 2019 (T0162.003: Historic Content Incorrectly Presented as Current).<br><br>The claim was being circulated as early as July 4, but Padilla has been active on her verified accounts since then. She posted on Instagram on July 6 and was retweeting posts on Twitter on July 7.<br><br>Moreover, no news organizations reported Padilla‚Äôs death.<br><br>Social media monitoring tool CrowdTangle also revealed that the randomnames.club link has been shared since March, but with a different headline which said that P20,000 will be provided to all SSS members due to COVID-19. </i> |
| [I00241 FACT CHECK: Ted Failon is alive](../../generated_pages/incidents/I00241.md) | <i>Fake news about the journalist‚Äôs death circulates following his remarks on the administration‚Äôs ‚ÄòBagong Pilipinas‚Äô movement in a radio show episode<br><br>Claim: News and radio anchor Ted Failon has passed away.<br><br>Rating: FALSE<br><br>Why we fact-checked this: The claim was made in the thumbnail and title of a June 19 video uploaded by the YouTube channel ‚ÄúBANAT NEWS UPDATES.‚Äù It was also re-uploaded by the channel ‚ÄúPINAS NEWS REVIEW‚Äù on June 20. <br><br>One version of the video‚Äôs thumbnails showed a black-and-white image of Failon, suggesting he had died. It also included photos of President Ferdinand Marcos Jr., Vice President Sara Duterte, and former president Rodrigo Duterte, all appearing to be in mourning. The text on the thumbnail reads: ‚ÄúMay ibinunyag. Jusko po! Pumanaw na. VP Sara, Ex-PRRD, di makapaniwala. Grabe ito ang dapat na malaman ng lahat!‚Äù <br><br>(There‚Äôs a revelation. Oh my God! He passed away. VP Sara, [ex-president Rodrigo Roa Duterte], can‚Äôt believe it. This is something everyone should know!) (T0167.001: Use of Clickbait, T0160.002: Information is False)<br><br>As of writing, the two videos have over 51,000 views, 200 comments, and 1,600 likes combined.  <br><br>The facts: Failon is alive. He appeared on the live broadcast of the ‚ÄúTed Failon at DJ Cha Cha sa Radyo Singko‚Äù program on Wednesday, June 26.  <br><br>There are also no official news reports from TV5 or other credible sources confirming the claim.<br><br>The title and thumbnail of the misleading YouTube videos were clickbait, merely mentioning Failon‚Äôs remarks about the Marcos administration‚Äôs brand of governance and leadership it calls ‚ÄúBagong Pilipinas (A New Philippines).‚Äù (T0167.002: Title Misrepresents Content)<br><br>In June, Malaca√±ang ordered government agencies and schools to recite the Bagong Pilipinas hymn and pledge to instill the principles of the movement.<br><br>Remarks on Bagong Pilipinas: The false claim targeting Failon was uploaded after he recited ‚ÄúPanata Laban sa mga Nambubudol sa Pilipino‚Äù (A Pledge Against those Fooling Filipinos) during the June 10 episode of his radio show with DJ Cha Cha (T0068: Respond to Breaking News Event or Active Crisis). Part of it reads: ‚ÄúBilang Pilipino, magiging  mapagbantay ako sa ginagawang pagnanakaw ng mga taong gobyerno sa Bagong Pilipinas‚Ä¶. Bilang Pilipino, iaaalay ko ang aking lakas, talino, at kakayahan sa pagmumulat sa kaisipan ng mga dukhang pinagmamalabisan at nililinlang ng mga taong nasa likod umano ng sinasabing Bagong Pilipinas.‚Äù <br><br>(As a Filipino, I will remain vigilant against the corruption committed by government officials in the New Philippines‚Ä¶. As a Filipino, I will dedicate my strength, intellect, and abilities to enlighten the minds of the impoverished who are being oppressed and deceived by those behind the so-called New Philippines.)<br><br>In January, during an episode of ‚ÄúThink About It by Ted Failon‚Äù titled ‚ÄúBagong Pilipinas, Daw?‚Äù the journalist also pointed out some ironies in Marcos‚Äô speech at the Bagong Pilipinas kick-off rally. Failon said: ‚ÄúWalang magiging pagbabago sa Pilipinas kung ang mga pinuno nito ay puro salita at kapos sa gawa. Walang bagong Pilipinas hanggang naghahari ang mga opisyal ng gobyerno na sobrang takaw sa pera at labis-labis ang kasakiman sa kapangyarihan.‚Äù <br><br>(There will be no change in the Philippines if its leaders are all talk and lack action. There can be no new Philippines as long as government officials are excessively greedy for money and insatiably hungry for power.)<br><br>Online disinformation against journalists: Failon was previously targeted by false claims alleging his expulsion from the Philippines following his critical remarks on presidential candidates‚Äô disclosure of their Statements of Assets, Liabilities, and Net Worth.<br><br>Aside from Failon, journalists like GMA newscaster Mel Tiangco, GMA host Kim Atienza, and ABS-CBN‚Äôs Karen Davila, have previously been subjected to this type of online disinformation.</i> |
| [I00242 Billy Dee Williams Says His Facebook Page Got Hacked and Began Posting Charlie Kirk Disinformation. Millions of People Were Fooled](../../generated_pages/incidents/I00242.md) | <i>Williams, best known to audiences for his iconic role as Star Wars‚Äô Lando Calrissian, announced Sunday that he had been hacked and that a number of recent politically-charged posts on his page were not his own (T0146: Account Asset, T0150.005: Compromised Asset, T0145.005: Compromised Persona).<br><br>‚ÄúMy friends, I want to let you know that my account was recently hacked,‚Äù Williams wrote on Facebook Sunday. ‚ÄúSeveral unauthorized posts were not legitimate.‚Äù<br><br>Facebook post data compiled by PressProgress shows that over a three-day period between September 10 and September 13, Williams‚Äô official verified Facebook page made 66 posts in reference to Charlie Kirk. These posts were collectively shared over 70,000 times and received over 1.6 million likes and other reactions.<br><br>Williams had been an infrequent social media user until September 10, when the 88-year-old actor‚Äôs Facebook page suddenly posted a ‚ÄúMay the Fourth be with you‚Äù message to confused fans, prompting one to reply: ‚ÄúIt‚Äôs September, Billy.‚Äù<br><br>For three days, the Star Wars actor‚Äôs page became obsessively fixated on Charlie Kirk‚Äôs murder (T0068: Respond to Breaking News Event or Active Crisis), posting dozens of times per day about Kirk, including posts equating the slain activist to Abraham Lincoln, JFK, Martin Luther King Jr. and Jesus Christ. Another post predicted retribution against ‚Äúleftists.‚Äù<br><br>[...]<br><br>In a statement to PressProgress on Saturday afternoon, Billy Dee Williams‚Äô management team confirmed that the Star Wars actor‚Äôs Facebook page had been compromised and that they were having difficulty regaining control.<br><br>‚ÄúUnfortunately Mr. Williams‚Äô official Facebook account was hacked,‚Äù the actor‚Äôs representative told PressProgress. ‚ÄúPlease note that all of the recent posts you‚Äôve seen are fraudulent and in no way reflect Mr. Williams‚Äô views, opinions or affiliations.‚Äù (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution)<br><br>[...]<br><br>One post, shared while police were still hunting for the suspect, provided the wrong name of the alleged shooter (T0068: Respond to Breaking News Event or Active Crisis, T0160.002: Information is False). Another post shared a fake screenshot of a non-existent tweet from Rolling Stones singer Mick Jagger lamenting the death of Charlie Kirk and predicting the return of Jesus Christ (T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).</i> |
| [I00243 Czech prime minister says his X account was hacked ‚Äòfrom abroad‚Äô](../../generated_pages/incidents/I00243.md) | <i>The Czech prime minister‚Äôs account on the social media platform X was hacked on Tuesday, with a series of false messages posted, including those claiming a Russian attack on Czech soldiers and a response to U.S. tariffs (T0146: Account Asset, T0097.111: Government Official Persona,  T0150.005: Compromised Asset, T0145.005: Compromised Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution). <br><br>Petr Fiala confirmed the incident, saying the hack originated ‚Äúfrom abroad.‚Äù<br><br>‚ÄúDespite thorough security measures, including two-factor authentication ‚Ä¶ the attackers managed to penetrate the profiles and publish fake posts,‚Äù he said. ‚ÄúWe are actively cooperating with the police to investigate this incident and identify the perpetrators.‚Äù<br><br>One of the posts alleged that Russian forces had attacked Czech military units near the border of Kaliningrad, a heavily militarized Russian exclave between Poland and Lithuania. The post was later deleted.<br><br>"The post about the attack on Czech soldiers is not true," government spokesperson Lucie Michut Jesatkova told local media.<br><br>The prime minister‚Äôs X account, which has more than 366,000 followers, also briefly displayed a post about preparing sanctions in response to U.S. tariffs. According to local reports, both fake posts included a live video link, which turned out to be a static image with no audio.<br><br>Similar fake content appeared on the X account of Spolu, a center-right political alliance that includes Fiala‚Äôs Civic Democratic Party (ODS) (T0146: Account Asset, T0097.111: Government Official Persona,  T0150.005: Compromised Asset, T0145.005: Compromised Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>‚ÄúWe reacted immediately and are working with experts to analyze the attack. High security of our channels is a priority for us,‚Äù said ODS spokesperson Jakub Skyva.<br><br>Petr Letocha, a member of the Czech parliament‚Äôs security committee, told local media that the breach may have involved someone with administrative access to both accounts.<br><br>‚ÄúGiven that the [prime minister's] account is fully under control again, I believe the attacker no longer has access to it,‚Äù he said, adding that two-factor authentication makes such breaches difficult and the investigation may clarify how it was bypassed.<br><br>Czech police confirmed they are investigating the breach to determine how it occurred and whether the false information posted could constitute a criminal offense.</i> |
| [I00244 Hackers publish fake story about Ukrainians attempting to assassinate Slovak president](../../generated_pages/incidents/I00244.md) | <i>An unidentified attacker hacked a Czech news service's website and published a fake story on Tuesday claiming that an assassination attempt had been made against the newly elected Slovak president, Peter Pellegrini (T0152.004: Website Asset, T0150.005: Compromised Asset, T0097.202: News Outlet Persona, T0145.005: Compromised Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>According to the government-owned public service Czech News Agency (CTK), the attacker posted the false article directly to its website, meaning the story was not distributed to the service‚Äôs clients.<br><br>The article has since been retracted, with CTK declaring it to be a fake and announcing that it had informed the country‚Äôs intelligence agencies and cybersecurity authority about the breach.<br><br>The headline of the fake story claimed that Slovakia‚Äôs domestic intelligence agency, the Security Information Service (BIS), ‚Äúprevented an assassination attempt on the newly elected Slovak President Petr Pelligrini.‚Äù<br><br>Readers noted that the story misspelled Peter Pellegrini‚Äôs name.<br><br>Pellegrini was elected earlier this month (T0068: Respond to Breaking News Event or Active Crisis), providing what Reuters reported was a boost to Slovakia‚Äôs pro-Russian prime minister Robert Fico. Despite Slovakia‚Äôs NATO membership, Pellegrini has said he would oppose sending the country‚Äôs armed forces to assist a member state if it were attacked by Russia.<br><br>The false story published on Tuesday in both Czech and English said that the fictitious attempted assassination of Pellegrini was planned by Ukrainian nationals. It named Vitaliy Usatyy, Kyiv‚Äôs charge d‚Äôaffaires in Prague, as one of the perpetrators.<br><br>CTK described the incident as an act of disinformation. No evidence has yet been published tying the hack to a particular actor, and the news agency said it would not be releasing further information.</i> |
| [I00245 Fake videos, AI deepfakes and hacked accounts: No truce in India-Pakistan disinformation war](../../generated_pages/incidents/I00245.md) | <i>NEW DELHI, May 13 ‚Äî In Islamabad India and Pakistan have announced a ceasefire after coming close to all-out conflict, but on social media citizens on both sides are vying to control public perceptions by peddling disinformation.<br><br>[...]<br><br>After the first round of Indian air strikes, the Pakistani military shared footage that had previously circulated in reports about a 2023 Israeli air strike in Gaza (T0162.003: Historic Content Incorrectly Presented as Current, T0162.004: Content Incorrectly Presented as Depicting Another Location).<br><br>The clip quickly appeared on television (T0117: Attract Traditional Media) and social media but was later retracted by numerous media outlets, including AFP.<br><br>AI-generated imagery has also muddied the waters (T0166: AI-Generated Content), including a video that purportedly shows a Pakistan Army general saying the country lost two of its aircraft (T0166.001: Deepfake Impersonation, T0097.111: Government Official Persona, T0161.002: Statement Incorrectly Presented as Made by Individual or Institution).<br><br>AFP fact-checkers found the clip was altered from a 2024 press conference (T0162.008: Context Reframed by Edits to Media).<br><br>‚ÄúWe have seen a new wave of AI-based content in both video and still images due to increased access to deepfake tools,‚Äù said Joyojeet Pal, an associate professor in the School of Information at the University of Michigan.<br><br>Cyber alert, social media crackdown<br><br>Both India and Pakistan have taken advantage of the information vacuum to raise alarm bells and promote their own claims and counter-claims.<br><br>Pakistan appears to have lifted a more than one-year-old ban on X the same day of the Indian strikes, according to an AFP analysis of data from the nonprofit Open Observatory of Network Interference.<br><br>‚ÄúIn a time of crisis, the government needed its people‚Äôs voice to be heard all around the world and not to be silenced anymore like it was before for domestic political purposes,‚Äù said Usama Khilji, a digital rights expert and activist in Pakistan.<br><br>The country‚Äôs National Cyber Emergency Response Team (NCERT) on May 8 issued an alert about ‚Äúincreased cyberattacks and misinformation via emails, social media, QR codes, and messaging apps‚Äù.<br><br>Both Pakistan‚Äôs Ministry of Economic Affairs and the Karachi Port Trust later said their X accounts had been hacked.<br><br>A post from the latter account said the port ‚Äî one of South Asia‚Äôs busiest ‚Äî was attacked by the Indian military (T0146: Account Asset, T0097.111: Government Official Persona, T0150.005: Compromised Asset, T0145.005: Compromised Persona).<br><br>The page was later restored and the port authority said no attack had taken place.<br><br>India, meanwhile, has executed a sweeping crackdown targeting the social media accounts of Pakistani politicians, celebrities and media organisations (T0124.001: Report Non-Violative Opposing Content).<br><br>The government ordered X to block more than 8,000 accounts and banned more than a dozen Pakistani YouTube channels for allegedly spreading ‚Äúprovocative‚Äù content, including news outlets.</i> |
| [I00246 I was deepfaked by my best friend'](../../generated_pages/incidents/I00246.md) | <i>In the spring of 2021, Jodie (not her real name) was sent a link to a porn website from an anonymous email account.<br><br>Clicking through, she found explicit images (T0086: Develop Image-Based Content, T0166.002: Sexual Deepfake Impersonation) and a video (T0087: Develop Video-Based Content, T0166.002: Sexual Deepfake Impersonation) of what appeared to be her having sex with various men. Jodie's face had been digitally added onto another woman's body - known as a "deepfake".‚ÄØ<br><br>Someone had posted photos of Jodie's face on a porn site saying she made them feel "so horny" and asking if other users on the site could make fake pornography of her. In exchange for the fakes, the user offered to share more photos of Jodie and details about her (T0048.004: Dox).<br><br>Speaking for the first time about her experience, Jodie, who is now in her mid-20s, says, "I was screaming and crying and violently scrolling through my phone to work out what I was reading and what I was looking at."<br><br>She adds: "I knew that this could genuinely ruin my life."<br><br>Forcing herself to scroll through the porn site, Jodie said she felt her "whole world fall away".<br><br>[...]<br><br>It was not the first time Jodie had been targeted.<br><br>In fact, it was the culmination of years of anonymous online abuse.<br><br>When Jodie was a teenager, she discovered that her name and photos were being used on dating apps without her consent (T0146: Account Asset, T0151.017: Dating Platform, T0143.003: Impersonated Persona).<br><br>This went on for years and she even received a Facebook message from a stranger in 2019 who said he was due to meet her at Liverpool Street station in London for a date.‚ÄØ<br><br>She told the man that it wasn't her who he had been speaking to. She says she felt "unnerved" because he knew all about who she was and had managed to find her online. He'd found her on Facebook after the "Jodie" on the dating app had stopped responding.‚ÄØ<br><br>In May 2020, during the UK's lockdown, Jodie was also alerted by a friend to a number of Twitter accounts that were posting pictures of her, with captions implying she was a sex worker.‚ÄØ<br><br>"What would you like to do with little teen Jodie?" read one caption next to an image of Jodie in a bikini, which had been taken from her private social media account.<br><br>The Twitter handles posting these images had names like "slut exposer," and "chief perv."<br><br>All of the images being used were ones she'd been happy to share on her social media with close friends and family - but no one else.<br><br>Then she found that these accounts were also posting images of other women she knew from university, as well as from her hometown of Cambridge.<br><br>"In that moment, I feel a very strong sense [that] I'm at the centre of this and this person is looking to hurt me," she said.</i> |
| [I00247 ‚ÄòIt was extremely pornographic‚Äô: Cara Hunter on the deepfake video that nearly ended her political career](../../generated_pages/incidents/I00247.md) | <i>When Cara Hunter, the Stormont politician, looks back on the moment she found out she had been deepfaked, she says it is ‚Äúlike watching a horror movie‚Äù. The setting is her grandmother‚Äôs rural home in the west of Tyrone on her 90th birthday, April 2022. ‚ÄúEveryone was there,‚Äù she says. ‚ÄúI was sitting with all my closest family members and family friends when I got a notification through Facebook Messenger.‚Äù It was from a stranger. ‚ÄúIs that you in the video ‚Ä¶ the one going round on WhatsApp?‚Äù he asked.<br><br>Hunter made videos all the time, especially then, less than three weeks before elections for the Northern Ireland assembly. She was defending her East Londonderry seat, campaigning, canvassing, debating. Yet, as a woman, this message from a man she didn‚Äôt know was enough to put her on alert. ‚ÄúI replied that I wasn‚Äôt sure which video he was talking about,‚Äù Hunter says. ‚ÄúSo he asked, did I want to see it?‚Äù Then he sent it over.<br><br>‚ÄúIt was extremely pornographic,‚Äù she says. ‚ÄúI won‚Äôt go into detail but I want you to understand what I had to compute. Even as I‚Äôm sitting here talking about it now, I suddenly feel roasting hot. It‚Äôs a clip of a blue-walled bedroom, and it has American plugs. There‚Äôs this woman ‚Äì a woman who seemed to have my face ‚Äì who is doing a handstand and having mutual oral sex with a man. And I‚Äôm looking at this, sitting surrounded by family, in the middle of a very heated election campaign.‚Äù (T0087: Develop Video-Based Content, T0166.002: Sexual Deepfake Impersonation, T0097.110: Party Official Persona, T0068: Respond to Breaking News Event or Active Crisis) At the same time, Hunter‚Äôs phone was blowing up with message after message from strangers who had seen the video. ‚ÄúAll of them were just really vitriolic,‚Äù she says. ‚ÄúThose messages were from people who hate women.‚Äù (T0048: Harass)<br><br>It‚Äôs hard to fathom how unknown and ‚Äúniche‚Äù deepfake pornography still was when this happened, only three years ago. ‚ÄúThe only ‚Äòaltered images‚Äô I really knew about at that time were Snapchat filters,‚Äù says Hunter. ‚ÄúMy initial reaction was: ‚ÄòIs this a woman who looks similar to me?‚Äô Then a friend asked if this could be one of those things where they put your face on to someone else‚Äôs body. We were Googling it, trying to see what it was called.‚Äù Since that time, this tech has come a frighteningly long way. ‚ÄúNow I have girls calling me, telling me this has happened to them and ruined their lives. Just recently, one young woman told me it had happened to her and 14 others, all when they were under 18,‚Äù she says. ‚ÄúTeachers tell me that they‚Äôve seen a spike in nudification apps in schools (T0166.003: AI-Nudified Imagery). The affordability and accessibility has increased tenfold.‚Äù<br><br>In England and Wales, legislation is finally grappling with the issue ‚Äì the Online Safety Act and the Data (Use and Access) Act 2025 have made the sharing, creation and requesting of deepfake intimate image abuse illegal. In Northern Ireland, too, there are plans to criminalise it ‚Äì the consultation process closed in October.<br><br>Yet it seems the public has been slow to grasp its harms. New police research, released last week, suggests one in four people still think there is nothing wrong with creating and sharing sexual deepfakes, or feel neutral about it. ‚ÄúI was shocked by that,‚Äù says Hunter. ‚ÄúThis is a world where falsified, highly sexualised images can ruin your life, ruin your relationships, your reputation and career, and there are people who think: ‚ÄòIt‚Äôs a bit of fun, it‚Äôs a bit of craic.‚Äô‚Äù She takes a long sigh. ‚ÄúI was shocked ‚Äì but at the same time, not surprised. The normalisation of violence against women and girls cannot be overstated.‚Äù<br><br>For Hunter, who has just turned 30, the weeks after the video‚Äôs release were ‚Äúhorrific‚Äù. ‚ÄúI didn‚Äôt know what to do. Should I do a press release? Should I put a Facebook status out? You‚Äôre a young woman, 27 years old, and it was so hard to be taken seriously politically anyway.‚Äù Her party, the Social Democratic and Labour party (SDLP), advised her to ignore it. ‚ÄúEven recalling it now, I can‚Äôt believe this happened, but they said: ‚ÄòWe‚Äôre two and a half weeks from an election. If you do a press release, your name will be right up there with words like ‚Äòpornography‚Äô ‚Äì and people will see you through a sexual lens and also go looking for it.‚Äô They said: ‚ÄòIf 10,000 people know about that video now, 100,000 will know after you‚Äôve drawn attention to it.‚Äô Those numbers are burnt into my brain.‚Äù<br><br>Hunter then turned to the police, who informed her (apologetically) that no crime had been committed, and they didn‚Äôt have the technology or expertise to investigate anyway. It was Hunter who found the original video, with the original woman‚Äôs face, by using screenshots from it in reverse image search engines. When it came to identifying who had released the deepfake video on WhatsApp, she learned it was an encrypted platform whose users had the right to privacy. ‚ÄúI‚Äôd like to think I have a right for my life not to be ruined,‚Äù she says. ‚ÄúYou‚Äôre one person up against the massive system of tech and coding.‚Äù<br><br>Many memories from that period still feel mortifying. Hunter‚Äôs uncle hammering at the door, having been shown the video by his friend. She had to invite him in, sit him down and explain that it wasn‚Äôt real. Then later, she had to explain it all again to her father.<br><br>‚ÄúEverywhere I went, people I used to speak to would cross the road to avoid me,‚Äù she says. ‚ÄúI live in a beautiful coastal town that I‚Äôm lucky to represent. A mile from my house is a bar and, a couple of days after this happened, there was a party for a staff member‚Äôs birthday. I thought: ‚ÄòI can‚Äôt let it consume me. I‚Äôm going to go there and have a drink.‚Äô On the way, a man approached me and asked me for oral sex. I kept going, reached the bar and there was complete silence when I walked in. I realised it was a mistake.‚Äù<br><br>Despite Hunter‚Äôs fear that silence would be viewed as evidence that the video was genuine, she followed the party‚Äôs advice and attempted to campaign as normal. ‚ÄúI remember saying to my boyfriend, who is now my husband: ‚ÄòI don‚Äôt care if I‚Äôm elected or not. I just want this to be over.‚Äô‚Äù As it turned out, Hunter won by just 14 votes ‚Äì making her seat the most marginal in Northern Ireland.<br><br>[...]<br><br>She fears experiences like hers will deter young women from entering politics (T0139.003: Deter). ‚ÄúI have these very capable young girls on work experience in my office and I don‚Äôt want them to think this is part and parcel of the political experience,‚Äù she says. ‚ÄúAny time I ask a woman to consider standing, I have to ask three or four times. With men, nine out of 10 times, they‚Äôll say yes straight away.‚Äù<br><br>There‚Äôs little doubt that the deepfake of Hunter directly affected the democratic process. How could it not have lost her votes? (T0139.001: Discourage) Women have been the first victims of this technology ‚Äì a 2023 study found that 98% of online deepfakes are pornographic and that 99% of the targets are women. However, in this era Hunter terms ‚Äúthe AI Olympics‚Äù, the potential for future harms goes far beyond this. The deepfake of Joe Biden‚Äôs voice urging voters not to vote in the New Hampshire primary, or the video of Volodymyr Zelenskyy telling his troops to surrender are early examples. While the UK government has begun to tackle deepfake pornography, Danish authorities are attempting to go much further by changing copyright law. The proposed changes guarantee everyone‚Äôs right to their own body, facial features and voice. (Though the law would allow expressions of satire and parody.) ‚ÄúIf we could bring that in here, I‚Äôd say ‚ÄòHallelujah!‚Äô‚Äù says Hunter. ‚ÄúWhat we could also do ‚Äì and what I‚Äôd really like to see ‚Äì is a mandatory marking which shows on every AI video so everyone understands what they‚Äôre seeing.‚Äù<br><br>Beyond politics, Hunter still thinks about that video clip, that blue-walled bedroom, that upside-down woman, almost every day.<br><br>‚ÄúEven now, I‚Äôm thinking: ‚ÄòShould I offer ¬£500 to anyone with information on who did this?‚Äô‚Äù she says. ‚ÄúI need to understand. Is it personal? Is it that they hate me or is it that they hate women and don‚Äôt like to see a woman in power? Is it sectarian, because I‚Äôm a Catholic nationalist woman? Is it just that people maybe see me up there and think: ‚ÄòI‚Äôll take her down a few pegs‚Äô?‚Äù</i> |
| [I00248 Woman's deepfake betrayal by close friend: 'Every moment turned into porn'](../../generated_pages/incidents/I00248.md) | <i>It was a warm February night when an ominous message popped into Hannah Grundy's inbox in Sydney.<br><br>"I will just keep emailing because I think this is worthy of your attention," the anonymous sender wrote.<br><br>Inside was a link, and a warning in bold: "[This] contains disturbing material."<br><br>She hesitated for a moment, fearing it was a scam.<br><br>The reality was so much worse. The link contained pages and pages of fake pornography featuring Hannah, alongside detailed rape fantasies and violent threats. (T0166.002: Sexual Deepfake Impersonation, T0048.010: Threaten Physical Sexual Violence)<br><br>"You're tied up in them," she recalls. "You look afraid. You've got tears in your eyes. You're in a cage."<br><br>Written in kitschy word art on some images was Hannah's full name. Her Instagram handle was posted, as was the suburb she lived in. She would later learn her phone number had also been given out (T0048.004: Dox).<br><br>That email kicked off a saga Hannah likens to a movie. She was left to become her own detective, uncovering a sickening betrayal by someone close to her, and building a case which changed her life - and Australian legal standards.<br><br>The web page was called "The Destruction of Hannah", and at the top of it was a poll where hundreds of people had voted on the vicious ways they wanted to abuse her (T0029: Online Polls).<br><br>Below was a thread of more than 600 vile photos, with Hannah's face stitched on to them. Buried in between them were chilling threats.<br><br>"I'm closing in on this slut," the main poster said.<br><br>"I want to hide in her house and wait until she is alone, grab her from behind and... feel her struggle." (T0048.009: Threaten Physical Violence)<br><br>It's been three years now, but the 35-year-old school teacher has no trouble recalling the "pure shock" that coursed through when she and partner Kris Ventura, 33, opened the page.<br><br>"You immediately feel unsafe," Hannah tells me, eyes wide as she grips a mug of peppermint tea in her living room.<br><br>Clicking through the website Kris had also found photos of their close friends, along with images depicting at least 60 other women, many also from Sydney.<br><br>The couple quickly realised the pictures used to create the deepfakes were from the women's private social media accounts. And the penny dropped: this was someone they all knew.</i> |
| [I00249 ‚ÄòIt‚Äôs Disgusting‚Äô: Rosal√≠a Fires Back at Artist Who Shared Photoshopped Nude Photos of Her](../../generated_pages/incidents/I00249.md) | <i>[Spanish musician] Rosal√≠a vented her frustration with Spanish artist JC Reyes after he posted photoshopped images of her naked on social media, calling out the musician for not asking for consent and ‚Äúcreating a false narrative when I don‚Äôt even know you.‚Äù (T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material)<br><br>[...]<br><br>According to screengrabs circulating on social media, Reyes, or someone with access to his Instagram account, shared the photographs on his Stories. They appeared to be altered versions of photos Rosal√≠a had originally taken and shared of herself (T0165: Edited Content, T0084.002: Plagiarise Content).<br><br>[...]<br><br>While the photos have since been removed from Reyes‚Äô Instagram Stories, he seemed to boast about them ‚Äî and suggest Rosal√≠a had sent them to him ‚Äî in a subsequent live video.</i> |
| [I00250 Chinese actress Jiang Mengjie praised for revealing upskirting blackmail](../../generated_pages/incidents/I00250.md) | <i>Chinese actress Jiang Mengjie has been praised for sharing she was the victim of upskirting (T0048.005: Voyeuristic Content) and was blackmailed over footage that circulated of her (T0048.012: Sextortion).<br><br>Upskirting - involving a device such as a camera phone to take explicit images underneath a victim's clothing without permission - often goes undetected.<br><br>Jiang Mengjie told her eight million followers on social network Weibo the video had been filmed "many years ago".<br><br>[...]<br><br>Jiang further revealed that she had begun receiving private messages blackmailing her over the footage "saying that they would send the video to major film and TV companies and brands, and ruin the rest of my life" (T0153.007: Direct Messaging, T0115.004: Send Message, T0048.012: Sextortion).<br><br>[Here, a screenshot of messages is shown. In it, a user sends the messages ‚Äúmy patience is limited‚Äù, ‚ÄúSister, don‚Äôt be so nervous. I‚Äôm just asking for money.‚Äù (T0137: Make Money) ‚ÄúYou have ignored me for so long, it seems you can afford the contract breach of contract, advertising breach of contract, and your acting career?]‚Äù<br><br>[...]<br><br>She told her followers: "as a public figure, maybe I can make more people pay attention to such vicious incidents by taking a stand.<br><br>"It is not our fault that we have been secretly photographed. Our lives should not be affected by this kind of thing."</i> |
| [I00251 The Deepfake Porn Crisis Is Here](../../generated_pages/incidents/I00251.md) | <i>In June 2024, Guistolise was having dinner with friends when she received a call that changed everything. It was Samantha*, the ex-wife of Guistolise‚Äôs friend Ben, whom she had known for many years. Samantha said she needed to meet. Immediately.<br><br>When they spoke, Samantha shared with Guistolise that Ben had secretly been making deepfake pornographic images and videos of women, including Guistolise, using a ‚Äúnudify‚Äù app, which takes real images and turns them into naked photos or videos using AI (T0166.003: AI-Nudified Imagery). There weren‚Äôt just a few images‚Äîthere were hundreds, depicting 80 different women. And that‚Äôs just what Samantha was able to catalog and capture of what Ben had made over the course of a single week.<br><br>"It was me, but it wasn‚Äôt me,‚Äú Guistolise tells Glamour about how she felt when she first saw the images. ‚ÄúIt was so bizarre and dystopian and confusing.‚Äù<br><br>After learning about the deepfakes, Guistolise and her friends got to work identifying as many of the women as they could. Some of those women, in turn, helped identify others.<br><br>Each deepfake was created using screengrabs from the woman‚Äôs social media channels. Nudify apps (T0154.002: AI Media Platform) like the one Ben used typically only need one image of a person‚Äôs face to create a deepfake image. For Guistolise, it was from a photo taken at her goddaughter‚Äôs graduation and another from a family vacation, which she had posted to Facebook. This is a problem mostly faced by women, as the apps used to make nudify images aren‚Äôt trained to create images of the male form.<br><br>A year later, the women still haven‚Äôt been able to identify every single person in the images. They also haven‚Äôt seen any justice‚Äîthe police officers they contacted said there is little they can do, even though Ben admitted to creating the images. And that‚Äôs because the laws around deepfake porn remains murky at best.<br><br>‚ÄúThis was not really something that had been on our radar,‚Äù says Omny Miranda Martone, the founder and CEO of the Sexual Violence Prevention Association (SVPA). ‚ÄúWe‚Äôd been working on child abuse, sexual violence, and rape. We had not been working on any digital sexual violence until about three years ago.‚Äù<br><br>That changed when a victim came to them, sharing a story of how a rejected date at the gym turned into a man creating a deepfake of her and sharing it with her entire fitness community. When Martone asked the organization‚Äôs lawyers if there was more the survivor could do to protect herself, their answer was a resounding, ‚ÄúNot really.‚Äù<br><br>According to the lawyers, the act of creating that image and showing it to others didn‚Äôt constitute defamation because the man hadn‚Äôt been trying to pass it off as real. The victim wasn‚Äôt underage. And there was no law yet requiring social media platforms to take it down if he decided to distribute it. Still, SVPA knew that this was sexual violence. So Martone, their organization, and a few lawmakers got to work.<br><br>In May, President Donald Trump signed the Take It Down Act, a bipartisan bill introduced by senators Ted Cruz (R-Tex.) and Amy Klobuchar (D-Minn.) and endorsed by organizations like Martone‚Äôs and the Rape, Abuse & Incest National Network (RAINN). The bill ‚Äúprohibits the nonconsensual online publication of intimate visual depictions of individuals, both authentic and computer-generated,‚Äù and, critically, will require platforms like Instagram and Meta to take down the images if they receive notice of their existence within 48 hours.<br><br>‚ÄúThat one was a really big win,‚Äù Martone says of the bill, which goes into effect in 2026. Stefan Turkheimer, the vice president for public policy at RAINN, noted that this law protects everybody, whether or not the images are authentic or fake. It also doesn‚Äôt matter whether they were taken consensually or nonconsensually, as long as they were shared nonconsensually.<br><br>But this is where things get complicated: This law protects people against the dissemination of deepfakes or intimate images shared without their consent. There is little to nothing someone can do about a person‚Äîbe it someone they know or a total stranger‚Äîmaking and keeping the deepfakes for themselves.<br><br>‚ÄúIt‚Äôs not enough,‚Äù says Guistolise of the Take It Down Act. While it‚Äôs certainly a step forward, Guistolise says it still requires people to find the content, which she likens to trying to ‚Äúfind a needle in a needle stack.‚Äù If and when victims do find it and report it, they must then wait 48 hours for it to be removed, revictimizing themselves in the process.<br><br>‚ÄúSearching for your own fake porn is not something, mental-health-wise, I‚Äôm willing to do,‚Äù Guistolise says. And again, this is only if the perpetrator distributes images in the first place. What Guistolise and many advocates want, instead, is to make the technology to create these deepfake images illegal in the first place.<br><br>‚ÄúThere are all kinds of things that have been said about it, like, ‚ÄòWe don‚Äôt wanna block innovation,‚Äô‚Äù Guistolise says. ‚ÄúBut women‚Äôs and children‚Äôs bodies are being sacrificed on the altar of capitalism. This is not innovation. This is exploitation. There is no reason for it to exist other than making money off of apps.‚Äù<br><br>It‚Äôs an issue that is absolutely exploding. In 2023, Time reported on an analysis by Graphika that found 24 million people had visited ‚Äúundressing websites‚Äù in the month of September alone. These nudify apps have also been advertised everywhere from Google to Reddit and across social platforms. In July, Wired reported that the apps are worth about $36 million thanks to paid users and free users, who pony up their personal data that is later sold to the highest bidder.<br><br>There is, however, a little hope on the horizon. Representative Alexandria Ocasio-Cortez (D-N.Y.) along with Representative Laurel Lee (R-Fla.), Senator Richard J. Durbin (D-Ill.), and Senator Lindsey Graham (R-S.C.) has reintroduced another bill known as the Disrupt Explicit Forged Images and Nonconsensual Edits Act (DEFIANCE Act). If passed, it will give victims the right to bring a civil action against ‚Äúindividuals who knowingly produce, distribute, solicit, and receive or possess with the intent to distribute nonconsensual sexually explicit digital forgeries.‚Äù This means victims would have the right to sue for damages, and it‚Äôs supported by both RAINN and the SVPA.<br><br>‚ÄúWe are reintroducing the DEFIANCE Act to grant survivors and victims of nonconsensual deepfake pornography the legal right to pursue justice,‚Äù Ocasio-Cortez said in a statement. ‚ÄúI am proud to lead this legislation with Representative Lee, and senators Durbin and Graham, to provide victims with the federal protections they deserve.‚Äù<br><br>Elsewhere in the world, nations like Denmark are hoping to help protect people by passing legislation that gives everyone the copyright to their own likeness, including their voice. Our counterparts at Glamour UK lobbied tirelessly to change the law in the United Kingdom; it's now a criminal offense to create sexually explicit digital forgeries.<br><br>In Minnesota, Guistolise and the women in her group are working alongside state senator Erin Maye Quade and the state‚Äôs Senate Judiciary and Public Safety Committee on a potential bill that would outlaw nudification, requiring AI companies to disable the function that allows it to create the images, or fine them up to $500,000 for each nonconsensual deepfake.<br><br>‚ÄúFor these AI-generated photos and videos, the harm begins at creation,‚Äù Quade told MPR News. ‚ÄúDissemination currently in Minnesota of nonconsensual, sexual deepfakes is illegal. But they can download these apps on their phones, and they‚Äôre doing that. They‚Äôre nudifying their teachers, their classmates, their siblings, friends.‚Äù<br><br>These laws, however common sense they may feel, will still face an upward battle. In May the Elon Musk‚Äìowned platform X sued the state of Minnesota over its law banning the creation of deepfakes to influence an election, which it said violated free speech. In August, Musk won a similar lawsuit against the state of California for its deepfake ban.<br><br>For Guistolise, this event has caused immeasurable pain. She‚Äôs lost trust in others. She‚Äôs afraid of how this may affect her future and her career. However, there is a ‚Äúnext‚Äù for her. She gets to go on being the sister and friend she‚Äôs always been. She‚Äôs excited to go to work tomorrow. She‚Äôs training her pitbull puppy, whom she appropriately named Olivia Benson, how to give a high five. And despite it all, ‚ÄúI love humans,‚Äù she says, before pausing. ‚ÄúI guess I still do.‚Äù</i> |



| Counters | Response types |
| -------- | -------------- |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW