# Technique T0163.001: Narrative Cites Nonexistent Academic Research

**Summary**: A narrative makes a claim, and points to academic research to back up the legitimacy of the claim - however, the research does not actually exist.<br><br>Citation of nonexistent academic research can occur when using AI-LLMs to write reports. [AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.](https://www.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/) Because of this, they may produce citations which look correct, but don’t point to research that really exists.<br><br>To count as a citation, the content must explicitly point to a specific piece of academic research to support a claim. This could be via an embedded link, footnotes, or providing the name of a paper, the journal it appears in, and the researchers who produced it.<br><br>To make this assertion, analysts must confirm that the cited research does not exist (while being lenient against honest mistakes such as slight errors in how the research is named), by investigating each citation made and confirming it exists as described.<br><br>For example, [this Fact Check](https://www.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/) looked into citations made in a report which was purportedly produced using AI:<br><br><i>”One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” [...]<br><br>“A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.”</i>

**Tactic**: TA14 Develop Narratives <br><br>**Parent Technique:** T0163 Issues with Cited Academic Research


| Associated Technique | Description |
| --------- | ------------------------- |
| [T0164.001 Narrative Presents Fabricated Statistics as Genuine Data](../../generated_pages/techniques/T0164.001.md) | T0163.001: Narrative Cites Nonexistent Academic Research differs from T0164.001: Narrative Presents Fabricated Statistics as Genuine Data in that T0163.001: Narrative Cites Nonexistent Academic Research specifically cites a study which doesn’t exist, rather than just providing statistics without supporting evidence. |



| Incident | Descriptions given for this incident |
| -------- | -------------------- |
| [I00180 RFK Jr.’s health report shows how AI slips fake studies into research](../../generated_pages/incidents/I00180.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |



| Counters | Response types |
| -------- | -------------- |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW