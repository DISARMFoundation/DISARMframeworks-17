# Technique T0166.002: Sexual Deepfake Impersonation

**Summary**: This sub-technique can be used to assert that a piece of content is a sexual AI-Generated deepfake impersonation (sometimes referred to as “AI Non-Consensual Intimate Imagery” / “AI NCII”); a deepfake which has placed a target into a sexual scene, or depicts them sexually.<br><br>A deepfake refers to AI-generated content that artificially inserts the likeness of a real person into a new or altered media scene. These synthetic images, videos, or audio clips use machine learning models to manipulate or replace the original content, making it appear as though the target is participating in actions or situations they never actually did.<br><br>To identify sexual deepfake impersonations, analysts will need to confirm that the imagery has been generated using AI, and to assert that the imagery is sexual in nature. The nature of what is considered sexual can differ based on cultural context. Analysts will need to consider the societal norms of the targeted individual, or the audience of the deepfake when asserting whether it is sexual in nature.<br><br>As AI continues to develop capability to produce more realistic media, it is difficult to provide a list of methods to spot materials that have been created using AI which will not rapidly become outdated; platforms providing AI media generation are attempting to make the materials they produce indistinguishable from media made by humans. <br><br>At the same time, organisations create tools designed to help identify AI-generated content, and publish resources listing the latest tells and identifiers for AI-generated content. Analysts may look for such tools or resources to help identify the use of AI.

**Tactic**: TA06 Develop Content <br><br>**Parent Technique:** T0166 AI-Generated Content


| Associated Technique | Description |
| --------- | ------------------------- |
| [T0161.004 Imagery Depicting Individual Edited to Introduce Sexual Material](../../generated_pages/techniques/T0161.004.md) | Where T0166.002: Sexual Deepfake Impersonation documents the use of AI to create a new image or video depicting an individual in an entirely fabricated sexual situation, T0161.004: Imagery Depicting Individual Edited to Introduce Sexual Material documents manual edits (i.e. not made using AI) which splice images or videos depicting an individual into existing sexual media. |
| [T0162.006 AI-Generated Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.006.md) | A T0166.002: Sexual Deepfake Impersonation which has been published alongside material which implies or directly claims that the T0166.002: Sexual Deepfake Impersonation is actually a depiction of the real world (i.e. captured by a camera, a recording device, or otherwise manually produced by a human without the use of AI) should also be documented using T0162.006: AI-Generated Content Incorrectly Presented as Depicting Reality.  |
| [T0166.003 AI-Nudified Imagery](../../generated_pages/techniques/T0166.003.md) | Where AI-nudification digitally alters an existing photo or video to produce a fabricated nude; T0166.002: Sexual Deepfake Impersonation creates falsified sexual imagery or video of a targeted individual imposing them into sexual source material, or creating entirely synthesized sexual content. |



| Incident | Descriptions given for this incident |
| -------- | -------------------- |
| [I00248 Woman's deepfake betrayal by close friend: 'Every moment turned into porn'](../../generated_pages/incidents/I00248.md) | <i>It was a warm February night when an ominous message popped into Hannah Grundy's inbox in Sydney.<br><br>"I will just keep emailing because I think this is worthy of your attention," the anonymous sender wrote.<br><br>Inside was a link, and a warning in bold: "[This] contains disturbing material."<br><br>She hesitated for a moment, fearing it was a scam.<br><br>The reality was so much worse. The link contained pages and pages of fake pornography featuring Hannah, alongside detailed rape fantasies and violent threats. (T0166.002: Sexual Deepfake Impersonation, T0048.010: Threaten Physical Sexual Violence)<br><br>"You're tied up in them," she recalls. "You look afraid. You've got tears in your eyes. You're in a cage."<br><br>Written in kitschy word art on some images was Hannah's full name. Her Instagram handle was posted, as was the suburb she lived in. She would later learn her phone number had also been given out (T0048.004: Dox).<br><br>That email kicked off a saga Hannah likens to a movie. She was left to become her own detective, uncovering a sickening betrayal by someone close to her, and building a case which changed her life - and Australian legal standards.<br><br>The web page was called "The Destruction of Hannah", and at the top of it was a poll where hundreds of people had voted on the vicious ways they wanted to abuse her (T0029: Online Polls).<br><br>Below was a thread of more than 600 vile photos, with Hannah's face stitched on to them. Buried in between them were chilling threats.<br><br>"I'm closing in on this slut," the main poster said.<br><br>"I want to hide in her house and wait until she is alone, grab her from behind and... feel her struggle." (T0048.009: Threaten Physical Violence)<br><br>It's been three years now, but the 35-year-old school teacher has no trouble recalling the "pure shock" that coursed through when she and partner Kris Ventura, 33, opened the page.<br><br>"You immediately feel unsafe," Hannah tells me, eyes wide as she grips a mug of peppermint tea in her living room.<br><br>Clicking through the website Kris had also found photos of their close friends, along with images depicting at least 60 other women, many also from Sydney.<br><br>The couple quickly realised the pictures used to create the deepfakes were from the women's private social media accounts. And the penny dropped: this was someone they all knew.</i> |



| Counters | Response types |
| -------- | -------------- |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW