# Incident I00180: RFK Jr.’s health report shows how AI slips fake studies into research

* **Summary:** <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist, and three mischaracterized the articles’ findings.</i>

* **incident type**: 

* **Year started:** 

* **Countries:**  , 

* **Found via:** 

* **Date added:** 


| Reference | Pub Date | Authors | Org | Archive |
| --------- | -------- | ------- | --- | ------- |
| [https://www.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/](https://www.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/) | 2025-06-02 00:00:00 | Loreben Tuquero | Poynter | [https://archive.is/qgsRH](https://archive.is/qgsRH) |

 

| Technique | Description given for this incident |
| --------- | ------------------------- |
| [T0039.001 Collaborating Assets Seed and Ping](../../generated_pages/techniques/T0039.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0039.002 Solicit Production of Fact Check](../../generated_pages/techniques/T0039.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0048.005 Voyeuristic Content](../../generated_pages/techniques/T0048.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0068 Respond to Breaking News Event or Active Crisis](../../generated_pages/techniques/T0068.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0085.004 Develop Document](../../generated_pages/techniques/T0085.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0086 Develop Image-Based Content](../../generated_pages/techniques/T0086.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0087 Develop Video-Based Content](../../generated_pages/techniques/T0087.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0087 Develop Video-Based Content](../../generated_pages/techniques/T0087.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0087 Develop Video-Based Content](../../generated_pages/techniques/T0087.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0087 Develop Video-Based Content](../../generated_pages/techniques/T0087.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0097.110 Party Official Persona](../../generated_pages/techniques/T0097.110.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0097.110 Party Official Persona](../../generated_pages/techniques/T0097.110.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0097.110 Party Official Persona](../../generated_pages/techniques/T0097.110.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0097.202 News Outlet Persona](../../generated_pages/techniques/T0097.202.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0120 Incentivize Sharing](../../generated_pages/techniques/T0120.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0120 Incentivize Sharing](../../generated_pages/techniques/T0120.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0146 Account Asset](../../generated_pages/techniques/T0146.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0146 Account Asset](../../generated_pages/techniques/T0146.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0146.003 Verified Account Asset](../../generated_pages/techniques/T0146.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0146.007 Automated Account Asset](../../generated_pages/techniques/T0146.007.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0149.004 Redirecting Domain Asset](../../generated_pages/techniques/T0149.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0151.001 Social Media Platform](../../generated_pages/techniques/T0151.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0152.004 Website Asset](../../generated_pages/techniques/T0152.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0152.004 Website Asset](../../generated_pages/techniques/T0152.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0153.004 QR Code Asset](../../generated_pages/techniques/T0153.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.002 Information is False](../../generated_pages/techniques/T0160.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.004 Information is Misleading](../../generated_pages/techniques/T0160.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.004 Information is Misleading](../../generated_pages/techniques/T0160.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.005 Content Produced as Satire](../../generated_pages/techniques/T0160.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0160.007 Claim Previously Fact Checked](../../generated_pages/techniques/T0160.007.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.001 Impersonated Content](../../generated_pages/techniques/T0161.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.001 Impersonated Content](../../generated_pages/techniques/T0161.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.002 Statement Incorrectly Presented as Made by Individual or Institution](../../generated_pages/techniques/T0161.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.003 Falsified Graffiti or Signage](../../generated_pages/techniques/T0161.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0161.004 Imagery Depicting Individual Edited to Introduce Sexual Material](../../generated_pages/techniques/T0161.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162 Reframe Context](../../generated_pages/techniques/T0162.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162 Reframe Context](../../generated_pages/techniques/T0162.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162 Reframe Context](../../generated_pages/techniques/T0162.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.001 Incorrect Subtitled Speech Reframes Context](../../generated_pages/techniques/T0162.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.002 Edits Made to News Report which Reframe Context](../../generated_pages/techniques/T0162.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.003 Historic Content Incorrectly Presented as Current](../../generated_pages/techniques/T0162.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.004 Content Incorrectly Presented as Depicting Another Location](../../generated_pages/techniques/T0162.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.004 Content Incorrectly Presented as Depicting Another Location](../../generated_pages/techniques/T0162.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.005 Video Game Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.005.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.008 Context Reframed by Edits to Media](../../generated_pages/techniques/T0162.008.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.008 Context Reframed by Edits to Media](../../generated_pages/techniques/T0162.008.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.008 Context Reframed by Edits to Media](../../generated_pages/techniques/T0162.008.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.010 Entertainment Media Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.010.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.010 Entertainment Media Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.010.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.010 Entertainment Media Content Incorrectly Presented as Depicting Reality](../../generated_pages/techniques/T0162.010.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0162.011 Content Originally Produced as Satire Presented as Not Satire](../../generated_pages/techniques/T0162.011.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.001 Narrative Cites Nonexistent Academic Research](../../generated_pages/techniques/T0163.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.002 Narrative Misrepresents Findings of Cited Academic Research](../../generated_pages/techniques/T0163.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.002 Narrative Misrepresents Findings of Cited Academic Research](../../generated_pages/techniques/T0163.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.002 Narrative Misrepresents Findings of Cited Academic Research](../../generated_pages/techniques/T0163.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.003 Narrative Cites Retracted Academic Research](../../generated_pages/techniques/T0163.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.003 Narrative Cites Retracted Academic Research](../../generated_pages/techniques/T0163.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.004 Narrative Cites Academic Research not Peer Reviewed](../../generated_pages/techniques/T0163.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0163.004 Narrative Cites Academic Research not Peer Reviewed](../../generated_pages/techniques/T0163.004.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0164.002 Narrative Uses Selective Statistics to Support Claim](../../generated_pages/techniques/T0164.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0164.003 Narrative Uses Misinterpreted Statistics to Support Claim](../../generated_pages/techniques/T0164.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0164.003 Narrative Uses Misinterpreted Statistics to Support Claim](../../generated_pages/techniques/T0164.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0164.003 Narrative Uses Misinterpreted Statistics to Support Claim](../../generated_pages/techniques/T0164.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0165 Edited Content](../../generated_pages/techniques/T0165.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0165 Edited Content](../../generated_pages/techniques/T0165.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0165 Edited Content](../../generated_pages/techniques/T0165.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0165.003 Playback Speed Altered](../../generated_pages/techniques/T0165.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166 AI-Generated Content](../../generated_pages/techniques/T0166.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166 AI-Generated Content](../../generated_pages/techniques/T0166.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166 AI-Generated Content](../../generated_pages/techniques/T0166.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166 AI-Generated Content](../../generated_pages/techniques/T0166.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166.001 Deepfake Impersonation](../../generated_pages/techniques/T0166.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166.002 Sexual Deepfake Impersonation](../../generated_pages/techniques/T0166.002.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0166.003 AI-Nudified Imagery](../../generated_pages/techniques/T0166.003.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0167.001 Use of Clickbait](../../generated_pages/techniques/T0167.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0167.001 Use of Clickbait](../../generated_pages/techniques/T0167.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0167.001 Use of Clickbait](../../generated_pages/techniques/T0167.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |
| [T0167.001 Use of Clickbait](../../generated_pages/techniques/T0167.001.md) | <i>The authors of the “Make America Healthy Again” report issued by Health and Human Services Secretary Robert F. Kennedy Jr. touted it as a landmark assessment providing “common scientific basis” to shape health policy.<br><br>But that “scientific basis” appeared to have errors generated by a likely culprit: generative artificial intelligence.<br><br>At least seven of the report’s citations were problematic, as NOTUS first reported. Four contained titles of papers that don’t exist (T0163.001: Narrative Cites Nonexistent Academic Research), and three mischaracterized the articles’ findings (T0163.002: Narrative Misrepresents Findings of Cited Academic Research).<br><br>[...]<br><br>AI models are trained to mimic language that humans use by predicting one word after another in a sequence. Although AI chatbots like ChatGPT often succeed in producing text that reads like it was written by a human, they often fail to ensure that what they’re saying is factual.<br><br>The faux citations were formatted correctly, listed reputable journals and included realistic-sounding digital object identifiers or DOIs.<br><br>But the fact that multiple articles cited did not exist (T0163.001: Narrative Cites Nonexistent Academic Research) “is a hallmark of AI-generated citations, which often replicate the structure of academic references without linking to actual sources,” said Oren Etzioni, a University of Washington professor emeritus and AI researcher.<br><br>[...]<br><br>The Washington Post reported that some citations included “oaicite” in their URLs, which ChatGPT users have reported as text that appears on their output (T0166: AI-Generated Content). (OpenAI owns ChatGPT.)<br><br>[...]<br><br>Even in the report’s legitimate citations, some findings were misrepresented or exaggerated (T0163.002: Narrative Misrepresents Findings of Cited Academic Research) — another error common to generative AI tools, which “can confidently produce incorrect summaries of research,” Etzioni said.<br><br>The updated version of the MAHA report replaced the fake citations with sources that backed its findings, and in some places, revised how it presented the findings previously linked to the fake citations. (See our spreadsheet.)<br><br>One of the fake articles flagged by NOTUS was titled, “Changes in mental health and substance use among US adolescents during the COVID-19 pandemic.” The line that cited it read, “Approximately 20-25% of adolescents reported anxiety symptoms and 15-20% reported depressive symptoms, with girls showing significantly higher rates.”<br><br>A closer examination of the citation shows why it’s not authentic: Searching the title does not yield a real article, clicking the DOI link in the citation leads to an error page saying “DOI not found,” and looking at the JAMA Pediatrics volume and issue number referenced leads to an article with a different title and different authors.<br><br>The updated MAHA report replaced the citation with a 2024 report from KFF, which said that in 2021 and 2022, 21% of adolescents reported anxiety symptoms and 17% reported depression symptoms.<br><br>The original report cited two nonexistent articles in a section about direct-to-consumer advertising for ADHD drug use by children and antidepressant use by teenagers. The report said advertising for antidepressants in teenagers showed “vague symptom lists that overlap with typical adolescent behaviors” and was linked to “inappropriate parental requests for antidepressants.”<br><br>The new version of the report now reads, “DTC advertising is believed to encourage greater use of psychotropic medications in adolescents, including antianxiety, antipsychotic, and antidepressant classes,” now citing a 2006 study that used data from 1994 to 2001. The authors believed direct-to-consumer advertising played a part in encouraging more use of psychotropics.<br><br>Another finding in the MAHA report about asthma drug prescriptions said “an estimated 25-40% of mild cases are overprescribed,” citing a nonexistent article. Pediatric pulmonologist Dr. Harold J. Farber, who was listed as the article’s supposed first author, told NOTUS that was an “overgeneralization” of his research.<br><br>The updated report removed those figures. It now reads, “There is evidence of overprescription of oral corticosteroids for mild cases of asthma.”<br><br>The MAHA report incident highlights the risks of including AI-generated content in official government reports without human review.</i> |


DO NOT EDIT ABOVE THIS LINE - PLEASE ADD NOTES BELOW